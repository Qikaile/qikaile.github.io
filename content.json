{"meta":{"title":"天镜云生","subtitle":"Welcome!","description":"心静思远，志行千里。","author":"TJYS","url":"https://qikaile.tk","root":"/"},"pages":[{"title":"404","date":"2020-03-13T16:00:00.000Z","updated":"2020-04-01T01:17:43.059Z","comments":false,"path":"404.html","permalink":"https://qikaile.tk/404.html","excerpt":"","text":""},{"title":"关于我","date":"2020-03-13T16:00:00.000Z","updated":"2020-04-04T08:45:44.469Z","comments":false,"path":"about/index.html","permalink":"https://qikaile.tk/about/index.html","excerpt":"","text":"念两句诗挑选中...jinrishici.load(function(n){poem.innerHTML=n.data.content,info.innerHTML=\"【\"+n.data.origin.dynasty+\"】\"+n.data.origin.author+\"《\"+n.data.origin.title+\"》\",document.getElementById(\"poem\").value(poem),document.getElementById(\"info\").value(info)})基础信息GitHub：https://github.com/Qikaile坐标：南京专业技能熟悉三维设计软件Python兴趣爱好电影更新日志2020.03.14 - Hello World2020.03.19 - Hexo搭建个人博客系列：主题美化篇2020.03.28 - 机器学习2020.03.28 - Pandas中文手册2020.03.28 - 十分钟搞定 Pandas2020.03.28 - PS功能精通课2021年-新年倒计时157 天 9 时 7 分 42 秒function getRTime(){var e=new Date(\"2021/02/12 00:00:00\"),t=new Date,n=e.getTime()-t.getTime(),m=Math.floor(n/1e3/60/60/24),o=Math.floor(n/1e3/60/60%24),r=Math.floor(n/1e3/60%60),d=Math.floor(n/1e3%60);document.getElementById(\"t_d\").innerHTML=m+\" 天\",document.getElementById(\"t_h\").innerHTML=o+\" 时\",document.getElementById(\"t_m\").innerHTML=r+\" 分\",document.getElementById(\"t_s\").innerHTML=d+\" 秒\"}setInterval(getRTime,1e3)"},{"title":"文章分类","date":"2020-03-14T06:29:13.000Z","updated":"2020-03-14T06:30:09.378Z","comments":false,"path":"categories/index.html","permalink":"https://qikaile.tk/categories/index.html","excerpt":"","text":""},{"title":"-画廊-","date":"2020-03-15T02:22:04.000Z","updated":"2020-04-02T07:31:02.819Z","comments":false,"path":"gallery/index.html","permalink":"https://qikaile.tk/gallery/index.html","excerpt":"","text":"获取中...- 翰宝阁 -- 润兰轩 -- 怡情轩 -.+ﾟ♪ﾟ+.ｏ.+ﾟ♪ﾟ+.ｏ.+ﾟ♪ﾟ+.ｏ.+ﾟ♪ﾟ+.ｏ.+ﾟ♪ﾟ+.ｏ.+ﾟ♪ﾟ+."},{"title":"友情链接","date":"2020-03-15T09:31:40.000Z","updated":"2020-04-01T12:34:10.440Z","comments":true,"path":"links/index.html","permalink":"https://qikaile.tk/links/index.html","excerpt":"","text":""},{"title":"音乐","date":"2018-12-20T15:14:28.000Z","updated":"2020-04-04T06:40:13.978Z","comments":true,"path":"music/index.html","permalink":"https://qikaile.tk/music/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-02T09:25:17.299Z","updated":"2020-04-02T09:25:17.299Z","comments":true,"path":"photos/data.json","permalink":"https://qikaile.tk/photos/data.json","excerpt":"","text":"{\"list\":[{\"date\":\"2020-04\",\"arr\":{\"year\":2020,\"month\":4,\"link\":[\"2020-04-02_1.jpg\",\"2020-04-02_10.jpg\",\"2020-04-02_11.jpg\",\"2020-04-02_12.jpg\",\"2020-04-02_13.jpg\",\"2020-04-02_14.jpg\",\"2020-04-02_15.jpg\",\"2020-04-02_16.jpg\",\"2020-04-02_17.jpg\",\"2020-04-02_18.jpg\",\"2020-04-02_19.jpg\",\"2020-04-02_2.jpg\",\"2020-04-02_20.jpg\",\"2020-04-02_21.jpg\",\"2020-04-02_22.jpg\",\"2020-04-02_23.jpg\",\"2020-04-02_24.jpg\",\"2020-04-02_25.jpg\",\"2020-04-02_26.jpg\",\"2020-04-02_27.jpg\",\"2020-04-02_28.jpg\",\"2020-04-02_29.jpg\",\"2020-04-02_3.jpg\",\"2020-04-02_30.jpg\",\"2020-04-02_31.jpg\",\"2020-04-02_4.jpg\",\"2020-04-02_5.jpg\",\"2020-04-02_6.jpg\",\"2020-04-02_7.jpg\",\"2020-04-02_8.jpg\",\"2020-04-02_9.jpg\"],\"text\":[\"1\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"2\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"3\",\"30\",\"31\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],\"type\":[\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\",\"image\"]}}]}"},{"title":"相册","date":"2020-04-03T06:55:02.846Z","updated":"2020-04-03T06:55:02.846Z","comments":false,"path":"photos/index.html","permalink":"https://qikaile.tk/photos/index.html","excerpt":"","text":"Photos 待续图片正在加载中…!function(){var t=function(t){var e=document.createElement(\"script\");document.getElementsByTagName(\"body\")[0].appendChild(e),e.setAttribute(\"src\",t)};setTimeout(function(){t(\"https://cdn.jsdelivr.net/gh/qikaile/cdn@1.1/js/ins.js\")},0)}()"},{"title":"","date":"2020-04-02T09:36:32.882Z","updated":"2020-03-31T03:23:50.000Z","comments":true,"path":"photos/ins.css","permalink":"https://qikaile.tk/photos/ins.css","excerpt":"","text":"/* build time:Sat Apr 04 2020 16:46:49 GMT+0800 (GMT+08:00)*/ #post-instagram{padding:30px}#post-instagram .article-entry{padding-right:0}.instagram{position:relative;min-height:500px}.instagram img{width:100%}.instagram .year{font-size:16px}.instagram .open-ins{padding:10px 0;color:#cdcdcd}.instagram .open-ins:hover{color:#657b83}.instagram .year{display:inline}.instagram .thumb{width:25%;height:0;padding-bottom:25%;position:relative;display:inline-block;text-align:center;background:#ededed;outline:1px solid #ddd}.instagram .thumb a{position:relative}.instagram .album h1 em{font-style:normal;font-size:14px;margin-left:10px}.instagram .album ul{display:flex;flex-wrap:wrap;clear:both;width:100%;text-align:left}.instagram .album li{list-style:none;display:inline-block;box-sizing:border-box;padding:0 5px;margin-bottom:-10px;height:0;width:25%;position:relative;padding-bottom:25%}.instagram .album li:before{display:none}.instagram .album div.img-box{position:absolute;width:90%;height:90%;-webkit-box-shadow:0 1px 0 rgba(255,255,255,.4),0 1px 0 1px rgba(255,255,255,.1);-moz-box-shadow:0 1px 0 rgba(255,255,255,.4),0 1px 0 1px rgba(255,255,255,.1);box-shadow:0 1px 0 rgba(255,255,255,.4),0 1px 0 1px rgba(255,255,255,.1)}.instagram .album div.img-box img{width:100%;height:100%;position:absolute;z-index:2}.instagram .album div.img-box .img-bg{position:absolute;top:0;left:0;bottom:0;width:100%;margin:-5px;padding:5px;-webkit-box-shadow:0 0 0 1px rgba(0,0,0,.04),0 1px 5px rgba(0,0,0,.1);-moz-box-shadow:0 0 0 1px rgba(0,0,0,.04),0 1px 5px rgba(0,0,0,.1);box-shadow:0 0 0 1px rgba(0,0,0,.04),0 1px 5px rgba(0,0,0,.1);-webkit-transition:all .15s ease-out .1s;-moz-transition:all .15s ease-out .1s;-o-transition:all .15s ease-out .1s;transition:all .15s ease-out .1s;opacity:.2;cursor:pointer;display:block;z-index:3}.instagram .album div.img-box .icon{font-size:14px;position:absolute;left:50%;top:50%;margin-left:-7px;margin-top:-7px;color:#999;z-index:1}.instagram .album div.img-box .img-bg:hover{opacity:0}.photos-btn-wrap{border-bottom:1px solid #e5e5e5;margin-bottom:20px}.photos-btn{font-size:16px;color:#333;margin-bottom:-4px;padding:5px 8px 3px}.photos-btn.active{color:#08c;border:1px solid #e5e5e5;border-bottom:5px solid #fff}@media screen and (max-width:600px){.instagram .thumb{width:50%;padding-bottom:50%}.instagram .album li{width:100%;position:relative;padding-bottom:100%;text-align:center}.instagram .album div.img-box{margin:0;width:90%;height:90%}} /* rebuild by neat */"},{"title":"","date":"2020-04-02T13:14:12.903Z","updated":"2020-04-02T13:14:12.903Z","comments":true,"path":"photos/ins.js","permalink":"https://qikaile.tk/photos/ins.js","excerpt":"","text":"// build time:Sat Apr 04 2020 16:46:48 GMT+0800 (GMT+08:00) (function(t){var e={};function n(r){if(e[r])return e[r].exports;var i=e[r]={exports:{},id:r,loaded:false};t[r].call(i.exports,i,i.exports,n);i.loaded=true;return i.exports}n.m=t;n.c=e;n.p=\"/dist/\";return n(0)})([function(t,e,n){\"use strict\";n(1);var r=n(2);var i=o(r);function o(t){return t&&t.__esModule?t:{\"default\":t}}var a=[];var u=0;var s;function l(t){var e=t.getBoundingClientRect();var n=getComputedStyle(t,null);var r=document.createElement(\"i\");r.className=\"icon-film\";r.style.color=\"#fff\";r.style.fontSize=\"26px\";r.style.position=\"absolute\";r.style.right=\"10px\";r.style.bottom=\"10px\";r.style.zIndex=1;t.parentNode.appendChild(r)}var c=function h(){var t=document.querySelectorAll('.thumb a[data-type=\"video\"]');for(var e=0,n=t.length;e '+o.text[u]+\" \"}e=e+''+o.year+\"年\"+o.month+'月 '+a+\" \"}document.querySelector(\".instagram\").innerHTML=''+e+\"\";c();i.default.init()};var d=function y(t){var e=t.split(\"/\");return\"/photos/assets/ins/\"+e[e.length-1]};var p=function b(t){var e={};for(var n=0,r=t.length;n=200&&this.status"},{"title":"","date":"2020-04-02T13:37:48.596Z","updated":"2020-03-31T03:23:50.000Z","comments":true,"path":"photos/photoswipe-ui-default.min.js","permalink":"https://qikaile.tk/photos/photoswipe-ui-default.min.js","excerpt":"","text":"/*! PhotoSwipe Default UI - 4.1.2 - 2017-04-05 * http://photoswipe.com * Copyright (c) 2017 Dmitry Semenov; */ !function(a,b){\"function\"==typeof define&&define.amd?define(b):\"object\"==typeof exports?module.exports=b():a.PhotoSwipeUI_Default=b()}(this,function(){\"use strict\";var a=function(a,b){var c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v=this,w=!1,x=!0,y=!0,z={barsSize:{top:44,bottom:\"auto\"},closeElClasses:[\"item\",\"caption\",\"zoom-wrap\",\"ui\",\"top-bar\"],timeToIdle:4e3,timeToIdleOutside:1e3,loadingIndicatorDelay:1e3,addCaptionHTMLFn:function(a,b){return a.title?(b.children[0].innerHTML=a.title,!0):(b.children[0].innerHTML=\"\",!1)},closeEl:!0,captionEl:!0,fullscreenEl:!0,zoomEl:!0,shareEl:!0,counterEl:!0,arrowEl:!0,preloaderEl:!0,tapToClose:!1,tapToToggleControls:!0,clickToCloseNonZoomable:!0,shareButtons:[{id:\"facebook\",label:\"Share on Facebook\",url:\"https://www.facebook.com/sharer/sharer.php?u=\"},{id:\"twitter\",label:\"Tweet\",url:\"https://twitter.com/intent/tweet?text=&url=\"},{id:\"pinterest\",label:\"Pin it\",url:\"http://www.pinterest.com/pin/create/button/?url=&media=&description=\"},{id:\"download\",label:\"Download image\",url:\"\",download:!0}],getImageURLForShare:function(){return a.currItem.src||\"\"},getPageURLForShare:function(){return window.location.href},getTextForShare:function(){return a.currItem.title||\"\"},indexIndicatorSep:\" / \",fitControlsWidth:1200},A=function(a){if(r)return!0;a=a||window.event,q.timeToIdle&&q.mouseUsed&&!k&&K();for(var c,d,e=a.target||a.srcElement,f=e.getAttribute(\"class\")||\"\",g=0;g-1&&(c.onTap(),d=!0);if(d){a.stopPropagation&&a.stopPropagation(),r=!0;var h=b.features.isOldAndroid?600:30;s=setTimeout(function(){r=!1},h)}},B=function(){return!a.likelyTouchDevice||q.mouseUsed||screen.width>q.fitControlsWidth},C=function(a,c,d){b[(d?\"add\":\"remove\")+\"Class\"](a,\"pswp__\"+c)},D=function(){var a=1===q.getNumItemsFn();a!==p&&(C(d,\"ui--one-slide\",a),p=a)},E=function(){C(i,\"share-modal--hidden\",y)},F=function(){return y=!y,y?(b.removeClass(i,\"pswp__share-modal--fade-in\"),setTimeout(function(){y&&E()},300)):(E(),setTimeout(function(){y||b.addClass(i,\"pswp__share-modal--fade-in\")},30)),y||H(),!1},G=function(b){b=b||window.event;var c=b.target||b.srcElement;return a.shout(\"shareLinkClick\",b,c),!!c.href&&(!!c.hasAttribute(\"download\")||(window.open(c.href,\"pswp_share\",\"scrollbars=yes,resizable=yes,toolbar=no,location=yes,width=550,height=420,top=100,left=\"+(window.screen?Math.round(screen.width/2-275):100)),y||F(),!1))},H=function(){for(var a,b,c,d,e,f=\"\",g=0;g-1&&(d.getAttribute(\"class\").indexOf(\"__caption\")>0||/(SMALL|STRONG|EM)/i.test(d.tagName))&&(c.prevent=!1)}),l(\"bindEvents\",function(){b.bind(d,\"pswpTap click\",A),b.bind(a.scrollWrap,\"pswpTap\",v.onGlobalTap),a.likelyTouchDevice||b.bind(a.scrollWrap,\"mouseover\",v.onMouseOver)}),l(\"unbindEvents\",function(){y||F(),t&&clearInterval(t),b.unbind(document,\"mouseout\",L),b.unbind(document,\"mousemove\",K),b.unbind(d,\"pswpTap click\",A),b.unbind(a.scrollWrap,\"pswpTap\",v.onGlobalTap),b.unbind(a.scrollWrap,\"mouseover\",v.onMouseOver),c&&(b.unbind(document,c.eventK,v.updateFullscreen),c.isFullscreen()&&(q.hideAnimationDuration=0,c.exit()),c=null)}),l(\"destroy\",function(){q.captionEl&&(f&&d.removeChild(f),b.removeClass(e,\"pswp__caption--empty\")),i&&(i.children[0].onclick=null),b.removeClass(d,\"pswp__ui--over-close\"),b.addClass(d,\"pswp__ui--hidden\"),v.setIdle(!1)}),q.showAnimationDuration||b.removeClass(d,\"pswp__ui--hidden\"),l(\"initialZoomIn\",function(){q.showAnimationDuration&&b.removeClass(d,\"pswp__ui--hidden\")}),l(\"initialZoomOut\",function(){b.addClass(d,\"pswp__ui--hidden\")}),l(\"parseVerticalMargin\",P),T(),q.shareEl&&h&&i&&(y=!0),D(),Q(),M(),N()},v.setIdle=function(a){k=a,C(d,\"ui--idle\",a)},v.update=function(){x&&a.currItem?(v.updateIndexIndicator(),q.captionEl&&(q.addCaptionHTMLFn(a.currItem,e),C(e,\"caption--empty\",!a.currItem.title)),w=!0):w=!1,y||F(),D()},v.updateFullscreen=function(d){d&&setTimeout(function(){a.setScrollOffset(0,b.getScrollY())},50),b[(c.isFullscreen()?\"add\":\"remove\")+\"Class\"](a.template,\"pswp--fs\")},v.updateIndexIndicator=function(){q.counterEl&&(g.innerHTML=a.getCurrentIndex()+1+q.indexIndicatorSep+q.getNumItemsFn())},v.onGlobalTap=function(c){c=c||window.event;var d=c.target||c.srcElement;if(!r)if(c.detail&&\"mouse\"===c.detail.pointerType){if(I(d))return void a.close();b.hasClass(d,\"pswp__img\")&&(1===a.getZoomLevel()&&a.getZoomLevel()"},{"title":"","date":"2020-04-02T09:36:32.897Z","updated":"2020-03-31T03:23:50.000Z","comments":true,"path":"photos/photoswipe.css","permalink":"https://qikaile.tk/photos/photoswipe.css","excerpt":"","text":"/* build time:Sat Apr 04 2020 16:46:49 GMT+0800 (GMT+08:00)*/ /*! PhotoSwipe main CSS by Dmitry Semenov | photoswipe.com | MIT license */.pswp{display:none;position:absolute;width:100%;height:100%;left:0;top:0;overflow:hidden;-ms-touch-action:none;touch-action:none;z-index:1500;-webkit-text-size-adjust:100%;-webkit-backface-visibility:hidden;outline:0}.pswp *{-webkit-box-sizing:border-box;box-sizing:border-box}.pswp img{max-width:none}.pswp--animate_opacity{opacity:.001;will-change:opacity;-webkit-transition:opacity 333ms cubic-bezier(.4,0,.22,1);transition:opacity 333ms cubic-bezier(.4,0,.22,1)}.pswp--open{display:block}.pswp--zoom-allowed .pswp__img{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in}.pswp--zoomed-in .pswp__img{cursor:-webkit-grab;cursor:-moz-grab;cursor:grab}.pswp--dragging .pswp__img{cursor:-webkit-grabbing;cursor:-moz-grabbing;cursor:grabbing}.pswp__bg{position:absolute;left:0;top:0;width:100%;height:100%;background:#000;opacity:0;-webkit-transform:translateZ(0);transform:translateZ(0);-webkit-backface-visibility:hidden;will-change:opacity}.pswp__scroll-wrap{position:absolute;left:0;top:0;width:100%;height:100%;overflow:hidden}.pswp__container,.pswp__zoom-wrap{-ms-touch-action:none;touch-action:none;position:absolute;left:0;right:0;top:0;bottom:0}.pswp__container,.pswp__img{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none}.pswp__zoom-wrap{position:absolute;width:100%;-webkit-transform-origin:left top;-ms-transform-origin:left top;transform-origin:left top;-webkit-transition:-webkit-transform 333ms cubic-bezier(.4,0,.22,1);transition:transform 333ms cubic-bezier(.4,0,.22,1)}.pswp__bg{will-change:opacity;-webkit-transition:opacity 333ms cubic-bezier(.4,0,.22,1);transition:opacity 333ms cubic-bezier(.4,0,.22,1)}.pswp--animated-in .pswp__bg,.pswp--animated-in .pswp__zoom-wrap{-webkit-transition:none;transition:none}.pswp__container,.pswp__zoom-wrap{-webkit-backface-visibility:hidden}.pswp__item{position:absolute;left:0;right:0;top:0;bottom:0;overflow:hidden}.pswp__img{position:absolute;width:auto;height:auto;top:0;left:0}.pswp__img--placeholder{-webkit-backface-visibility:hidden}.pswp__img--placeholder--blank{background:#222}.pswp--ie .pswp__img{width:100%!important;height:auto!important;left:0;top:0}.pswp__error-msg{position:absolute;left:0;top:50%;width:100%;text-align:center;font-size:14px;line-height:16px;margin-top:-8px;color:#ccc}.pswp__error-msg a{color:#ccc;text-decoration:underline} /* rebuild by neat */"},{"title":"","date":"2020-04-02T13:37:48.594Z","updated":"2020-03-31T03:23:50.000Z","comments":true,"path":"photos/photoswipe.min.js","permalink":"https://qikaile.tk/photos/photoswipe.min.js","excerpt":"","text":"/*! PhotoSwipe - v4.1.2 - 2017-04-05 * http://photoswipe.com * Copyright (c) 2017 Dmitry Semenov; */ !function(a,b){\"function\"==typeof define&&define.amd?define(b):\"object\"==typeof exports?module.exports=b():a.PhotoSwipe=b()}(this,function(){\"use strict\";var a=function(a,b,c,d){var e={features:null,bind:function(a,b,c,d){var e=(d?\"remove\":\"add\")+\"EventListener\";b=b.split(\" \");for(var f=0;f0&&(g=parseInt(g[1],10),g>=1&&g=1&&(i=ac()-1&&d0?i.maxSpreadZoom:1},Va=function(a,b,c,d){return d===f.currItem.initialZoomLevel?(c[a]=f.currItem.initialPosition[a],!0):(c[a]=La(a,d),c[a]>b.min[a]?(c[a]=b.min[a],!0):c[a]1?1:a.fitRatio,c=a.container.style,d=b*a.w,e=b*a.h;c.width=d+\"px\",c.height=e+\"px\",c.left=a.initialPosition.x+\"px\",c.top=a.initialPosition.y+\"px\"},Ha=function(){if(ea){var a=ea,b=f.currItem,c=b.fitRatio>1?1:b.fitRatio,d=c*b.w,e=c*b.h;a.width=d+\"px\",a.height=e+\"px\",a.left=pa.x+\"px\",a.top=pa.y+\"px\"}}},Xa=function(a){var b=\"\";i.escKey&&27===a.keyCode?b=\"close\":i.arrowKeys&&(37===a.keyCode?b=\"prev\":39===a.keyCode&&(b=\"next\")),b&&(a.ctrlKey||a.altKey||a.shiftKey||a.metaKey||(a.preventDefault?a.preventDefault():a.returnValue=!1,f[b]()))},Ya=function(a){a&&(Y||X||fa||T)&&(a.preventDefault(),a.stopPropagation())},Za=function(){f.setScrollOffset(0,e.getScrollY())},$a={},_a=0,ab=function(a){$a[a]&&($a[a].raf&&I($a[a].raf),_a--,delete $a[a])},bb=function(a){$a[a]&&ab(a),$a[a]||(_a++,$a[a]={})},cb=function(){for(var a in $a)$a.hasOwnProperty(a)&&ab(a)},db=function(a,b,c,d,e,f,g){var h,i=Ea();bb(a);var j=function(){if($a[a]){if(h=Ea()-i,h>=d)return ab(a),f(c),void(g&&g());f((c-b)*e(h/d)+b),$a[a].raf=H(j)}};j()},eb={shout:Da,listen:Ca,viewportSize:qa,options:i,isMainScrollAnimating:function(){return fa},getZoomLevel:function(){return s},getCurrentIndex:function(){return m},isDragging:function(){return V},isZooming:function(){return aa},setScrollOffset:function(a,b){sa.x=a,M=sa.y=b,Da(\"updateScrollOffset\",sa)},applyZoomPan:function(a,b,c,d){pa.x=b,pa.y=c,s=a,Ha(d)},init:function(){if(!j&&!k){var c;f.framework=e,f.template=a,f.bg=e.getChildByClass(a,\"pswp__bg\"),J=a.className,j=!0,N=e.detectFeatures(),H=N.raf,I=N.caf,E=N.transform,L=N.oldIE,f.scrollWrap=e.getChildByClass(a,\"pswp__scroll-wrap\"),f.container=e.getChildByClass(f.scrollWrap,\"pswp__container\"),n=f.container.style,f.itemHolders=y=[{el:f.container.children[0],wrap:0,index:-1},{el:f.container.children[1],wrap:0,index:-1},{el:f.container.children[2],wrap:0,index:-1}],y[0].el.style.display=y[2].el.style.display=\"none\",Wa(),r={resize:f.updateSize,orientationchange:function(){clearTimeout(O),O=setTimeout(function(){qa.x!==f.scrollWrap.clientWidth&&f.updateSize()},500)},scroll:Za,keydown:Xa,click:Ya};var d=N.isOldIOSPhone||N.isOldAndroid||N.isMobileOpera;for(N.animationName&&N.transform&&!d||(i.showAnimationDuration=i.hideAnimationDuration=0),c=0;c=ac())&&(m=0),f.currItem=_b(m),(N.isOldIOSPhone||N.isOldAndroid)&&(va=!1),a.setAttribute(\"aria-hidden\",\"false\"),i.modal&&(va?a.style.position=\"fixed\":(a.style.position=\"absolute\",a.style.top=e.getScrollY()+\"px\")),void 0===M&&(Da(\"initialLayout\"),M=K=e.getScrollY());var l=\"pswp--open \";for(i.mainClass&&(l+=i.mainClass+\" \"),i.showHideOpacity&&(l+=\"pswp--animate_opacity \"),l+=G?\"pswp--touch\":\"pswp--notouch\",l+=N.animationName?\" pswp--css_animation\":\"\",l+=N.svg?\" pswp--svg\":\"\",e.addClass(a,l),f.updateSize(),o=-1,ua=null,c=0;cda.min.x?a=da.min.x:ada.min.y?b=da.min.y:b=h&&(o+=ua+(ua>0?-h:h),c=h);for(var d=0;d0?(b=y.shift(),y[h-1]=b,o++,Ja((o+2)*ta.x,b.el.style),f.setContent(b,m-c+d+1+1)):(b=y.pop(),y.unshift(b),o--,Ja(o*ta.x,b.el.style),f.setContent(b,m+c-d-1-1));if(ea&&1===Math.abs(ua)){var e=_b(z);e.initialZoomLevel!==s&&(ic(e,qa),mc(e),Ia(e))}ua=0,f.updateCurrZoomItem(),z=m,Da(\"afterChange\")}}},updateSize:function(b){if(!va&&i.modal){var c=e.getScrollY();if(M!==c&&(a.style.top=c+\"px\",M=c),!b&&xa.x===window.innerWidth&&xa.y===window.innerHeight)return;xa.x=window.innerWidth,xa.y=window.innerHeight,a.style.height=xa.y+\"px\"}if(qa.x=f.scrollWrap.clientWidth,qa.y=f.scrollWrap.clientHeight,Za(),ta.x=qa.x+Math.round(qa.x*i.spacing),ta.y=qa.y,Ka(ta.x*ra),Da(\"beforeResize\"),void 0!==o){for(var d,g,j,k=0;k2&&(j=Aa(j)),g=_b(j),g&&(x||g.needsUpdate||!g.bounds)?(f.cleanSlide(g),f.setContent(d,j),1===k&&(f.currItem=g,f.updateCurrZoomItem(!0)),g.needsUpdate=!1):d.index===-1&&j>=0&&f.setContent(d,j),g&&g.container&&(ic(g,qa),mc(g),Ia(g));x=!1}t=s=f.currItem.initialZoomLevel,da=f.currItem.bounds,da&&(pa.x=da.center.x,pa.y=da.center.y,Ha(!0)),Da(\"resize\")},zoomTo:function(a,b,c,d,f){b&&(t=s,ub.x=Math.abs(b.x)-pa.x,ub.y=Math.abs(b.y)-pa.y,Ma(oa,pa));var g=Sa(a,!1),h={};Va(\"x\",g,h,a),Va(\"y\",g,h,a);var i=s,j={x:pa.x,y:pa.y};Na(h);var k=function(b){1===b?(s=a,pa.x=h.x,pa.y=h.y):(s=(a-i)*b+i,pa.x=(h.x-j.x)*b+j.x,pa.y=(h.y-j.y)*b+j.y),f&&f(b),Ha(1===b)};c?db(\"customZoomTo\",0,1,c,d||e.easing.sine.inOut,k):k(1)}},fb=30,gb=10,hb={},ib={},jb={},kb={},lb={},mb=[],nb={},ob=[],pb={},qb=0,rb=ma(),sb=0,tb=ma(),ub=ma(),vb=ma(),wb=function(a,b){return a.x===b.x&&a.y===b.y},xb=function(a,b){return Math.abs(a.x-b.x)50){var d=ob.length>2?ob.shift():{};d.x=b,d.y=c,ob.push(d),Q=a}},Ib=function(){var a=pa.y-f.currItem.initialPosition.y;return 1-Math.abs(a/(qa.y/2))},Jb={},Kb={},Lb=[],Mb=function(a){for(;Lb.length>0;)Lb.pop();return F?(la=0,mb.forEach(function(a){0===la?Lb[0]=a:1===la&&(Lb[1]=a),la++})):a.type.indexOf(\"touch\")>-1?a.touches&&a.touches.length>0&&(Lb[0]=Fb(a.touches[0],Jb),a.touches.length>1&&(Lb[1]=Fb(a.touches[1],Kb))):(Jb.x=a.pageX,Jb.y=a.pageY,Jb.id=\"\",Lb[0]=Jb),Lb},Nb=function(a,b){var c,d,e,g,h=0,j=pa[a]+b[a],k=b[a]>0,l=tb.x+b.x,m=tb.x-nb.x;return c=j>da.min[a]||jda.min[a]&&(c=i.panEndFriction,h=da.min[a]-j,d=da.min[a]-oa[a]),(d0&&lf.currItem.fitRatio&&(pa[a]+=b[a]*c)):(void 0!==g&&(Ka(g,!0),$=g!==nb.x),da.min.x!==da.max.x&&(void 0!==e?pa.x=e:$||(pa.x+=b.x*c)),void 0!==g)},Ob=function(a){if(!(\"mousedown\"===a.type&&a.button>0)){if($b)return void a.preventDefault();if(!U||\"mousedown\"!==a.type){if(Eb(a,!0)&&a.preventDefault(),Da(\"pointerDown\"),F){var b=e.arraySearch(mb,a.pointerId,\"id\");b1&&!fa&&!$&&(t=s,X=!1,aa=W=!0,na.y=na.x=0,Ma(oa,pa),Ma(hb,c[0]),Ma(ib,c[1]),Gb(hb,ib,vb),ub.x=Math.abs(vb.x)-pa.x,ub.y=Math.abs(vb.y)-pa.y,ba=ca=yb(hb,ib))}}},Pb=function(a){if(a.preventDefault(),F){var b=e.arraySearch(mb,a.pointerId,\"id\");if(b>-1){var c=mb[b];c.x=a.pageX,c.y=a.pageY}}if(V){var d=Mb(a);if(ga||Y||aa)_=d;else if(tb.x!==ta.x*ra)ga=\"h\";else{var f=Math.abs(d[0].x-kb.x)-Math.abs(d[0].y-kb.y);Math.abs(f)>=gb&&(ga=f>0?\"h\":\"v\",_=d)}}},Qb=function(){if(_){var a=_.length;if(0!==a)if(Ma(hb,_[0]),jb.x=hb.x-kb.x,jb.y=hb.y-kb.y,aa&&a>1){if(kb.x=hb.x,kb.y=hb.y,!jb.x&&!jb.y&&wb(_[1],ib))return;Ma(ib,_[1]),X||(X=!0,Da(\"zoomGestureStarted\"));var b=yb(hb,ib),c=Vb(b);c>f.currItem.initialZoomLevel+f.currItem.initialZoomLevel/15&&(ka=!0);var d=1,e=Ta(),g=Ua();if(cg&&(d=(c-g)/(6*e),d>1&&(d=1),c=g+d*e);ds,s=c,Ha()}else{if(!ga)return;if(ha&&(ha=!1,Math.abs(jb.x)>=gb&&(jb.x-=_[0].x-lb.x),Math.abs(jb.y)>=gb&&(jb.y-=_[0].y-lb.y)),kb.x=hb.x,kb.y=hb.y,0===jb.x&&0===jb.y)return;if(\"v\"===ga&&i.closeOnVerticalDrag&&!Bb()){na.y+=jb.y,pa.y+=jb.y;var k=Ib();return T=!0,Da(\"onVerticalDrag\",k),Fa(k),void Ha()}Hb(Ea(),hb.x,hb.y),Y=!0,da=f.currItem.bounds;var l=Nb(\"x\",jb);l||(Nb(\"y\",jb),Na(pa),Ha())}}},Rb=function(a){if(N.isOldAndroid){if(U&&\"mouseup\"===a.type)return;a.type.indexOf(\"touch\")>-1&&(clearTimeout(U),U=setTimeout(function(){U=0},600))}Da(\"pointerUp\"),Eb(a,!1)&&a.preventDefault();var b;if(F){var c=e.arraySearch(mb,a.pointerId,\"id\");if(c>-1)if(b=mb.splice(c,1)[0],navigator.pointerEnabled)b.type=a.pointerType||\"mouse\";else{var d={4:\"mouse\",2:\"touch\",3:\"pen\"};b.type=d[a.pointerType],b.type||(b.type=a.pointerType||\"mouse\")}}var g,h=Mb(a),j=h.length;if(\"mouseup\"===a.type&&(j=0),2===j)return _=null,!0;1===j&&Ma(lb,h[0]),0!==j||ga||fa||(b||(\"mouseup\"===a.type?b={x:a.pageX,y:a.pageY,type:\"mouse\"}:a.changedTouches&&a.changedTouches[0]&&(b={x:a.changedTouches[0].pageX,y:a.changedTouches[0].pageY,type:\"touch\"})),Da(\"touchRelease\",a,b));var k=-1;if(0===j&&(V=!1,e.unbind(window,p,f),zb(),aa?k=0:sb!==-1&&(k=Ea()-sb)),sb=1===j?Ea():-1,g=k!==-1&&k1?(a=Ea()-Q+50,b=ob[ob.length-2][d]):(a=Ea()-P,b=lb[d]),c.lastFlickOffset[d]=kb[d]-b,c.lastFlickDist[d]=Math.abs(c.lastFlickOffset[d]),c.lastFlickDist[d]>20?c.lastFlickSpeed[d]=c.lastFlickOffset[d]/a:c.lastFlickSpeed[d]=0,Math.abs(c.lastFlickSpeed[d])da.min[a]?c.backAnimDestination[a]=da.min[a]:pa[a]fb&&(h||b.lastFlickOffset.x>20)?d=-1:gtb.x==b.lastFlickSpeed.x>0?(k=Math.abs(b.lastFlickSpeed.x)>0?n/Math.abs(b.lastFlickSpeed.x):333,k=Math.min(k,400),k=Math.max(k,250)):k=333,qb===m&&(c=!1),fa=!0,Da(\"mainScrollAnimStart\"),db(\"mainScroll\",tb.x,l,k,e.easing.cubic.out,Ka,function(){cb(),fa=!1,qb=-1,(c||qb!==m)&&f.updateCurrItem(),Da(\"mainScrollAnimComplete\")}),c&&f.updateCurrItem(!0),c},Vb=function(a){return 1/ca*a*t},Wb=function(){var a=s,b=Ta(),c=Ua();sc&&(a=c);var d,g=1,h=ja;return ia&&!S&&!ka&&s1||navigator.msMaxTouchPoints>1),f.likelyTouchDevice=G,r[A]=Ob,r[B]=Pb,r[C]=Rb,D&&(r[D]=r[C]),N.touch&&(q+=\" mousedown\",p+=\" mousemove mouseup\",r.mousedown=r[A],r.mousemove=r[B],r.mouseup=r[C]),G||(i.allowPanToNext=!1)}}});var Xb,Yb,Zb,$b,_b,ac,bc,cc=function(b,c,d,g){Xb&&clearTimeout(Xb),$b=!0,Zb=!0;var h;b.initialLayout?(h=b.initialLayout,b.initialLayout=null):h=i.getThumbBoundsFn&&i.getThumbBoundsFn(m);var j=d?i.hideAnimationDuration:i.showAnimationDuration,k=function(){ab(\"initialZoom\"),d?(f.template.removeAttribute(\"style\"),f.bg.removeAttribute(\"style\")):(Fa(1),c&&(c.style.display=\"block\"),e.addClass(a,\"pswp--animated-in\"),Da(\"initialZoom\"+(d?\"OutEnd\":\"InEnd\"))),g&&g(),$b=!1};if(!j||!h||void 0===h.x)return Da(\"initialZoom\"+(d?\"Out\":\"In\")),s=b.initialZoomLevel,Ma(pa,b.initialPosition),Ha(),a.style.opacity=d?0:1,Fa(1),void(j?setTimeout(function(){k()},j):k());var n=function(){var c=l,g=!f.currItem.src||f.currItem.loadError||i.showHideOpacity;b.miniImg&&(b.miniImg.style.webkitBackfaceVisibility=\"hidden\"),d||(s=h.w/b.w,pa.x=h.x,pa.y=h.y-K,f[g?\"template\":\"bg\"].style.opacity=.001,Ha()),bb(\"initialZoom\"),d&&!c&&e.removeClass(a,\"pswp--animated-in\"),g&&(d?e[(c?\"remove\":\"add\")+\"Class\"](a,\"pswp--animate_opacity\"):setTimeout(function(){e.addClass(a,\"pswp--animate_opacity\")},30)),Xb=setTimeout(function(){if(Da(\"initialZoom\"+(d?\"Out\":\"In\")),d){var f=h.w/b.w,i={x:pa.x,y:pa.y},l=s,m=ja,n=function(b){1===b?(s=f,pa.x=h.x,pa.y=h.y-M):(s=(f-l)*b+l,pa.x=(h.x-i.x)*b+i.x,pa.y=(h.y-M-i.y)*b+i.y),Ha(),g?a.style.opacity=1-b:Fa(m-b*m)};c?db(\"initialZoom\",0,1,j,e.easing.cubic.out,n,k):(n(1),Xb=setTimeout(k,j+20))}else s=b.initialZoomLevel,Ma(pa,b.initialPosition),Ha(),Fa(1),g?a.style.opacity=1:Fa(1),Xb=setTimeout(k,j+20)},d?25:90)};n()},dc={},ec=[],fc={index:0,errorMsg:'The image could not be loaded.',forceProgressiveLoading:!1,preload:[1,1],getNumItemsFn:function(){return Yb.length}},gc=function(){return{center:{x:0,y:0},max:{x:0,y:0},min:{x:0,y:0}}},hc=function(a,b,c){var d=a.bounds;d.center.x=Math.round((dc.x-b)/2),d.center.y=Math.round((dc.y-c)/2)+a.vGap.top,d.max.x=b>dc.x?Math.round(dc.x-b):d.center.x,d.max.y=c>dc.y?Math.round(dc.y-c)+a.vGap.top:d.center.y,d.min.x=b>dc.x?0:d.center.x,d.min.y=c>dc.y?a.vGap.top:d.center.y},ic=function(a,b,c){if(a.src&&!a.loadError){var d=!c;if(d&&(a.vGap||(a.vGap={top:0,bottom:0}),Da(\"parseVerticalMargin\",a)),dc.x=b.x,dc.y=b.y-a.vGap.top-a.vGap.bottom,d){var e=dc.x/a.w,f=dc.y/a.h;a.fitRatio=e1&&(c=1),a.initialZoomLevel=c,a.bounds||(a.bounds=gc())}if(!c)return;return hc(a,a.w*c,a.h*c),d&&c===a.initialZoomLevel&&(a.initialPosition=a.bounds.center),a.bounds}return a.w=a.h=0,a.initialZoomLevel=a.fitRatio=1,a.bounds=gc(),a.initialPosition=a.bounds.center,a.bounds},jc=function(a,b,c,d,e,g){b.loadError||d&&(b.imageAppended=!0,mc(b,d,b===f.currItem&&ya),c.appendChild(d),g&&setTimeout(function(){b&&b.loaded&&b.placeholder&&(b.placeholder.style.display=\"none\",b.placeholder=null)},500))},kc=function(a){a.loading=!0,a.loaded=!1;var b=a.img=e.createEl(\"pswp__img\",\"img\"),c=function(){a.loading=!1,a.loaded=!0,a.loadComplete?a.loadComplete(a):a.img=null,b.onload=b.onerror=null,b=null};return b.onload=c,b.onerror=function(){a.loadError=!0,c()},b.src=a.src,b},lc=function(a,b){if(a.src&&a.loadError&&a.container)return b&&(a.container.innerHTML=\"\"),a.container.innerHTML=i.errorMsg.replace(\"%url%\",a.src),!0},mc=function(a,b,c){if(a.src){b||(b=a.container.lastChild);var d=c?a.w:Math.round(a.w*a.fitRatio),e=c?a.h:Math.round(a.h*a.fitRatio);a.placeholder&&!a.loaded&&(a.placeholder.style.width=d+\"px\",a.placeholder.style.height=e+\"px\"),b.style.width=d+\"px\",b.style.height=e+\"px\"}},nc=function(){if(ec.length){for(var a,b=0;b=0,e=Math.min(c[0],ac()),g=Math.min(c[1],ac());for(b=1;b1200},setContent:function(a,b){i.loop&&(b=Aa(b));var c=f.getItemAt(a.index);c&&(c.container=null);var d,g=f.getItemAt(b);if(!g)return void(a.el.innerHTML=\"\");Da(\"gettingData\",b,g),a.index=b,a.item=g;var h=g.container=e.createEl(\"pswp__zoom-wrap\");if(!g.src&&g.html&&(g.html.tagName?h.appendChild(g.html):h.innerHTML=g.html),lc(g),ic(g,qa),!g.src||g.loadError||g.loaded)g.src&&!g.loadError&&(d=e.createEl(\"pswp__img\",\"img\"),d.style.opacity=1,d.src=g.src,mc(g,d),jc(b,g,h,d,!0));else{if(g.loadComplete=function(c){if(j){if(a&&a.index===b){if(lc(c,!0))return c.loadComplete=c.img=null,ic(c,qa),Ia(c),void(a.index===m&&f.updateCurrZoomItem());c.imageAppended?!$b&&c.placeholder&&(c.placeholder.style.display=\"none\",c.placeholder=null):N.transform&&(fa||$b)?ec.push({item:c,baseDiv:h,img:c.img,index:b,holder:a,clearPlaceholder:!0}):jc(b,c,h,c.img,fa||$b,!0)}c.loadComplete=null,c.img=null,Da(\"imageLoadComplete\",b,c)}},e.features.transform){var k=\"pswp__img pswp__img--placeholder\";k+=g.msrc?\"\":\" pswp__img--placeholder--blank\";var l=e.createEl(k,g.msrc?\"img\":\"\");g.msrc&&(l.src=g.msrc),mc(g,l),h.appendChild(l),g.placeholder=l}g.loading||kc(g),f.allowProgressiveImg()&&(!Zb&&N.transform?ec.push({item:g,baseDiv:h,img:g.img,index:b,holder:a}):jc(b,g,h,g.img,!0,!0))}Zb||b!==m?Ia(g):(ea=h.style,cc(g,d||g.img)),a.el.innerHTML=\"\",a.el.appendChild(h)},cleanSlide:function(a){a.img&&(a.img.onload=a.img.onerror=null),a.loaded=a.loading=a.img=a.imageAppended=!1}}});var oc,pc={},qc=function(a,b,c){var d=document.createEvent(\"CustomEvent\"),e={origEvent:a,target:a.target,releasePoint:b,pointerType:c||\"touch\"};d.initCustomEvent(\"pswpTap\",!0,!0,e),a.target.dispatchEvent(d)};za(\"Tap\",{publicMethods:{initTap:function(){Ca(\"firstTouchStart\",f.onTapStart),Ca(\"touchRelease\",f.onTapRelease),Ca(\"destroy\",function(){pc={},oc=null})},onTapStart:function(a){a.length>1&&(clearTimeout(oc),oc=null)},onTapRelease:function(a,b){if(b&&!Y&&!W&&!_a){var c=b;if(oc&&(clearTimeout(oc),oc=null,xb(c,pc)))return void Da(\"doubleTap\",c);if(\"mouse\"===b.type)return void qc(a,b,\"mouse\");var d=a.target.tagName.toUpperCase();if(\"BUTTON\"===d||e.hasClass(a.target,\"pswp__single-tap\"))return void qc(a,b);Ma(pc,c),oc=setTimeout(function(){qc(a,b),oc=null},300)}}}});var rc;za(\"DesktopZoom\",{publicMethods:{initDesktopZoom:function(){L||(G?Ca(\"mouseUsed\",function(){f.setupDesktopZoom()}):f.setupDesktopZoom(!0))},setupDesktopZoom:function(b){rc={};var c=\"wheel mousewheel DOMMouseScroll\";Ca(\"bindEvents\",function(){e.bind(a,c,f.handleMouseWheel)}),Ca(\"unbindEvents\",function(){rc&&e.unbind(a,c,f.handleMouseWheel)}),f.mouseZoomedIn=!1;var d,g=function(){f.mouseZoomedIn&&(e.removeClass(a,\"pswp--zoomed-in\"),f.mouseZoomedIn=!1),s"},{"title":"说说","date":"2020-04-01T13:38:18.006Z","updated":"2020-04-01T13:38:18.006Z","comments":false,"path":"shuoshuo/index.html","permalink":"https://qikaile.tk/shuoshuo/index.html","excerpt":"","text":"希望今天突然有人跳出来说，你被骗了，然后把2020年重启~ 2020年4月1日四月，水逆退散，一切好运！我一定会超级超级超级超级幸运的！冲鸭！ 2020年4月1日第一个说说 2020年4月1日"},{"title":"","date":"2020-04-01T12:44:46.357Z","updated":"2020-04-01T12:44:46.357Z","comments":true,"path":"shuoshuo/shuoshuo.css","permalink":"https://qikaile.tk/shuoshuo/shuoshuo.css","excerpt":"","text":"/* build time:Sat Apr 04 2020 16:46:49 GMT+0800 (GMT+08:00)*/ #shuoshuo_content{background-color:#fff;padding:10px;min-height:500px}body.theme-dark .cbp_tmtimeline::before{background:RGBA(255,255,255,.06)}ul.cbp_tmtimeline{padding:0}div class.cdp_tmlabel>li .cbp_tmlabel{margin-bottom:0}.cbp_tmtimeline{margin:30px 0 0 0;padding:0;list-style:none;position:relative}.cbp_tmtimeline:before{content:'';position:absolute;top:0;bottom:0;width:4px;background:RGBA(0,0,0,.02);left:80px;margin-left:10px}.cbp_tmtimeline>li .cbp_tmtime{display:block;max-width:70px;position:absolute}.cbp_tmtimeline>li .cbp_tmtime span{display:block;text-align:right}.cbp_tmtimeline>li .cbp_tmtime span:first-child{font-size:.9em;color:#bdd0db}.cbp_tmtimeline>li .cbp_tmtime span:last-child{font-size:1.2em;color:#9bcd9b}.cbp_tmtimeline>li:nth-child(odd) .cbp_tmtime span:last-child{color:RGBA(255,125,73,.75)}div.cbp_tmlabel>p{margin-bottom:0}.cbp_tmtimeline>li .cbp_tmlabel{margin:0 0 45px 65px;background:#9bcd9b;color:#fff;padding:.8em 1.2em .4em 1.2em;font-weight:300;line-height:1.4;position:relative;border-radius:5px;transition:all .3s ease 0s;box-shadow:0 1px 2px rgba(0,0,0,.15);cursor:pointer;display:block}.cbp_tmlabel:hover{transform:translateY(-3px);z-index:1;box-shadow:0 15px 32px rgba(0,0,0,.15)!important}.cbp_tmtimeline>li:nth-child(odd) .cbp_tmlabel{background:RGBA(255,125,73,.75)}.cbp_tmtimeline>li .cbp_tmlabel:after{right:100%;border:solid transparent;content:\" \";height:0;width:0;position:absolute;pointer-events:none;border-right-color:#9bcd9b;border-width:10px;top:4px}.cbp_tmtimeline>li:nth-child(odd) .cbp_tmlabel:after{border-right-color:RGBA(255,125,73,.75)}p.shuoshuo_time{margin-top:10px;border-top:1px dashed #fff;padding-top:5px}@media screen and (max-width:65.375em){.cbp_tmtimeline>li .cbp_tmtime span:last-child{font-size:1.2em}}.shuoshuo_author_img img{border:1px solid #ddd;padding:2px;float:left;border-radius:64px;transition:all 1s}.avatar{border-radius:100%!important;-moz-border-radius:100%!important;box-shadow:inset 0 -1px 0 3333sf;-webkit-box-shadow:inset 0 -1px 0 3333sf;-webkit-transition:.4s;-webkit-transition:-webkit-transform .4s ease-out;transition:transform .4s ease-out;-moz-transition:-moz-transform .4s ease-out}.zhuan{transform:rotateZ(720deg);-webkit-transform:rotateZ(720deg);-moz-transform:rotateZ(720deg)} /* rebuild by neat */"},{"title":"标签","date":"2020-03-14T06:25:44.000Z","updated":"2020-04-04T05:22:23.872Z","comments":false,"path":"tags/index.html","permalink":"https://qikaile.tk/tags/index.html","excerpt":"","text":""},{"title":"热门文章Top 10","date":"2020-03-16T01:26:32.000Z","updated":"2020-03-22T07:05:36.233Z","comments":false,"path":"top/index.html","permalink":"https://qikaile.tk/top/index.html","excerpt":"","text":""},{"title":"-润兰轩-","date":"2020-03-15T02:22:04.000Z","updated":"2020-03-31T12:26:59.441Z","comments":false,"path":"gallery/润兰轩/index.html","permalink":"https://qikaile.tk/gallery/%E6%B6%A6%E5%85%B0%E8%BD%A9/index.html","excerpt":"","text":""},{"title":"-怡情轩-","date":"2020-03-15T02:22:04.000Z","updated":"2020-03-31T12:29:05.106Z","comments":false,"path":"gallery/怡情轩/index.html","permalink":"https://qikaile.tk/gallery/%E6%80%A1%E6%83%85%E8%BD%A9/index.html","excerpt":"","text":""},{"title":"-翰宝阁-","date":"2020-03-15T02:22:04.000Z","updated":"2020-03-31T12:26:49.147Z","comments":false,"path":"gallery/翰宝阁/index.html","permalink":"https://qikaile.tk/gallery/%E7%BF%B0%E5%AE%9D%E9%98%81/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-02T09:36:32.866Z","updated":"2020-03-31T03:23:50.000Z","comments":true,"path":"photos/default-skin/default-skin.css","permalink":"https://qikaile.tk/photos/default-skin/default-skin.css","excerpt":"","text":"/* build time:Sat Apr 04 2020 16:46:49 GMT+0800 (GMT+08:00)*/ /*! PhotoSwipe Default UI CSS by Dmitry Semenov | photoswipe.com | MIT license */.pswp__button{width:44px;height:44px;position:relative;background:0 0;cursor:pointer;overflow:visible;-webkit-appearance:none;display:block;border:0;padding:0;margin:0;float:right;opacity:.75;-webkit-transition:opacity .2s;transition:opacity .2s;-webkit-box-shadow:none;box-shadow:none}.pswp__button:focus,.pswp__button:hover{opacity:1}.pswp__button:active{outline:0;opacity:.9}.pswp__button::-moz-focus-inner{padding:0;border:0}.pswp__ui--over-close .pswp__button--close{opacity:1}.pswp__button,.pswp__button--arrow--left:before,.pswp__button--arrow--right:before{background:url(default-skin.png) 0 0 no-repeat;background-size:264px 88px;width:44px;height:44px}@media (-webkit-min-device-pixel-ratio:1.1),(-webkit-min-device-pixel-ratio:1.09375),(min-resolution:105dpi),(min-resolution:1.1dppx){.pswp--svg .pswp__button,.pswp--svg .pswp__button--arrow--left:before,.pswp--svg .pswp__button--arrow--right:before{background-image:url(default-skin.svg)}.pswp--svg .pswp__button--arrow--left,.pswp--svg .pswp__button--arrow--right{background:0 0}}.pswp__button--close{background-position:0 -44px}.pswp__button--share{background-position:-44px -44px}.pswp__button--fs{display:none}.pswp--supports-fs .pswp__button--fs{display:block}.pswp--fs .pswp__button--fs{background-position:-44px 0}.pswp__button--zoom{display:none;background-position:-88px 0}.pswp--zoom-allowed .pswp__button--zoom{display:block}.pswp--zoomed-in .pswp__button--zoom{background-position:-132px 0}.pswp--touch .pswp__button--arrow--left,.pswp--touch .pswp__button--arrow--right{visibility:hidden}.pswp__button--arrow--left,.pswp__button--arrow--right{background:0 0;top:50%;margin-top:-50px;width:70px;height:100px;position:absolute}.pswp__button--arrow--left{left:0}.pswp__button--arrow--right{right:0}.pswp__button--arrow--left:before,.pswp__button--arrow--right:before{content:'';top:35px;background-color:rgba(0,0,0,.3);height:30px;width:32px;position:absolute}.pswp__button--arrow--left:before{left:6px;background-position:-138px -44px}.pswp__button--arrow--right:before{right:6px;background-position:-94px -44px}.pswp__counter,.pswp__share-modal{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.pswp__share-modal{display:block;background:rgba(0,0,0,.5);width:100%;height:100%;top:0;left:0;padding:10px;position:absolute;z-index:1600;opacity:0;-webkit-transition:opacity .25s ease-out;transition:opacity .25s ease-out;-webkit-backface-visibility:hidden;will-change:opacity}.pswp__share-modal--hidden{display:none}.pswp__share-tooltip{z-index:1620;position:absolute;background:#fff;top:56px;border-radius:2px;display:block;width:auto;right:44px;-webkit-box-shadow:0 2px 5px rgba(0,0,0,.25);box-shadow:0 2px 5px rgba(0,0,0,.25);-webkit-transform:translateY(6px);-ms-transform:translateY(6px);transform:translateY(6px);-webkit-transition:-webkit-transform .25s;transition:transform .25s;-webkit-backface-visibility:hidden;will-change:transform}.pswp__share-tooltip a{display:block;padding:8px 12px;color:#000;text-decoration:none;font-size:14px;line-height:18px}.pswp__share-tooltip a:hover{text-decoration:none;color:#000}.pswp__share-tooltip a:first-child{border-radius:2px 2px 0 0}.pswp__share-tooltip a:last-child{border-radius:0 0 2px 2px}.pswp__share-modal--fade-in{opacity:1}.pswp__share-modal--fade-in .pswp__share-tooltip{-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0)}.pswp--touch .pswp__share-tooltip a{padding:16px 12px}a.pswp__share--facebook:before{content:'';display:block;width:0;height:0;position:absolute;top:-12px;right:15px;border:6px solid transparent;border-bottom-color:#fff;-webkit-pointer-events:none;-moz-pointer-events:none;pointer-events:none}a.pswp__share--facebook:hover{background:#3e5c9a;color:#fff}a.pswp__share--facebook:hover:before{border-bottom-color:#3e5c9a}a.pswp__share--twitter:hover{background:#55acee;color:#fff}a.pswp__share--pinterest:hover{background:#ccc;color:#ce272d}a.pswp__share--download:hover{background:#ddd}.pswp__counter{position:absolute;left:0;top:0;height:44px;font-size:13px;line-height:44px;color:#fff;opacity:.75;padding:0 10px}.pswp__caption{position:absolute;left:0;bottom:0;width:100%;min-height:44px}.pswp__caption small{font-size:11px;color:#bbb}.pswp__caption__center{text-align:left;max-width:420px;margin:0 auto;font-size:13px;padding:10px;line-height:20px;color:#ccc}.pswp__caption--empty{display:none}.pswp__caption--fake{visibility:hidden}.pswp__preloader{width:44px;height:44px;position:absolute;top:0;left:50%;margin-left:-22px;opacity:0;-webkit-transition:opacity .25s ease-out;transition:opacity .25s ease-out;will-change:opacity;direction:ltr}.pswp__preloader__icn{width:20px;height:20px;margin:12px}.pswp__preloader--active{opacity:1}.pswp__preloader--active .pswp__preloader__icn{background:url(preloader.gif) 0 0 no-repeat}.pswp--css_animation .pswp__preloader--active{opacity:1}.pswp--css_animation .pswp__preloader--active .pswp__preloader__icn{-webkit-animation:clockwise .5s linear infinite;animation:clockwise .5s linear infinite}.pswp--css_animation .pswp__preloader--active .pswp__preloader__donut{-webkit-animation:donut-rotate 1s cubic-bezier(.4,0,.22,1) infinite;animation:donut-rotate 1s cubic-bezier(.4,0,.22,1) infinite}.pswp--css_animation .pswp__preloader__icn{background:0 0;opacity:.75;width:14px;height:14px;position:absolute;left:15px;top:15px;margin:0}.pswp--css_animation .pswp__preloader__cut{position:relative;width:7px;height:14px;overflow:hidden}.pswp--css_animation .pswp__preloader__donut{-webkit-box-sizing:border-box;box-sizing:border-box;width:14px;height:14px;border:2px solid #fff;border-radius:50%;border-left-color:transparent;border-bottom-color:transparent;position:absolute;top:0;left:0;background:0 0;margin:0}@media screen and (max-width:1024px){.pswp__preloader{position:relative;left:auto;top:auto;margin:0;float:right}}@-webkit-keyframes clockwise{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes clockwise{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@-webkit-keyframes donut-rotate{0%{-webkit-transform:rotate(0);transform:rotate(0)}50%{-webkit-transform:rotate(-140deg);transform:rotate(-140deg)}100%{-webkit-transform:rotate(0);transform:rotate(0)}}@keyframes donut-rotate{0%{-webkit-transform:rotate(0);transform:rotate(0)}50%{-webkit-transform:rotate(-140deg);transform:rotate(-140deg)}100%{-webkit-transform:rotate(0);transform:rotate(0)}}.pswp__ui{-webkit-font-smoothing:auto;visibility:visible;opacity:1;z-index:1550}.pswp__top-bar{position:absolute;left:0;top:0;height:44px;width:100%}.pswp--has_mouse .pswp__button--arrow--left,.pswp--has_mouse .pswp__button--arrow--right,.pswp__caption,.pswp__top-bar{-webkit-backface-visibility:hidden;will-change:opacity;-webkit-transition:opacity 333ms cubic-bezier(.4,0,.22,1);transition:opacity 333ms cubic-bezier(.4,0,.22,1)}.pswp--has_mouse .pswp__button--arrow--left,.pswp--has_mouse .pswp__button--arrow--right{visibility:visible}.pswp__caption,.pswp__top-bar{background-color:rgba(0,0,0,.5)}.pswp__ui--fit .pswp__caption,.pswp__ui--fit .pswp__top-bar{background-color:rgba(0,0,0,.3)}.pswp__ui--idle .pswp__top-bar{opacity:0}.pswp__ui--idle .pswp__button--arrow--left,.pswp__ui--idle .pswp__button--arrow--right{opacity:0}.pswp__ui--hidden .pswp__button--arrow--left,.pswp__ui--hidden .pswp__button--arrow--right,.pswp__ui--hidden .pswp__caption,.pswp__ui--hidden .pswp__top-bar{opacity:.001}.pswp__ui--one-slide .pswp__button--arrow--left,.pswp__ui--one-slide .pswp__button--arrow--right,.pswp__ui--one-slide .pswp__counter{display:none}.pswp__element--disabled{display:none!important}.pswp--minimal--dark .pswp__top-bar{background:0 0} /* rebuild by neat */"}],"posts":[{"title":"PS功能精通课","slug":"PS功能精通课","date":"2020-03-27T16:00:00.000Z","updated":"2020-04-04T08:34:26.719Z","comments":true,"path":"archives/20b03a28d.html","link":"","permalink":"https://qikaile.tk/archives/20b03a28d.html","excerpt":"","text":"我用双手成就你的梦想课程结构精通14节+提升22节+实战36节交作业M站：m.dapengjiaoyu.com上传图片格式：JPG单张作业不可大过2M、最多上传9张、9张不可大过9M认识软件美国Adobe公司开发 中文：奥多比PS：位图的处理软件位图：图片缩小拉大之后图像会失真、色彩表现力好，文件大PS的使用方向：摄影后期、效果图的后期处理、图像合成、电影海报、电商、网页设计、UI设计、等等…很多方面，较为广泛Ai：矢量图的处理软件矢量图：图片缩小拉大之后图像不会失真、色彩表现不如ps细腻、层次感不强、文件小Ai的使用方向：印刷（名片）、包装盒的设计制作、企业VI手册设计（企业形象识别系统）、LOGO设计等ID：专业书籍排版软件ID的使用方向：杂志、广告设计、目录、零售商设计工作室和报纸出版方案等PS基本操作1.新建画布（快捷键Ctrl+N）①分辨率：打印时用300分辨率(大尺寸设计稿分比率会根据具体情况变小)；屏幕显示时用72分辨率②颜色模式：显示时用RGB RGB代表红绿蓝​ 打印时用CMYK CMYK代表青、品红、黄、黑2.打开（在工作区域外）①文件——打开（快捷键Ctrl+O）②拖拽打开：在计算机里选中需要打开的文件点击并拖拽至ps图标上，然后移动到菜单栏或属性栏3.置入（在工作区域内）：拖拽到画布中，需确定置入命令，可直接按回车Enter确定4.保存（快捷键Ctrl+S）：格式psd留给设计师自己的源文件可再次修改， Jpg图片格式 不可修改， png透底图 背景透明5.另存为：快捷键Ctrl+Shift+S6.存储为web所用格式：快捷键Ctrl+Shift+Alt+S（可以更改素材质量大小，优化文件大小）7.调整ps：编辑—首选项—暂存盘 快捷键Ctrl+K勾选所有硬盘可缓解卡顿快捷键整理新建画布：Ctrl+N打开：Ctrl+O保存：Ctrl+S另存为：Ctrl+Shift+S存储为web所用格式：Ctrl+Shift+Alt+S揭开ps的神秘面纱画布操作1.放大缩小画布：Alt+鼠标滚轮 或 Ctrl+加减键 或缩放工具（Alt缩小）2.百分之百显示：Ctrl+1 根据显示器显示：Ctrl+0（零）3.移动画板：按住空格键可临时切换为抓手工具鼠标左键拖拽即可4.撤销一步：Ctrl+Z 撤销多步：Ctrl+Alt+Z 还原多步：Ctrl+Shift+Z5.自由变换：Ctrl+T①普通变换：通过对角点拉伸，鼠标放在对角（出现弯箭头）旋转。按Alt（中心点）和Shift（等比）以中心点等比例缩放图层基本操作1.解锁背景图层：点击图层后方锁头解锁2.显示隐藏图层：点击该图层前的眼睛图标3.删除图层：选中图层按delete键删除 或选中图层后点击下方垃圾桶图标 或选中图层拖拽至垃圾桶图标上4.创建新图层：点击垃圾桶左侧的图标（快捷键Ctrl+Alt+Shift+N）5.上色（前景色与背景色，需要自行添加新图层）6.恢复默认前景色和背景色（黑白）：按D7.切换前景色与背景色：点击弯箭头或按X填充前景色：Alt+Delete填充背景色：Ctrl+Delete移动工具（快捷键V）移动工具可以移动，可以复制，还可以跨画布复制对齐：必须有两个及以上图层分布：必须有三个及以上图层快速选择图层：移动工具下按住ctrl可以临时切换到自动选择模式，同时按住Shift可以进行加选或减选画笔（快捷键B）1.画笔加号情况：可能开了大写或画笔过大2.画笔工具用于涂抹前景色3.画笔大小更改：P后面【】 画笔硬度更改：Shift+P后面【】快捷键整理适应显示器显示：Ctrl+0百分百显示：Ctrl+1撤销：Ctrl+Z自由变换：Ctrl+T还原多步：Ctrl+Shift+Z撤销多步：Ctrl+Alt+Z画笔（快捷键：B）新建图层：Ctrl+Alt+Shift+N填充前景色：Alt+Delete填充背景色：Ctrl+Delete成为大神前的第一步图层操作1.复制图层①移动工具下，按住Alt点击并拖拽可以复制，按住Shift可以控制复制的图层水平或垂直方向平移②原位复制，快捷键Ctrl+J2.编组：Ctrl+G 取消编组：Ctrl+Shift+G3.合并图层：Ctrl+E（尽量少用）4.分离图像：Ctrl+Shift+J把选中的内容剪切出来并放在原位选框工具（快捷键M，切换工具Shift+M）1.选框：流动的虚线（蚂蚁线）2.绘制方法：点击并拖拽（点击鼠标不要松开）按住Shift绘制正图形选区，按住Alt以鼠标为中心绘制选区，按Shift+Alt以鼠标为中心绘制正图形选区3.取消选区：Ctrl+D4.羽化：制作边缘虚化的效果属性栏处进行羽化时选择羽化后填充（选框绘画之前）羽化需要选择—修改—羽化快捷键Shift+F6进行羽化(选框画完之后)5.载入选区：按住Ctrl点击该图层的图层缩览图选区的操作1.新选区：新绘制的选区每次都形成一个新选区2.添加到选区：新绘制的选区与之前绘制的选区进行相加（快捷方法先按住Shift再绘制新选区）3.从选区减去：新绘制的选区与之前绘制的选区进行相减（快捷方法先按住Alt再绘制新选区）4.与选区交叉：新绘制的选区与之前绘制的选区相交部分留下（快捷方法先按住Shift+Alt再绘制新选区）5.绘制时移动：绘制时按住空格键可以移动选区快捷键整理编组：Ctrl+G（原位）复制：Ctrl+J合并图层：Ctrl+E取消编组：Ctrl+Shift+G填充背景色：Ctrl+Delete填充前景色：Alt+Delete取消选区：Ctrl+D分离图像：Ctrl+Shift+J羽化：Shift+F6神奇的布尔运算矢量工具（快捷键U，切换工具Shift+U）1.形状模式：在绘制过程中会自动新建图层，默认自动填充前景色2.颜色填充：纯色填充、渐变填充、图案填充3.图形描边：纯色填充、渐变填充、图案填充；描边大小；描边选项4.图形大小：属性栏处可以精确调整大小或Ctrl+T5.图形绘制：按住Shift可以绘制正图形，按住Alt键可以以鼠标为中心点绘制图形，按住Shift+Alt可以以鼠标为中心点绘制正图形6.圆角矩形：绘制的时候先设置半径，高版本可以在属性栏中修改(CS版本的就不好意思啦，不能在属性栏调整，因为没有这个功能，哈哈哈！！！)7.多边形：绘制的时候先设置多边形的边数与平滑星星等8.直线：绘制的时候先改变线的粗细，按住shift可以成角度约束9.自定形状：软件预设好的形状，方便使用，还可以追加。10.自定形状追加：设置—全部—追加11.定义自定形状：选择想要定义的图层右击选择定义自定形状（必须有路径的图层）12.布尔运算（通过形状层的加、减、交得到新的图形）：同选区操作13.矢量图形计算后：必须要合并形状组件小黑小白1.小黑（A）：移动和复制路径，单独选中图形2.小白（A）：选择和移动路径的上锚点，以及调节控制手柄，按住Shift可以加选锚点渐变工具在属性栏处选择一种渐变类型，并设置渐变颜色和其他属性等，创建渐变快捷键整理填充前景色：Alt+Delete填充背景色：Ctrl+Delete分离图像：Ctrl+Shift+J取消选区：Ctrl+D祖传抠图技法套索工具1.套索工具：大致框选，不适合精确抠图2.多边形套索工具：适合抠有棱角的图片，直线（回车可快速成选区）3.磁性套索工具：具有磁性，可以识别物体边缘（边缘清晰），操作发生偏移可以通过Delete进行点的删除（回车可快速成选区）a 宽度：该值决定了以光标中心为基准，其周围有多少个像素能够被工具检测到。边界清晰时数值高b 对比度：设置工具感应图像边缘的灵敏度，图像清晰时数值高c 频率：决定产生的锚点数量。数值越高，捕捉的边界越准确快速选择工具（属于画笔类）1.调成大小：P后面的【】可以调节画笔大小2.可以移动十字光标，快速连续选择相近的图像，会自动识别边缘（创建选框）3.选区的反向选择：Ctrl+Shift+I魔棒工具1.可以创建选区，选择颜色相近的范围2.容差值越大，选择颜色相似的范围越大，5~35之间3.不勾选连续时，主体物与背景颜色相近时，主体物也会选中4.魔棒工具抠图适用情况：背景是纯色，或背景与主体物颜色差距大色彩范围1.选择—色彩范围 选择改为取样颜色 图形下面选择选择范围2.根据图像的颜色范围，进行创建选区白色为被选中的，黑色没被选中的，灰色透明的橡皮擦工具1.橡皮擦工具：直接擦涂，删掉不需要的图像2.魔术橡皮擦：直接删掉颜色相近的区域，容差值与魔棒相同其他辅助操作1.图层顺序：下/上移一层图层 Ctrl+【】​ 置底/置顶图层 Ctrl+Shift+【】2.收缩（选择—修改）：在原有基础上进行缩小选区快捷键整理反向选则：Ctrl+Shift+I下/上移动图层：Ctrl+【】置底/置顶：Ctrl+Shift+【】高效修图法污点修复画笔1.调节大小：P后面的【】进行调节2.类型：内容识别（常用）/创建纹理/近似匹配3.内容识别：点击需要修复的区域。软件会自动在他的周围进行取样，通过计算对其进行光线和明暗的匹配，并进行羽化融合4.创建纹理：可以创建纹理，纹理为ps自带不可修改5.近似匹配：使用工具边缘的像素来修补图像 扩散数值为画笔附近几像素的范围。（可以自动调节明暗）修复画笔工具1.调节大小：P后面的【】进行调节2.取样：在需要修复的区域四周，找到颜色相似的区域，按住Alt键，鼠标点击进行取样，然后在需要修复的区域点击或涂抹，（在修复时，修复画笔尽量要比修复的区域大，否则，修复效果不是很好。）3.对齐：勾选对齐后吸取点跟随修复点移动，不勾选每次单击修复都是用同一吸取点去修复4.图案：直接涂抹即可，不需要取样，类似图案叠加修补工具1.源：选区位置被鼠标停留位置覆盖2.目标：选区位置覆盖鼠标停留位置内容感知移动工具可以移动画面当中物体的位置，移动 之后可以自动填充。可以在需要修改的位置绘制选区，移动选区到画布外，留一小部分选区再画布当中，来用于修补水印红眼工具可以修复相机在光线昏暗的情况下，产生的红眼效果，点击红眼部位，会自动修复。（了解即可）仿制图章1.使用方法同修复画笔一致2.仿制图章工具与修复画笔工具的区别：①仿制图章是无损仿制，取样什么颜色/皮肤，仿制的就是什么样子②修复画笔有一个运算过程，在涂抹当中将取样图像和目标位置融合，自动适应周围环境图案图章工具选择图案可以涂背景，类似图案添加液化（快捷键：Ctrl+Shift+X）如果液化点不开或者灰色的，首选项-性能-使用图形处理器勾选上1.位置：滤镜—液化2.向前变形：可以制作瘦身瘦脸效果3.重建工具：可以恢复之前的变形4.顺时针旋转扭曲工具：按住alt键点击可以逆时针旋转5.褶皱工具（挤压、褶皱效果）6.膨胀工具（与褶皱工具相反）7.左推工具（从上往下是往左推，从下往上是往右推）8.冻结蒙版工具（保护图层）9.解冻蒙版工具（取消冻结的蒙版）10.人脸识别11.移动工具、放大缩小内容识别（快捷键：Shift+F5）通过绘制选区选择 需要修复的区域，软件会自动识别与画面不匹配的区域，进行修复图像快捷键整理液化 Ctrl+Shift+X内容识别 Shift+F5玩转钢笔钢笔工具（快捷键P）1.钢笔工具：①绘制直线的方法：在起始点位置点击定点，连续点击，按住Shift键，可以绘制成角度的直线②绘制曲线的方法：在起始点位置点击定点，在下一点处点击并拖拽鼠标，拉出弧线，会出现控制手柄，再一次绘制时，需要按住Alt键取消一侧手柄③自动添加删除：可以直接在路径上点击添加锚点或者点击锚点删除锚点④临时切换：按住Ctrl键可以临时切换到小白工具进行锚点移动（自带控制手柄，可以调节弧度大小）⑤将路径转换为选区：右击，选择建立选区、或Ctrl+Enter回车、或在路径面板下，Ctrl+路径缩览图⑥Delete键删除最后一个锚点的同时会结束钢笔工具这一次路径的绘制2.自由钢笔工具：点击拖拽鼠标可以画出流畅的线条路径。右击路径，选择画笔勾选模拟压力（需先设置好画笔大小、硬度等）3.转换点工具：点击曲线位置的点，可以将其变成直线。点击直线位置的点，选中并拖拽，可以出现控制手柄，调节弧度路径面板1.路径面板可以实现选区与路径的互相转换2.储存为jpg,psd时，路径面板可以储存路径，类似图层，便于抠图便于工作画笔（快捷键：B）1.载入画笔：设置中找到载入画笔，找到画笔点击载入，右键可以删除画笔2.画笔面板（快捷键F5）：形状动态、散布、颜色动态3.定义画笔预设：编辑—定义画笔预设 画笔只认黑白灰，黑（实色颜色）、白（没有颜色）、灰（半透明）多元化的文字文字工具（推荐：www.qiuziti.com来找字体）1.横排文字蒙版（直排文字蒙版）工具：点击就会出现红色蒙版，输入文字确定后不会新建图层，并且文字会变为选区2.横排文字（竖排文字）工具：点击会自动新建文字图层，可以再属性栏处更改文字属性3.确定文字输入：属性栏的对勾 或Ctrl+Enter回车 或小键盘下的Enter4.全选：Ctrl+A或双击文字图层缩览图5.调节字间距：Alt+左右箭头6.调节行间距：Alt+上下箭头7.点文字：不会自动换行，换行需要手动回车进行换行，适合做标题文字8.段文字（区域文字）：在画布上点击并拖拽拉出文本框，会自动换行，文字溢出时下方有加号提示，适合做说明文字9.路径文字：用钢笔或者形状工具，绘制一段路径，将文字工具的光标放在路径上，点击输入文字。用小白调节文字形态图层样式+图层混合模式混合模式（27个）1.使用要求：必须两个或两个以上的图层才能进行混合2.混合模式分组：A.组合模式：需要降低图层的不透明度才能产生作用B.加深混合组：可以使图像变暗，将下方图层中的亮色被上方较暗的像素替代C.减淡混合组：与加深混合组相反，可以使图像变亮，将下方图层中的暗色被上方较亮的像素替代D.对比混合组：50%的灰色完全消失，高于50%灰的像素会使底图变亮，低于50%灰的像素会使底图变暗E.比较混合组：相同的区域显示为黑色，不同的区域显示为灰度层次或彩色。当图层中包含白色，白色区域会使底层图像反相，而黑色不会对底层图像产生影响。F.色彩混合组：将色彩的色相、饱和度和亮度，替换给下方图层3.重要的混合模式选项（4个）①加深混合组：正片叠底（去白留黑）②减淡混合组：滤色（去黑留白）③比较混合组：叠加，使你的颜色跟下方图层进行有机的的叠加，同时修改下方图层的本身的亮度和明暗程度，比较柔和的效果④柔光 ，效果更好，画面更融合图层样式1.添加图层样式：①双击图层缩览图的后方，弹出对话框②点击图层面板下方Fx按钮，添加图层样式③图层菜单中选择④在画布区域右击弹出混合选项 选择（移动工具、抓手工具、放大镜工具不可）2.复制图层样式：按住Alt键点击图层样式Fx进行拖拽到需要复制的图层或在图层上右击鼠标选择拷贝图层样式 在需要复制的图层上右击选择粘贴图层样式3.填充：可以将颜色降低透明度，图层样式不变蒙版带你领略台前幕后的故事快速蒙版（快捷键Q）快速蒙版是一种选区工具 结合画笔工具使用，常用与影楼。双击快速蒙版，可以更改快速蒙版建立的选区形式剪贴蒙版（上图下形）1.原理是将上层图层置于下层图层内，他们必须是上下层关系2.下方图层可以是形状、图层、画笔、文字、智能对象3.上图层右击选择创建剪贴蒙版，或按住Alt键，在上下图层之间移动，出现方框带箭头形状，单击鼠标左键，或Ctrl+Alt+G 创建/释放4.剪切蒙版可以同时多个图层 进行剪贴蒙版图层蒙版（黑隐藏白显示灰色半透明）1.蒙版颜色表示的意义：黑色：隐藏图像、白色：显示图像、灰色半透明 蒙版只认黑白灰，除了黑白其他颜色都是不同程度的灰2.可以在蒙版上添加颜色的方式：画笔、渐变、填充等3.暂停蒙版使用：按住Shift点击图层蒙版缩览图4.使用蒙版时容易出现的问题：①在使用时出现涂抹颜色的情况，多数是没有添加或选择蒙版缩览图。②在使用蒙版时，涂抹无效果，看下当前前景色是否是白色③在使用蒙版时，涂抹无效果，看下画笔的透明度或流量是否是1%通道1.作用：用于储存颜色信息，相当于颜色银行2.第一个通道为复合通道，不同的通道会显示不同的颜色信息3.单色通道中黑白灰的意义：白色表示颜色值最高255，黑色0，灰色0-2554.Alpha通道的作用：可以储存和制作选区，黑色非选区，白色选区，灰色半透明选区快捷键整理创建/释放剪贴蒙版 Ctrl+Alt+G打造滤镜下的艺术效果滤镜1.转换为智能滤镜：可以将普通的位图转为智能对象2.滤镜使用规则：▼ RGB模式下滤镜都可以使用▼ CMYK Lab 模式下有部分滤镜不能使用▼ 索引模式下滤镜不能使用.3.智能滤镜的优点：自带蒙版，可编辑性强，可以对滤镜的效果单独进行多次修改或调整4.上次滤镜操作：快捷键Ctrl+F可以再次执行上次的滤镜操作5.渐隐：快捷键Ctrl+Shift+F 编辑—渐隐 对普通图层滤镜效果再编辑，可以调整不透明度和混合模式（不常用）6.图像中有选区时，滤镜效果只对选区内有效，没有选区时，对整体图像有效7.滤镜库：里面有现成的滤镜效果，可以通过调成参数从而改变滤镜的效果，通过下方新建按钮可以创建多个滤镜效果8.滤镜库—素描：多个滤镜应用前/背景色，亮部应用背景色，暗部应用前景色9.▷ 自适应广角：可较正广角导致的变形图像▷ camera raw滤镜：可以调整图像颜色等▷ 镜头较正：可制作鱼眼镜头拍摄产生的效果，可以对照片的畸变和暗角进行一定程度的矫正▷ 液化：可通过平移、旋转、进行像素变形（瘦身瘦脸等）▷ 消失点：可以通过内置透视网格、进行图像修图（透视：近大远小效果）▷ 模糊系列：可以根据参数对图像进行各种形式整体模糊处理。局部模糊可以通过选框进行控制。▷ 扭曲系列：根据不同方式对图像进行整体像素变形▷ 锐化系列：对图像进行整体边缘对比强化，使图片整体更加清晰。如果参数过大图像会损坏。▷ 杂色：添加杂色：可以添加颗粒杂色▷ 其他滤镜：高反差保留：可以保留细节与叠加柔光一起使用10.渲染—云彩：可以应用没有像素的区域（空白图层），应用的是前/背景色快捷键整理重复上次滤镜 Ctrl+Alt＋F低版本重复上次滤镜 Ctrl＋F渐隐 Ctrl+Shift+F让我们换一种颜色看世界（调色一）色彩模式1.RGB：光学三原色，也是调色运用最多的一种颜色模式2.CMYK：印刷用的颜色 青、洋红、黄、黑3.灰度模式：图像不包含颜色，只有黑白灰三种颜色，并影响之后的颜色使用4.去色（Ctrl+Shift+U ）：把图像的饱和度降到最低，不影响色彩模式，对于之后的颜色使用没有影响5.更改模式：菜单栏—图像—模式调色1.调整面板：点击效果，直接新建图层，自带图层蒙版，可以多次调解，只对下方图层起作用2.亮度/对比度：亮度、添加/减少图像明暗程度​ 对比度、增加/降低图像明暗对比程度3.色相/饱和度Ctrl+U ：色相、色彩的相貌​ 饱和度、颜色鲜艳程度​ 明度、颜色的明暗程度​ 单色制作时勾选着色4.三原色：红绿蓝 间色：原色+原色=间色 黄、洋红、青互补色（反色）：可以互相抵消的颜色、180°的颜色、相对的颜色三对互补色：红色与青色 蓝色与黄色 绿色与洋红5.色彩平衡（Ctrl+B ）：可以根据颜色的色相来调节6.渐变映射：一般结合混合模式和不透明度来使用 通俗的说就是用你所设定的颜色，对应到原图的色彩上。用这种效果可以达到某些非常夸张的色彩搭配效果。7.可选颜色：对单一颜色进行调整“相对”比较柔和 “绝对”比较犀利 相对运用的较多8.替换颜色：图像—调整—替换颜色 拾取一种颜色，选择另一种颜色替换（可以用添加吸管添加颜色）快捷键整理去色 Ctrl+Shift+U从昏暗到光明（调色二）色阶（快捷键Ctrl+L）1.输入色阶——数字图像本来的色阶范围，通过调节改变黑白灰范围输出色阶——是指为打印机指定最小的暗调色阶和最大的高光色阶，调整时调整的是整体的明暗度2.可以调整图像的阴影、中间调和高光的强度级别，校正色调范围和色彩平衡曲线（快捷键Ctrl+M）它整合了“色阶”、“亮度/对比度”等多个命令的功能。曲线上可以添加14个控点，移动这些控制点可以对色彩和色调进行非常精确的调整a 按Shift 点击可以选中并控制多个控制点b 点击Delete键可以删掉控制点照片滤镜可用于矫正照片的颜色颜色查找查询颜色，形成滤镜效果快捷键总结","categories":[{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.tk/categories/photoshop/"}],"tags":[{"name":"photoshop","slug":"photoshop","permalink":"https://qikaile.tk/tags/photoshop/"}]},{"title":"Pandas中文手册","slug":"pandas中文手册","date":"2020-03-27T16:00:00.000Z","updated":"2020-04-04T08:34:00.367Z","comments":true,"path":"archives/20b03m28d.html","link":"","permalink":"https://qikaile.tk/archives/20b03m28d.html","excerpt":"","text":"如果你想学习Pandas，建议先看两个网站。（1）官网：Python Data Analysis Library（2）十分钟入门Pandas：10 Minutes to pandas关键缩写和包导入在这个速查手册中，我们使用如下缩写：df：任意的Pandas DataFrame对象s：任意的Pandas Series对象同时我们需要做如下的引入：import pandas as pdimport numpy as np导入数据pd.read_csv(filename)：从CSV文件导入数据pd.read_table(filename)：从限定分隔符的文本文件导入数据pd.read_excel(filename)：从Excel文件导入数据pd.read_sql(query, connection_object)：从SQL表/库导入数据pd.read_json(json_string)：从JSON格式的字符串导入数据pd.read_html(url)：解析URL、字符串或者HTML文件，抽取其中的tables表格pd.read_clipboard()：从你的粘贴板获取内容，并传给read_table()pd.DataFrame(dict)：从字典对象导入数据，Key是列名，Value是数据导出数据df.to_csv(filename)：导出数据到CSV文件df.to_excel(filename)：导出数据到Excel文件df.to_sql(table_name, connection_object)：导出数据到SQL表df.to_json(filename)：以Json格式导出数据到文本文件创建测试对象pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象pd.Series(my_list)：从可迭代对象my_list创建一个Series对象df.index = pd.date_range(‘1900/1/30’, periods=df.shape[0])：增加一个日期索引查看、检查数据df.head(n)：查看DataFrame对象的前n行df.tail(n)：查看DataFrame对象的最后n行df.shape()：查看行数和列数http://df.info()：查看索引、数据类型和内存信息df.describe()：查看数值型列的汇总统计s.value_counts(dropna=False)：查看Series对象的唯一值和计数df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数数据选取df[col]：根据列名，并以Series的形式返回列df[[col1, col2]]：以DataFrame形式返回多列s.iloc[0]：按位置选取数据s.loc[‘index_one’]：按索引选取数据df.iloc[0,:]：返回第一行df.iloc[0,0]：返回第一列的第一个元素数据清理df.columns = [‘a’,’b’,’c’]：重命名列名pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组df.dropna()：删除所有包含空值的行df.dropna(axis=1)：删除所有包含空值的列df.dropna(axis=1,thresh=n)：删除所有小于n个非空值的行df.fillna(x)：用x替换DataFrame对象中所有的空值s.astype(float)：将Series中的数据类型更改为float类型s.replace(1,’one’)：用‘one’代替所有等于1的值s.replace([1,3],[‘one’,’three’])：用’one’代替1，用’three’代替3df.rename(columns=lambda x: x + 1)：批量更改列名df.rename(columns={‘old_name’: ‘new_ name’})：选择性更改列名df.set_index(‘column_one’)：更改索引列df.rename(index=lambda x: x + 1)：批量重命名索引数据处理：Filter 、Sort 和 GroupBydf[df[col] &gt; 0.5]：选择col列的值大于0.5的行df.sort_values(col1)：按照列col1排序数据，默认升序排列df.sort_values(col2, ascending=False)：按照列col1降序排列数据df.sort_values([col1,col2], ascending=[True,False])：先按列col1升序排列，后按col2降序排列数据df.groupby(col)：返回一个按列col进行分组的Groupby对象df.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象df.groupby(col1)[col2]：返回按列col1进行分组后，列col2的均值df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值data.apply(np.mean)：对DataFrame中的每一列应用函数np.meandata.apply(np.max,axis=1)：对DataFrame中的每一行应用函数np.max数据合并df1.append(df2)：将df2中的行添加到df1的尾部df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部df1.join(df2,on=col1,how=’inner’)：对df1的列和df2的列执行SQL形式的join数据统计df.describe()：查看数据值列的汇总统计df.mean()：返回所有列的均值df.corr()：返回列与列之间的相关系数df.count()：返回每一列中的非空值的个数df.max()：返回每一列的最大值df.min()：返回每一列的最小值df.median()：返回每一列的中位数df.std()：返回每一列的标准差","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://qikaile.tk/categories/Pandas/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://qikaile.tk/tags/pandas/"}]},{"title":"十分钟搞定 pandas","slug":"十分钟搞定pandas","date":"2020-03-27T16:00:00.000Z","updated":"2020-04-04T08:35:09.041Z","comments":true,"path":"archives/20b03c28d.html","link":"","permalink":"https://qikaile.tk/archives/20b03c28d.html","excerpt":"","text":"官方网站上《10 Minutes to pandas》的一个简单的翻译，原文在这里。这篇文章是对 pandas 的一个简单的介绍，详细的介绍请参考：秘籍 。习惯上，我们会按下面格式引入所需要的包：12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt一、 创建对象可以通过 数据结构入门 来查看有关该节内容的详细信息。1、可以通过传递一个list对象来创建一个Series，pandas 会默认创建整型索引：1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float642、通过传递一个 numpyarray，时间索引以及列标签来创建一个DataFrame：12345678910111213141516171819In [6]: dates = pd.date_range('20130101', periods=6)In [7]: datesOut[7]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.5249883、通过传递一个能够被转换成类似序列结构的字典对象来创建一个DataFrame：123456789101112131415In [10]: df2 = pd.DataFrame(&#123; 'A' : 1., ....: 'B' : pd.Timestamp('20130102'), ....: 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), ....: 'D' : np.array([3] * 4,dtype='int32'), ....: 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), ....: 'F' : 'foo' &#125;) ....: In [11]: df2Out[11]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo4、查看不同列的数据类型：123456789In [12]: df2.dtypesOut[12]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object5、如果你使用的是 IPython，使用 Tab 自动补全功能会自动识别所有的属性以及自定义的列，下图中是所有能够被自动识别的属性的一个子集：123456789101112131415161718192021222324In [13]: df2.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D二、 查看数据详情请参阅：基础。1、 查看DataFrame中头部和尾部的行：123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.5249882、 显示索引、列和底层的 numpy 数据：1234567891011121314151617In [16]: df.indexOut[16]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [17]: df.columnsOut[17]: Index([u'A', u'B', u'C', u'D'], dtype='object')In [18]: df.valuesOut[18]: array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])3、 describe()函数对于数据的快速统计汇总：1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.0718044、 对数据的转置：1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.5249885、 按轴进行排序123456789In [21]: df.sort_index(axis=1, ascending=False)Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.6736906、 按值进行排序123456789In [22]: df.sort_values(by='B')Out[22]: A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401三、 选择虽然标准的 Python/Numpy 的选择和设置表达式都能够直接派上用场，但是作为工程使用的代码，我们推荐使用经过优化的 pandas 数据访问方式： .at, .iat, .loc, .iloc 和 .ix。详情请参阅索引和选取数据 和 多重索引/高级索引。获取1、 选择一个单独的列，这将会返回一个Series，等同于df.A：123456789In [23]: df['A']Out[23]: 2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float642、 通过[]进行选择，这将会对行进行切片12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df['20130102':'20130104']Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860通过标签选择1、 使用标签来获取一个交叉的区域1234567In [26]: df.loc[dates[0]]Out[26]: A 0.469112B -0.282863C -1.509059D -1.135632Name: 2013-01-01 00:00:00, dtype: float642、 通过标签来在多个轴上进行选择123456789In [27]: df.loc[:,['A','B']]Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.1136483、 标签切片123456In [28]: df.loc['20130102':'20130104',['A','B']]Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067714、 对于返回的对象进行维度缩减12345In [29]: df.loc['20130102',['A','B']]Out[29]: A 1.212112B -0.173215Name: 2013-01-02 00:00:00, dtype: float645、 获取一个标量12In [30]: df.loc[dates[0],'A']Out[30]: 0.469112299907186286、 快速访问一个标量（与上一个方法等价）12In [31]: df.at[dates[0],'A']Out[31]: 0.46911229990718628通过位置选择1、 通过传递数值进行位置选择（选择的是行）1234567In [32]: df.iloc[3]Out[32]: A 0.721555B -0.706771C -1.039575D 0.271860Name: 2013-01-04 00:00:00, dtype: float642、 通过数值进行切片，与 numpy/python 中的情况类似12345In [33]: df.iloc[3:5,0:2]Out[33]: A B2013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670203、 通过指定一个位置的列表，与 numpy/python 中的情况类似123456In [34]: df.iloc[[1,2,4],[0,2]]Out[34]: A C2013-01-02 1.212112 0.1192092013-01-03 -0.861849 -0.4949292013-01-05 -0.424972 0.2762324、 对行进行切片12345In [35]: df.iloc[1:3,:]Out[35]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718045、 对列进行切片123456789In [36]: df.iloc[:,1:3]Out[36]: B C2013-01-01 -0.282863 -1.5090592013-01-02 -0.173215 0.1192092013-01-03 -2.104569 -0.4949292013-01-04 -0.706771 -1.0395752013-01-05 0.567020 0.2762322013-01-06 0.113648 -1.4784276、 获取特定的值12In [37]: df.iloc[1,1]Out[37]: -0.17321464905330858快速访问标量（等同于前一个方法）：12In [38]: df.iat[1,1]Out[38]: -0.17321464905330858布尔索引1、 使用一个单独列的值来选择数据：123456In [39]: df[df.A &gt; 0]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-04 0.721555 -0.706771 -1.039575 0.2718602、 使用where操作来选择数据：123456789In [40]: df[df &gt; 0]Out[40]: A B C D2013-01-01 0.469112 NaN NaN NaN2013-01-02 1.212112 NaN 0.119209 NaN2013-01-03 NaN NaN NaN 1.0718042013-01-04 0.721555 NaN NaN 0.2718602013-01-05 NaN 0.567020 0.276232 NaN2013-01-06 NaN 0.113648 NaN 0.5249883、 使用isin()方法来过滤：12345678910111213141516171819In [41]: df2 = df.copy()In [42]: df2['E'] = ['one', 'one','two','three','four','three']In [43]: df2Out[43]: A B C D E2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four2013-01-06 -0.673690 0.113648 -1.478427 0.524988 threeIn [44]: df2[df2['E'].isin(['two','four'])]Out[44]: A B C D E2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four设置1、 设置一个新的列：12345678910111213In [45]: s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))In [46]: s1Out[46]: 2013-01-02 12013-01-03 22013-01-04 32013-01-05 42013-01-06 52013-01-07 6Freq: D, dtype: int64In [47]: df['F'] = s12、 通过标签设置新的值：1In [48]: df.at[dates[0],'A'] = 03、 通过位置设置新的值：1In [49]: df.iat[0,1] = 04、 通过一个numpy数组设置一组新值：1In [50]: df.loc[:,'D'] = np.array([5] * len(df))上述操作结果如下：123456789In [51]: dfOut[51]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 0.119209 5 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.02013-01-04 0.721555 -0.706771 -1.039575 5 3.02013-01-05 -0.424972 0.567020 0.276232 5 4.02013-01-06 -0.673690 0.113648 -1.478427 5 5.05、 通过where操作来设置新的值：12345678910111213In [52]: df2 = df.copy()In [53]: df2[df2 &gt; 0] = -df2In [54]: df2Out[54]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 -5 NaN2013-01-02 -1.212112 -0.173215 -0.119209 -5 -1.02013-01-03 -0.861849 -2.104569 -0.494929 -5 -2.02013-01-04 -0.721555 -0.706771 -1.039575 -5 -3.02013-01-05 -0.424972 -0.567020 -0.276232 -5 -4.02013-01-06 -0.673690 -0.113648 -1.478427 -5 -5.0四、 缺失值处理在 pandas 中，使用np.nan来代替缺失值，这些值将默认不会包含在计算中，详情请参阅：缺失的数据。1、 reindex()方法可以对指定轴上的索引进行改变/增加/删除操作，这将返回原始数据的一个拷贝：1234567891011In [55]: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])In [56]: df1.loc[dates[0]:dates[1],'E'] = 1In [57]: df1Out[57]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 NaN 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 NaN2013-01-04 0.721555 -0.706771 -1.039575 5 3.0 NaN2、 去掉包含缺失值的行：1234In [58]: df1.dropna(how='any')Out[58]: A B C D F E2013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.03、 对缺失值进行填充：1234567In [59]: df1.fillna(value=5)Out[59]: A B C D F E2013-01-01 0.000000 0.000000 -1.509059 5 5.0 1.02013-01-02 1.212112 -0.173215 0.119209 5 1.0 1.02013-01-03 -0.861849 -2.104569 -0.494929 5 2.0 5.02013-01-04 0.721555 -0.706771 -1.039575 5 3.0 5.04、 对数据进行布尔填充：1234567n [60]: pd.isnull(df1)Out[60]: A B C D F E2013-01-01 False False False False True False2013-01-02 False False False False False False2013-01-03 False False False False False True2013-01-04 False False False False False True五、 相关操作详情请参与 基本的二进制操作统计（相关操作通常情况下不包括缺失值）1、 执行描述性统计：12345678In [61]: df.mean()Out[61]: A -0.004474B -0.383981C -0.687758D 5.000000F 3.000000dtype: float642、 在其他轴上进行相同的操作：123456789In [62]: df.mean(1)Out[62]: 2013-01-01 0.8727352013-01-02 1.4316212013-01-03 0.7077312013-01-04 1.3950422013-01-05 1.8836562013-01-06 1.592306Freq: D, dtype: float643、 对于拥有不同维度，需要对齐的对象进行操作。Pandas 会自动的沿着指定的维度进行广播：123456789101112131415161718192021In [63]: s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)In [64]: sOut[64]: 2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64In [65]: df.sub(s, axis='index')Out[65]: A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.861849 -3.104569 -1.494929 4.0 1.02013-01-04 -2.278445 -3.706771 -4.039575 2.0 0.02013-01-05 -5.424972 -4.432980 -4.723768 0.0 -1.02013-01-06 NaN NaN NaN NaN NaNApply1、 对数据应用函数：123456789101112131415161718In [66]: df.apply(np.cumsum)Out[66]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 -1.389850 10 1.02013-01-03 0.350263 -2.277784 -1.884779 15 3.02013-01-04 1.071818 -2.984555 -2.924354 20 6.02013-01-05 0.646846 -2.417535 -2.648122 25 10.02013-01-06 -0.026844 -2.303886 -4.126549 30 15.0In [67]: df.apply(lambda x: x.max() - x.min())Out[67]: A 2.073961B 2.671590C 1.785291D 0.000000F 4.000000dtype: float64直方图具体请参照：直方图和离散化。1234567891011121314151617181920212223In [68]: s = pd.Series(np.random.randint(0, 7, size=10))In [69]: sOut[69]: 0 41 22 13 24 65 46 47 68 49 4dtype: int64In [70]: s.value_counts()Out[70]: 4 56 22 21 1dtype: int64字符串方法Series对象在其str属性中配备了一组字符串处理方法，可以很容易的应用到数组中的每个元素，如下段代码所示。更多详情请参考：字符串向量化方法。1234567891011121314In [71]: s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])In [72]: s.str.lower()Out[72]: 0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object六、 合并Pandas 提供了大量的方法能够轻松的对Series，DataFrame和Panel对象进行各种符合各种逻辑关系的合并操作。具体请参阅：合并。Concat1234567891011121314151617181920212223242526272829303132In [73]: df = pd.DataFrame(np.random.randn(10, 4))In [74]: dfOut[74]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495# break it into piecesIn [75]: pieces = [df[:3], df[3:7], df[7:]]In [76]: pd.concat(pieces)Out[76]: 0 1 2 30 -0.548702 1.467327 -1.015962 -0.4830751 1.637550 -1.217659 -0.291519 -1.7455052 -0.263952 0.991460 -0.919069 0.2660463 -0.709661 1.669052 1.037882 -1.7057754 -0.919854 -0.042379 1.247642 -0.0099205 0.290213 0.495767 0.362949 1.5481066 -1.131345 -0.089329 0.337863 -0.9458677 -0.932132 1.956030 0.017587 -0.0166928 -0.575247 0.254161 -1.143704 0.2158979 1.193555 -0.077118 -0.408530 -0.862495Join类似于 SQL 类型的合并，具体请参阅：数据库风格的连接1234567891011121314151617181920212223In [77]: left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)In [78]: right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)In [79]: leftOut[79]: key lval0 foo 11 foo 2In [80]: rightOut[80]: key rval0 foo 41 foo 5In [81]: pd.merge(left, right, on='key')Out[81]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5另一个例子：123456789101112131415161718192021In [82]: left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;)In [83]: right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;)In [84]: leftOut[84]: key lval0 foo 11 bar 2In [85]: rightOut[85]: key rval0 foo 41 bar 5In [86]: pd.merge(left, right, on='key')Out[86]: key lval rval0 foo 1 41 bar 2 5Append将一行连接到一个DataFrame上，具体请参阅附加：12345678910111213141516171819202122232425262728In [87]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])In [88]: dfOut[88]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.708758In [89]: s = df.iloc[3]In [90]: df.append(s, ignore_index=True)Out[90]: A B C D0 1.346061 1.511763 1.627081 -0.9905821 -0.441652 1.211526 0.268520 0.0245802 -1.577585 0.396823 -0.105381 -0.5325323 1.453749 1.208843 -0.080952 -0.2646104 -0.727965 -0.589346 0.339969 -0.6932055 -0.339355 0.593616 0.884345 1.5914316 0.141809 0.220390 0.435589 0.1924517 -0.096701 0.803351 1.715071 -0.7087588 1.453749 1.208843 -0.080952 -0.264610七、 分组对于”group by”操作，我们通常是指以下一个或多个操作步骤：（Splitting）按照一些规则将数据分为不同的组；（Applying）对于每组数据分别执行一个函数；（Combining）将结果组合到一个数据结构中；详情请参阅：_Grouping section_12345678910111213141516171819In [91]: df = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', ....: 'foo', 'bar', 'foo', 'foo'], ....: 'B' : ['one', 'one', 'two', 'three', ....: 'two', 'two', 'one', 'three'], ....: 'C' : np.random.randn(8), ....: 'D' : np.random.randn(8)&#125;) ....: In [92]: dfOut[92]: A B C D0 foo one -1.202872 -0.0552241 bar one -1.814470 2.3959852 foo two 1.018601 1.5528253 bar three -0.595447 0.1665994 foo two 1.395433 0.0476095 bar two -0.392670 -0.1364736 foo one 0.007207 -0.5617577 foo three 1.928123 -1.6230331、 分组并对每个分组执行sum函数：123456In [93]: df.groupby('A').sum()Out[93]: C DA bar -2.802588 2.42611foo 3.146492 -0.639582、 通过多个列进行分组形成一个层次索引，然后执行函数：12345678910In [94]: df.groupby(['A','B']).sum()Out[94]: C DA B bar one -1.814470 2.395985 three -0.595447 0.166599 two -0.392670 -0.136473foo one -1.195665 -0.616981 three 1.928123 -1.623033 two 2.414034 1.600434八、 改变形状详情请参阅 层次索引 和 改变形状。Stack1234567891011121314151617181920In [95]: tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', ....: 'foo', 'foo', 'qux', 'qux'], ....: ['one', 'two', 'one', 'two', ....: 'one', 'two', 'one', 'two']])) ....: In [96]: index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])In [97]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])In [98]: df2 = df[:4]In [99]: df2Out[99]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.1002301234567891011121314In [100]: stacked = df2.stack()In [101]: stackedOut[101]: first second bar one A 0.029399 B -0.542108 two A 0.282696 B -0.087302baz one A -1.575170 B 1.771208 two A 0.816482 B 1.100230dtype: float641234567891011121314151617181920212223242526In [102]: stacked.unstack()Out[102]: A Bfirst second bar one 0.029399 -0.542108 two 0.282696 -0.087302baz one -1.575170 1.771208 two 0.816482 1.100230In [103]: stacked.unstack(1)Out[103]: second one twofirst bar A 0.029399 0.282696 B -0.542108 -0.087302baz A -1.575170 0.816482 B 1.771208 1.100230In [104]: stacked.unstack(0)Out[104]: first bar bazsecond one A 0.029399 -1.575170 B -0.542108 1.771208two A 0.282696 0.816482 B -0.087302 1.100230数据透视表详情请参阅：数据透视表.12345678910111213141516171819202122In [105]: df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, .....: 'B' : ['A', 'B', 'C'] * 4, .....: 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, .....: 'D' : np.random.randn(12), .....: 'E' : np.random.randn(12)&#125;) .....: In [106]: dfOut[106]: A B C D E0 one A foo 1.418757 -0.1796661 one B foo -1.879024 1.2918362 two C foo 0.536826 -0.0096143 three A bar 1.006160 0.3921494 one B bar -0.029716 0.2645995 one C bar -1.146178 -0.0574096 two A foo 0.100900 -1.4256387 three B foo -1.035018 1.0240988 one C foo 0.314665 -0.1060629 one A bar -0.773723 1.82437510 two B bar -1.170653 0.59597411 three C bar 0.648740 1.167115可以从这个数据中轻松的生成数据透视表：12345678910111213In [107]: pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])Out[107]: C bar fooA B one A -0.773723 1.418757 B -0.029716 -1.879024 C -1.146178 0.314665three A 1.006160 NaN B NaN -1.035018 C 0.648740 NaNtwo A NaN 0.100900 B -1.170653 NaN C NaN 0.536826九、 时间序列Pandas 在对频率转换进行重新采样时拥有简单、强大且高效的功能（如将按秒采样的数据转换为按5分钟为单位进行采样的数据）。这种操作在金融领域非常常见。具体参考：时间序列。12345678In [108]: rng = pd.date_range('1/1/2012', periods=100, freq='S')In [109]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)In [110]: ts.resample('5Min').sum()Out[110]: 2012-01-01 25083Freq: 5T, dtype: int641、 时区表示：1234567891011121314151617181920212223In [111]: rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')In [112]: ts = pd.Series(np.random.randn(len(rng)), rng)In [113]: tsOut[113]: 2012-03-06 0.4640002012-03-07 0.2273712012-03-08 -0.4969222012-03-09 0.3063892012-03-10 -2.290613Freq: D, dtype: float64In [114]: ts_utc = ts.tz_localize('UTC')In [115]: ts_utcOut[115]: 2012-03-06 00:00:00+00:00 0.4640002012-03-07 00:00:00+00:00 0.2273712012-03-08 00:00:00+00:00 -0.4969222012-03-09 00:00:00+00:00 0.3063892012-03-10 00:00:00+00:00 -2.290613Freq: D, dtype: float642、 时区转换：12345678In [116]: ts_utc.tz_convert('US/Eastern')Out[116]: 2012-03-05 19:00:00-05:00 0.4640002012-03-06 19:00:00-05:00 0.2273712012-03-07 19:00:00-05:00 -0.4969222012-03-08 19:00:00-05:00 0.3063892012-03-09 19:00:00-05:00 -2.290613Freq: D, dtype: float643、 时间跨度转换：1234567891011121314151617181920212223242526272829303132In [117]: rng = pd.date_range('1/1/2012', periods=5, freq='M')In [118]: ts = pd.Series(np.random.randn(len(rng)), index=rng)In [119]: tsOut[119]: 2012-01-31 -1.1346232012-02-29 -1.5618192012-03-31 -0.2608382012-04-30 0.2819572012-05-31 1.523962Freq: M, dtype: float64In [120]: ps = ts.to_period()In [121]: psOut[121]: 2012-01 -1.1346232012-02 -1.5618192012-03 -0.2608382012-04 0.2819572012-05 1.523962Freq: M, dtype: float64In [122]: ps.to_timestamp()Out[122]: 2012-01-01 -1.1346232012-02-01 -1.5618192012-03-01 -0.2608382012-04-01 0.2819572012-05-01 1.523962Freq: MS, dtype: float644、 时期和时间戳之间的转换使得可以使用一些方便的算术函数。1234567891011121314In [123]: prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')In [124]: ts = pd.Series(np.random.randn(len(prng)), prng)In [125]: ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9In [126]: ts.head()Out[126]: 1990-03-01 09:00 -0.9029371990-06-01 09:00 0.0681591990-09-01 09:00 -0.0578731990-12-01 09:00 -0.3682041991-03-01 09:00 -1.144073Freq: H, dtype: float64十、 Categorical从 0.15 版本开始，pandas 可以在DataFrame中支持 Categorical 类型的数据，详细 介绍参看：Categorical 简介和_API documentation_。1In [127]: df = pd.DataFrame(&#123;\"id\":[1,2,3,4,5,6], \"raw_grade\":['a', 'b', 'b', 'a', 'a', 'e']&#125;)1、 将原始的grade转换为 Categorical 数据类型：123456789101112In [128]: df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")In [129]: df[\"grade\"]Out[129]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e]2、 将 Categorical 类型数据重命名为更有意义的名称：1In [130]: df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]3、 对类别进行重新排序，增加缺失的类别：123456789101112In [131]: df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])In [132]: df[\"grade\"]Out[132]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good]4、 排序是按照 Categorical 的顺序进行的而不是按照字典顺序进行：123456789In [133]: df.sort_values(by=\"grade\")Out[133]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good5、 对 Categorical 列进行排序时存在空的类别：123456789In [134]: df.groupby(\"grade\").size()Out[134]: gradevery bad 1bad 0medium 0good 2very good 3dtype: int64十一、 画图具体文档参看：绘图文档。123456In [135]: ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))In [136]: ts = ts.cumsum()In [137]: ts.plot()Out[137]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff2ab2af550&gt;对于DataFrame来说，plot是一种将所有列及其标签进行绘制的简便方法：12345678In [138]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, .....: columns=['A', 'B', 'C', 'D']) .....: In [139]: df = df.cumsum()In [140]: plt.figure(); df.plot(); plt.legend(loc='best')Out[140]: &lt;matplotlib.legend.Legend at 0x7ff29c8163d0&gt;十二、 导入和保存数据CSV参考：写入 CSV 文件。1、 写入 csv 文件：1In [141]: df.to_csv('foo.csv')2、 从 csv 文件中读取：1234567891011121314151617181920In [142]: pd.read_csv('foo.csv')Out[142]: Unnamed: 0 A B C D0 2000-01-01 0.266457 -0.399641 -0.219582 1.1868601 2000-01-02 -1.170732 -0.345873 1.653061 -0.2829532 2000-01-03 -1.734933 0.530468 2.060811 -0.5155363 2000-01-04 -1.555121 1.452620 0.239859 -1.1568964 2000-01-05 0.578117 0.511371 0.103552 -2.4282025 2000-01-06 0.478344 0.449933 -0.741620 -1.9624096 2000-01-07 1.235339 -0.091757 -1.543861 -1.084753.. ... ... ... ... ...993 2002-09-20 -10.628548 -9.153563 -7.883146 28.313940994 2002-09-21 -10.390377 -8.727491 -6.399645 30.914107995 2002-09-22 -8.985362 -8.485624 -4.669462 31.367740996 2002-09-23 -9.558560 -8.781216 -4.499815 30.518439997 2002-09-24 -9.902058 -9.340490 -4.386639 30.105593998 2002-09-25 -10.216020 -9.480682 -3.933802 29.758560999 2002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 5 columns]HDF5参考：HDF5 存储1、 写入 HDF5 存储：1In [143]: df.to_hdf('foo.h5','df')2、 从 HDF5 存储中读取：1234567891011121314151617181920In [144]: pd.read_hdf('foo.h5','df')Out[144]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns]Excel参考：_MS Excel_1、 写入excel文件：1In [145]: df.to_excel('foo.xlsx', sheet_name='Sheet1')2、 从excel文件中读取：1234567891011121314151617181920In [146]: pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])Out[146]: A B C D2000-01-01 0.266457 -0.399641 -0.219582 1.1868602000-01-02 -1.170732 -0.345873 1.653061 -0.2829532000-01-03 -1.734933 0.530468 2.060811 -0.5155362000-01-04 -1.555121 1.452620 0.239859 -1.1568962000-01-05 0.578117 0.511371 0.103552 -2.4282022000-01-06 0.478344 0.449933 -0.741620 -1.9624092000-01-07 1.235339 -0.091757 -1.543861 -1.084753... ... ... ... ...2002-09-20 -10.628548 -9.153563 -7.883146 28.3139402002-09-21 -10.390377 -8.727491 -6.399645 30.9141072002-09-22 -8.985362 -8.485624 -4.669462 31.3677402002-09-23 -9.558560 -8.781216 -4.499815 30.5184392002-09-24 -9.902058 -9.340490 -4.386639 30.1055932002-09-25 -10.216020 -9.480682 -3.933802 29.7585602002-09-26 -11.856774 -10.671012 -3.216025 29.369368[1000 rows x 4 columns]十三、陷阱如果你尝试某个操作并且看到如下异常：12345&gt;&gt;&gt; if pd.Series([False, True, False]): print(\"I was true\")Traceback ...ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().解释及处理方式请见比较。同时请见陷阱。","categories":[{"name":"pandas","slug":"pandas","permalink":"https://qikaile.tk/categories/pandas/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://qikaile.tk/tags/pandas/"}]},{"title":"机器学习课程","slug":"机器学习课程","date":"2020-03-27T16:00:00.000Z","updated":"2020-04-04T08:34:46.259Z","comments":true,"path":"archives/20b03b19d.html","link":"","permalink":"https://qikaile.tk/archives/20b03b19d.html","excerpt":"","text":"机器学习概述什么是机器学习机器学习是从数据中自动分析获得规律(模型)，并利用规律对未知数据进行预测。机器学习算法分类监督学习目标值：类别—分类问题KNN、贝叶斯分类、决策树与随机森林、逻辑回归目标值：连续型数据—回归问题线性回归、岭回归无监督学习目标值：无聚类K-means机器学习开发流程1）获取数据2）数据处理3）特征工程4）机器学习算法训练—模型5）模型评估6）应用特征工程数据集数据———数据集的构成———特征值 + 目标值可用数据集Kaggle特点：1、大数据竞赛平台 2、80万科学家 3、真实数据 4、数据量巨大UCI特点：1、收录了360个数据集 2、覆盖科学、生活、经济等领域 3、数据量几十万scikit-learn特点：1、数据量较小 2、方便学习网址：Kaggle网址：https://www.kaggle.com/datasetsUCI数据集网址： http://archive.ics.uci.edu/ml/scikit-learn网址：http://scikit-learn.org/stable/datasets/index.html#datasetssklearn数据集load_*小规模的数据集fetch_*大规模的数据集Bunch类型数据集划分—model_selection.train_test_split()训练数据：用于训练，构建模型测试数据：在模型检验时使用，用于评估模型是否有效特征工程介绍特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的模型准确性。sklearn 特征工程Scikit-learn包含的内容：分类、聚类、回归、特征工程、模型选择和调优。pandas 数据清洗、数据处理特征处理是特征工程的核心部分，包括特征提取、数据预处理、特征选择、特征降维等。特征提取特征提取包括将任意数据（如文本或图像）转换为可用于机器学习的数字特征。注：特征值化是为了计算机更好的去理解数据包：sklearn.feature_extraction字典特征提取应用DictVectorizer实现对类别特征进行数值化、离散化sklearn.feature_extraction.DictVectorizer(sparse=True,…)DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式DictVectorizer.get_feature_names() 返回类别名称应用：代码：12345678910#对字典类型的数据进行特征抽取from sklearn.feature_extraction import DictVectorizerdata = [&#123;'city': '北京','temperature':100&#125;, &#123;'city': '上海','temperature':60&#125;, &#123;'city': '深圳','temperature':30&#125;]# 1、实例化一个转换器类transfer = DictVectorizer(sparse=False)# 2、调用fit_transformdata = transfer.fit_transform(data)print(\"返回的结果:\\n\", data)# 打印特征名字print(\"特征名字：\\n\", transfer.get_feature_names())输出结果：123456返回的结果: [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]]特征名字： ['city=上海', 'city=北京', 'city=深圳', 'temperature']文本特征提取独热编码（One-HotEncoding）应用CountVectorizer实现对文本特征进行数值化应用TfidfVectorizer(TF-IDF)实现对文本特征进行数值化sklearn.feature_extraction.text.CountVectorizer(stop_words=[])返回词频矩阵CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格CountVectorizer.get_feature_names() 返回值:单词列表sklearn.feature_extraction.text.TfidfVectorizerTF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。12345678from sklearn.feature_extraction.text import TfidfVectorizer# 对于文本数据，进行特征抽取tf = TfidfVectorizer()x_train = tf.fit_transform(x_train)#20类新闻分类数据集#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表print(tf.get_feature_names())# print(x_train.toarray())x_test = tf.transform(x_test) # 不需要fit_transform应用代码：1234567891011121314151617181920212223from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerimport jiebadef cut_word(text): # 用jieba对中文字符串进行分词 text = \" \".join(list(jieba.cut(text))) return text#对中文进行特征抽取data = [\"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\", \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\", \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\", \"life is short,i like like python\", \"life is too long,i dislike python\"]# 将原始数据转换成分好词的形式text_list = []for sent in data: text_list.append(cut_word(sent))print(text_list)# 1、实例化一个转换器类transfer = CountVectorizer()#transfer = TfidfVectorizer()# 2、调用fit_transformdata = transfer.fit_transform(text_list)print(\"文本特征抽取的结果：\\n\", data.toarray())print(\"返回特征名字：\\n\", transfer.get_feature_names())输出结果：1234567891011121314['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。', 'life is short , i like like python', 'life is too long , i dislike python']文本特征抽取的结果： [[0 0 0 0 0 0 0 0 2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1] [0 0 0 0 0 0 0 0 1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0] [0 1 1 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]返回特征名字： ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too', '一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']图像特征提取（深度学习将介绍）数据预处理去除唯一属性唯一属性通常是一些id属性，这些属性并不能刻画样本自身的分布规律，所以简单地删除这些属性即可。处理缺失值缺失值处理的三种方法：直接使用含有缺失值的特征；删除含有缺失值的特征（该方法在包含缺失值的属性含有大量缺失值而仅仅包含极少量有效值时是有效的）；缺失值补全。常见的缺失值补全方法：均值插补、同类均值插补、建模预测、高维映射、多重插补、极大似然估计、压缩感知和矩阵补全。（1）均值插补如果样本属性的距离是可度量的，则使用该属性有效值的平均值来插补缺失的值；如果的距离是不可度量的，则使用该属性有效值的众数来插补缺失的值。如果使用众数插补，出现数据倾斜会造成什么影响？（2）同类均值插补首先将样本进行分类，然后以该类中样本的均值来插补缺失值。（3）建模预测将缺失的属性作为预测目标来预测，将数据集按照是否含有特定属性的缺失值分为两类，利用现有的机器学习算法对待预测数据集的缺失值进行预测。该方法的根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。（4）高维映射将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含K个离散取值范围的属性值扩展为K+1个属性值，若该属性值缺失，则扩展后的第K+1个属性值置为1。这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。（5）多重插补（MultipleImputation，MI）多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。（6）压缩感知和矩阵补全（7）手动插补插补处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。特征编码（1）标签处理通常我们会把字符型的标签转换成数值型的代码：123456789101112131415161718import pandas as pddf = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']])df.columns = ['color', 'size', 'prize', 'class label']#标签处理class_mapping = &#123;label:idx for idx,label in enumerate(set(df['class label']))&#125;df['class label'] = df['class label'].map(class_mapping)print(df)print('-----------------------------------')size_mapping = &#123; 'XL': 3, 'L': 2, 'M': 1&#125;df['size'] = df['size'].map(size_mapping)print(df)输出结果:123456789 color size prize class label0 green M 10.1 11 red L 13.5 02 blue XL 15.3 1----------------------------------- color size prize class label0 green 1 10.1 11 red 2 13.5 02 blue 3 15.3 1（2）二值化二值化的过程是将数值型的属性转换为布尔值的属性，设定一个阈值作为划分属性值为0和1的分隔点。使用preproccessing库的Binarizer类对数据进行二值化的代码如下：123from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(X) #X=iris.data（鸢尾花）数据集（3）scikit DictVectorizer代码：123456789101112131415161718192021222324252627import pandas as pddf = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']])df.columns = ['color', 'size', 'prize', 'class label']print(df)print('----------------------------------------------------------------------')#print(df.transpose().to_dict().values())#print('----------------------------------------------------------------------')feature = df.iloc[:, :-1]print(feature)print('----------------------------------------------------------------------')# ②对于x转换成字典数据feature=feature.to_dict(orient=\"records\")#feature=feature.transpose().to_dict().values() #对所有的数据都做了映射#使用 DictVectorizer将得到特征的字典from sklearn.feature_extraction import DictVectorizerdvec = DictVectorizer(sparse=False)X = dvec.fit_transform(feature)print(dvec.get_feature_names())print('----------------------------------------------------------------------')print(X)print('----------------------------------------------------------------------')#可以调用 get_feature_names 来返回新的列的名字，其中0和1就代表是不是这个属性.data=pd.DataFrame(X, columns=dvec.get_feature_names())print(data)输出结果:1234567891011121314151617181920 color size prize class label0 green M 10.1 class11 red L 13.5 class22 blue XL 15.3 class1---------------------------------------------------------------------- color size prize0 green M 10.11 red L 13.52 blue XL 15.3----------------------------------------------------------------------['color=blue', 'color=green', 'color=red', 'prize', 'size=L', 'size=M', 'size=XL']----------------------------------------------------------------------[[ 0. 1. 0. 10.1 0. 1. 0. ] [ 0. 0. 1. 13.5 1. 0. 0. ] [ 1. 0. 0. 15.3 0. 0. 1. ]]---------------------------------------------------------------------- color=blue color=green color=red prize size=L size=M size=XL0 0.0 1.0 0.0 10.1 0.0 1.0 0.01 0.0 0.0 1.0 13.5 1.0 0.0 0.02 1.0 0.0 0.0 15.3 0.0 0.0 1.0（4）独热编码（One-HotEncoding）独热编码采用N位状态寄存器来对N个可能的取值进行编码，每个状态都由独立的寄存器来表示，并且在任意时刻只有其中一位有效。独热编码的优点：能够处理非数值属性；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。代码：12345678910111213141516import pandas as pddf = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']])df.columns = ['color', 'size', 'prize', 'class label']#OneHotEncoder 必须使用整数作为输入，所以得先使用scikit LabelEncoder处理一下from sklearn.preprocessing import LabelEncoderclass_le = LabelEncoder()df['class label'] = class_le.fit_transform(df['class label'])print(df)print('-----------------------------------')from sklearn.preprocessing import OneHotEncoderohe = OneHotEncoder(sparse=False)X = ohe.fit_transform(df[['color']].values)print(X)输出结果:12345678 color size prize class label0 green M 10.1 01 red L 13.5 12 blue XL 15.3 0-----------------------------------[[0. 1. 0.] [0. 0. 1.] [1. 0. 0.]]注：Pandas库中同样有类似的操作，使用get_dummies也可以得到相应的特征（5）pandas get_dummies代码：12345678import pandas as pddf = pd.DataFrame([ ['green',10.1], ['red', 13.5], ['blue',15.3]])df.columns = ['color', 'prize']df=pd.get_dummies(df)print(df)输出结果:1234 prize color_blue color_green color_red0 10.1 0 1 01 13.5 0 0 12 15.3 1 0 0无量纲化、正则化无量纲化数据标准化是将样本的属性缩放到某个指定的范围。标准化：基于原始数据的均值（mean）和标准差（standarddeviation）进行数据的标准化。要求 均值$\\mu = 0$ 和标准差 $\\sigma = 1$公式表达为：\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation}使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：1234from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据ss=StandardScaler()X1=ss.fit_transform(X) #X=iris.data（鸢尾花）数据集归一化：处理后的所有特征的值都会被压缩到 0到1区间上.这样做还可以抑制离群值对结果的影响.公式表达为：{\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}}使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据mms=MinMaxScaler()X2=mms.fit_transform(X) #X=iris.data（鸢尾花）数据集正则化正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。该方法主要应用于文本分类和聚类中。123from sklearn.preprocessing import Normalizerss=Normalizer()X3=ss.fit_transform(X) #X=iris.data（鸢尾花）数据集标准化与正则化的区别简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。正则化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。特征选择特征选择：数据中包含冗余或相关变量（或特征、属性、指标），旨在从原有特征中找出主要特征。包：sklearn.feature_selection当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除移除低方差法外，本文介绍的其他方法均从相关性考虑。根据特征选择的形式又可以将特征选择方法分为3种：FilterFilter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。方差选择法：低方差特征过滤代码：12345from sklearn.feature_selection import VarianceThresholdX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]sel = VarianceThreshold(threshold=(.8 * (1 - .8)))X1=sel.fit_transform(X)print(X1)输出结果:123456[[0 1] [1 0] [0 0] [1 1] [1 0] [1 1]]果然, VarianceThreshold 移除了第一列特征，第一列中特征值为0的概率达到了5/6.相关系数：特征与特征之间的相关程度（与目标相关性高的特征，应当优选选择）对于分类问题(y离散)，可采用：卡方检验，f_classif, mutual_info_classif，互信息对于回归问题(y连续)，可采用：皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数卡方(Chi2)检验​ 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：\\chi^{2}=\\sum \\frac{(A-E)^{2}}{E}假设有两个分类变量X和Y，它们的值域分别为{x1, x2}和{y1, y2}，其样本频数列联表为：经典的卡方检验是检验定性自变量对定性因变量的相关性，针对分类问题。比如，我们可以对样本进行一次chi2测试来选择最佳的两项特征：代码：12345678from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2iris = load_iris()X, y = iris.data, iris.targetprint(X.shape)X_new = SelectKBest(chi2, k=2).fit_transform(X, y)print(X_new.shape)输出结果:12(150, 4)(150, 2)Pearson相关系数 (Pearson Correlation)​ 皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。​ Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的 pearsonr 方法能够同时计算相关系数和p-value.代码：12345678import numpy as npfrom scipy.stats import pearsonrnp.random.seed(0)size = 300x = np.random.normal(0, 1, size)# pearsonr(x, y)的输入为特征矩阵和目标向量print(\"Lower noise\", pearsonr(x, x + np.random.normal(0, 1, size)))print(\"Higher noise\", pearsonr(x, x + np.random.normal(0, 10, size)))输出结果：123#输出为二元组(sorce, p-value)的数组Lower noise (0.7182483686213842, 7.324017312997672e-49)Higher noise (0.05796429207933815, 0.31700993885325246)这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。实例分析：股票的财务指标相关性计算分析两两特征之间进行相关性计算代码：12345678import numpy as npfrom scipy.stats import pearsonrimport pandas as pddata = pd.read_csv(\"factor_returns.csv\")factor = ['pe_ratio', 'pb_ratio', 'market_cap', 'return_on_asset_net_profit', 'du_return_on_equity', 'ev','earnings_per_share', 'revenue', 'total_expense']for i in range(len(factor)): for j in range(i, len(factor) - 1): print(\"指标%s与指标%s之间的相关性大小为%f\" % (factor[i], factor[j + 1], pearsonr(data[factor[i]], data[factor[j + 1]])[0]))输出结果：（展示部分数据结果）1234567指标pe_ratio与指标pb_ratio之间的相关性大小为-0.004389指标pe_ratio与指标market_cap之间的相关性大小为-0.068861………………………………………………………………………………………………………………………………………………………………指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697指标return_on_asset_net_profit与指标ev之间的相关性大小为-0.101225………………………………………………………………………………………………………………………………………………………………指标revenue与指标total_expense之间的相关性大小为0.995845从中我们得出指标revenue与指标total_expense之间的相关性大小为0.995845指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697我们也可以通过画图来观察结果代码：1234import matplotlib.pyplot as pltplt.figure(figsize=(20, 8), dpi=100)plt.scatter(data['revenue'], data['total_expense'])plt.show()注：特征与特征之间相关性很高：1）选取其中一个；2）权重加权求和；3）主成分分析Scikit-learn提供的 f_regrssion 方法能够批量计算特征的f_score和p-value，非常方便，参考sklearn的 pipelinePearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：代码：12x = np.random.uniform(-1, 1, 100000)print pearsonr(x, x**2)[0]输出结果：1-0.00230804707612更多类似的例子参考 sample plots 。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如 Anscombe’s quartet ，最好把数据可视化出来，以免得出错误的结论。互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC)经典的互信息（互信息为随机变量X与Y之间的互信息$I(X;Y)$为单个事件之间互信息的数学期望）也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：I(X ; Y)=\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。代码：123456import numpy as npfrom minepy import MINEm = MINE()x = np.random.uniform(-1, 1, 10000)m.compute_score(x, x**2)print(m.mic())输出结果：11.0000000000000009MIC的统计能力遭到了 一些质疑 ，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。距离相关系数 (Distance Correlation)距离相关系数是为了克服Pearson相关系数的弱点而生的。在$X$和$X^2$这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。R的 energy 包里提供了距离相关系数的实现，另外这是 Python gist 的实现。尽管有 MIC 和 距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一，Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二，Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。基于模型的特征排序 (Model based ranking)这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如特征和响应变量之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子(这里使用了交叉验证)：代码：12345678910111213141516from sklearn.model_selection import cross_val_score, ShuffleSplitfrom sklearn.datasets import load_bostonfrom sklearn.ensemble import RandomForestRegressorimport numpy as np# Load boston housing dataset as an exampleboston = load_boston()X = boston[\"data\"]Y = boston[\"target\"]names = boston[\"feature_names\"]rf = RandomForestRegressor(n_estimators=20, max_depth=4)scores = []# 单独采用每个特征进行建模，并进行交叉验证for i in range(X.shape[1]): score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\", cv=ShuffleSplit(len(X), 3, .3)) # 注意X[:, i]和X[:, i:i+1]的区别 scores.append((format(np.mean(score), '.3f'), names[i]))print(sorted(scores, reverse=True))输出结果：1[('-8.082', 'TAX'), ('-6.871', 'CHAS'), ('-6.420', 'RM'), ('-6.315', 'DIS'), ('-4.833', 'INDUS'), ('-4.816', 'AGE'), ('-4.742', 'LSTAT'), ('-4.638', 'RAD'), ('-3.411', 'NOX'), ('-3.123', 'CRIM'), ('-26.603', 'PTRATIO'), ('-12.284', 'B'), ('-1.995', 'ZN')]WrapperWrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。主要方法有：recursive feature elimination algorithm(递归特征消除算法)EmbeddedEmbedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。比如，Lasso和RF（随机森林）都有各自的特征选择方法。注：使用SelectFromModel选择特征1from sklearn.feature_selection import SelectFromMode特征降维二维数组 此处的降维：降低特征的个数 效果：特征与特征之间不相关降维是指在某种限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法：主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。主成分分析法（PCA）基本思想：构造原变量的一系列线性组合形成几个综合指标，以去除数据的相关性，并使低维数据最大程度保持原始高维数据的方差信息。主成分个数的确定：贡献率：第i个主成分的方差在全部方差中所占比重，反映第i个主成分所提取的总信息的份额。累计贡献率：前k个主成分在全部方差中所占比重主成分个数的确定：累计贡献率&gt;0.85相关系数矩阵or协方差阵？当涉及变量的量纲不同或取值范围相差较大的指标时，应考虑从相关系数矩阵出发进行主成分分析；（相关系数矩阵消除了量纲的影响。）对同度量或取值范围相差不大的数据，从协方差阵出发。使用decomposition库的PCA类选择特征的代码如下：1234from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(X) #X=iris.data（鸢尾花）数据集n_components：小数：表示保留百分之多少的信息整数：减少到多少特征线性判别分析法（LDA）至多能把C类数据降维到C-1维子空间使用lda库的LDA类选择特征的代码如下：1234from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(X,Y) #X=iris.data,Y= iris.target（鸢尾花）数据集小结分类算法分类问题：目标值—类别sklearn转换器和估计器转换器1.实例化 (实例化的是一个转换器类(Transformer))2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)标准化：(x - mean) / stdfit_transform()fit() #计算 每一列的平均值、标准差transform() # (x - mean) / std进行最终的转换估计器sklearn机器学习算法的实现1、用于分类的估计器：sklearn.neighbors k-近邻算法sklearn.naive_bayes 贝叶斯sklearn.linear_model.LogisticRegression 逻辑回归sklearn.tree 决策树与随机森林2、用于回归的估计器：sklearn.linear_model.LinearRegression 线性回归sklearn.linear_model.Ridge 岭回归3、用于无监督学习的估计器sklearn.cluster.KMeans 聚类估计器工作流程1、实例化一个estimator2、estimator.fit(x_train, y_train) 计算—— 调用完毕，模型生成3 模型评估：1）直接比对真实值和预测值y_predict = estimator.predict(x_test)y_test y_predict2）计算准确率accuracy = estimator.score(x_test, y_test)K-近邻算法K-近邻算法（KNN）K-近邻算法(KNN)理论/原理：“物以类聚，人以群分”相同/近似样本在样本空间中是比较接近的，所以可以使用和当前样本比较近的其他样本的目标属性值作为当前样本的预测值。k-近邻算法的工作机制很简单：给定测试样本，基于某种距离度量（一般使用欧几里德距离）找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。如何确定谁是邻居？计算距离：距离公式：欧氏距离：d = \\sqrt{(x1 - y1)^2 + (x2 - y2)^2 + (x3 - y3)^2 + ……}曼哈顿距离—绝对值距离明可夫斯基距离：欧氏距离和曼哈顿距离的推广 metric_params=None图中红线代表曼哈顿距离，绿线代表欧氏距离，也就是直线距离，而蓝线和黄线代表等价的曼哈顿距离。电影类型分析假设我们有现在几部电影其中？ 号电影不知道类别，如何去预测？我们可以利用K近邻算法的思想k = 1 ——&gt;最近距离18.7——&gt;电影为爱情片——&gt;预测？号电影为爱情片k = 2 ——&gt;最近距离18.7和19.2——两部电影都是爱情片——预测？号电影为爱情片……k = 6——&gt; 六部电影爱情片和动作片一样多——&gt;无法确定k = 7 ——&gt;若4部动作片，3部爱情片——&gt;预测？号电影为动作片,但实际电影为爱情片如果取的最近的电影数量不一样？会是什么结果？k 值取得过小，容易受到异常点的影响k 值取得过大，样本不均衡的影响K-近邻算法APIsklearn.neighbors.KNeighborsClassifier(n_neighbors=5,weights=’uniform’,algorithm=’auto’,leaf_size=30,p=2,metric=’minkowski’,metric_params=None,n_jobs=None,**kwargs)邻居数k: n_neighbors:int,可选(默认= 5)权重weights: weights = ‘uniform’ weights = ‘distance’距离度量: p=1距离度量采用曼哈顿距离；p=2距离度量采用欧氏距离algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)案例：鸢尾花种类预测代码：12345678910111213141516171819202122232425#1)导入库import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier#2)获取数据x,y = datasets.load_iris(True)#3）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#4）特征工程：标准化ss=StandardScaler()x_train=ss.fit_transform(x_train)x_test=ss.transform(x_test)#5)KNN算法预估器(训练数据)estimator=KNeighborsClassifier(n_neighbors=3) #设置k=3estimator.fit(x_train,y_train)#6)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print('准确率为：\\n',score)输出结果：12345678y_predict: [0 2 0 2 2 0 2 1 0 1 2 0 2 2 0 0 2 1 0 2 2 1 2 0 1 0 1 0 1 1]直接比对真实值和预测值： [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True False True]准确率为： 0.9666666666666667K-近邻总结优点：简单，易于理解，易于实现，无需训练缺点：1）必须指定K值，K值选择不当则分类精度不能保证2）懒惰算法，对测试样本分类时的计算量大，内存开销大使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试模型选择与调优什么是交叉验证(cross validation)交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。训练集：训练集+验证集测试集：测试集超参数搜索-网格搜索(Grid Search)通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。模型选择与调优:sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)对估计器的指定参数值进行详尽搜索estimator：估计器对象param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}cv：指定几折交叉验证fit：输入训练数据score：准确率结果分析：bestscore:在交叉验证中验证的最好结果_bestestimator：最好的参数模型cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果鸢尾花案例调优代码：12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# grid网格，search搜索，cv：cross_validation# 搜索算法最合适的参数from sklearn.model_selection import GridSearchCV#1)获取数据x,y = datasets.load_iris(True)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#3）特征工程：标准化ss=StandardScaler()x_train=ss.fit_transform(x_train)x_test=ss.transform(x_test)#4)KNN算法预估器estimator=KNeighborsClassifier()#网格搜索GridSearchCV进行最佳参数的查找params = &#123;'n_neighbors':[i for i in range(1,30)], 'weights':['distance','uniform'], 'p':[1,2]&#125;# cross_val_score类似estimator = GridSearchCV(estimator,param_grid=params,scoring='accuracy',cv = 6)estimator.fit(x_train,y_train)#5)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print('准确率为：\\n',score)#查看了GridSearchCV最佳的参数组合#最佳参数：best_params_print('最佳参数：\\n',estimator.best_params_)#最佳结果：best_score_print('最佳结果：\\n',estimator.best_score_)#最佳估计器：best_estimator_print('最佳估计器：\\n',estimator.best_estimator_)#交叉验证结果：cv_results_#print('交叉验证结果：\\n',estimator.cv_results_)输出结果：12345678910111213141516y_predict: [1 1 2 2 1 0 1 0 1 2 1 2 1 0 1 2 1 0 0 2 1 1 1 2 2 2 0 0 1 1]直接比对真实值和预测值： [ True True True True True True True True False True True True True True True True False False True True True True True True True True True True False True]准确率为： 0.8666666666666667最佳参数： &#123;'n_neighbors': 15, 'p': 2, 'weights': 'distance'&#125;最佳结果： 0.9833333333333333最佳估计器： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=15, p=2, weights='distance')注：最佳结果—训练集再分为训练集+验证集，验证集的效果；准确率—整体测试集的效果。数据集不同朴素贝叶斯算法朴素贝叶斯分类器是基于概率论的分类模型，其思想是先计算样本的先验概率，然后利用贝叶斯公式测算未知样本属于某个类别的后验概率，最终以最大后验概率对应的类别作为未知样本的预测类别。之所以叫”朴素”，是因为整个形式化过程只做最简单、最原始的假设。理论基础案例问题：1、女神喜欢的概率？ $P(喜欢)=\\frac{4}{7}$2、职业是程序员并且体型匀称的概率？ $P(程序员，匀称)=\\frac{1}{7}$ (联合概率)3、在女神喜欢的条件下，职业是程序员的概率？ $P(程序员|喜欢)=\\frac{2}{4}=\\frac{1}{2}$ （条件概率）4、在女神喜欢的条件下，职业是产品，体重是超重的概率？​ $P(程序员，超重|喜欢)=\\frac{1}{4}$ （联合概率、条件概率）相互独立：$P(AB)=P(A)P(B)$&lt;=&gt;事件A与事件B相互独立$P(程序员，匀称)=\\frac{1}{7}$ $P(程序员)=\\frac{3}{7}$ $P(匀称)=\\frac{4}{7}$ $P(程序员，匀称)≠P(程序员)P(匀称)$ 不独立贝叶斯公式：$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$P(喜欢|产品经理,超重)=\\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=0$朴素? 假设:特征与特征之间是相互独立的朴素贝叶斯算法=朴素+贝叶斯$P(喜欢|产品经理,超重)=\\frac{P(产品经理,超重|喜欢)P(喜欢)}{P(产品经理,超重)}=\\frac{P(产品经理|喜欢)P(超重|喜欢)P(喜欢)}{P(产品经理)P(超重)}=\\frac{7}{12}$应用场景朴素贝叶斯算法应用场景：文本分类公式：$P(C|W)=\\frac{P(W|C)P(C)}{P(W)}$注：$W$为给定文档的特征值（频数统计，预测文档提供），$C$为文档类别公式如果应用在文章分类的场景当中，我们可以这样看：P(C|F1,F2,…)=\\frac{P(F1,F2,…|C)P(C)}{P(F1,F2,…)}其中$C$可以是不同类别公式分为三个部分：$P(C)$：每个文档类别的概率(某文档类别数／总文档数量)$P(W|C)$：给定类别下特征（被预测文档中出现的词）的概率​ 计算方法：$P(F1│C)=Ni/N $（训练文档中去计算）​ $Ni$为该$F1$词在$C$类别所有文档中出现的次数​ $N$为所属类别$C$下的文档所有词出现的次数和$P(F1,F2,…) $预测文档中每个词的概率常用贝叶斯分类器1.高斯贝叶斯分类器适用条件：自变量X均为连续的数值型假设条件：自变量X服从高斯正态分布自变量X的条件概率：$P(x_j∣C_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{ji}}exp(-\\frac{(x_j-\\mu_{ji})^2}{2\\sigma_{ji}^2})$其中$x_j$为第$j$个自变量的取值，$μ_{ji}$为训练集中自变量$x_j$属于类别 $C_i$的均值，$σ_{ji}$为训练集中自变量$x_j$属于类别 $C_i$的标准差。2.多项式贝叶斯分类器适用条件：自变量X均为离散型变量假设条件：自变量X的条件概率服从多项式分布自变量X的条件概率：$P(x_j=x_{jk}∣C_i)=\\frac{N_{ik}+\\alpha}{N_i+n\\alpha}$其中$x_{jk}$为自变量$x_j$的第$k$个取值，$N_{ik}$表示因变量为类别$C_i$时自变量$x_j$取值$x_{jk}$的样本个数，$N_i$为类别$C_i$的样本个数，$α$为平滑系数（防止条件概率等于0，通常取1），n为训练文档中统计出的特征词个数。3.伯努利贝叶斯分类器适用条件：自变量X均为0-1二元变量假设条件：自变量X的条件概率服从伯努利分布自变量X的条件概率$P(x_j∣C_i)=px_j+(1−p)(1−x_j)$其中$x_j$为第$j$个自变量，其取值为0或1；$p$表示类别为$C_i$时自变量取1的概率，可以用经验频率代替$p=P(x_j=1∣C_i)=\\frac{N_{i1}+α}{N_i+nα}$$N_{i1}$表示在类别$C_i$时自变量$x_j$取1的样本量。代码实现1.高斯贝叶斯分类器代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#高斯贝叶斯分类器进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\", names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace='?', value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值print('-----------------------------------')# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data['Class']#设置正例和负例y = y.map(&#123;2:0,4:1&#125;)print(y.value_counts())print('-----------------------------------')#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)#5、estimator估计器流程from sklearn import naive_bayes#调用高斯朴素贝叶斯分类器的“类”gnb = naive_bayes.GaussianNB()#模型拟合gnb.fit(x_train,y_train)#6、进行预测（模型评估）#模型在测试数据集上的预测gnb_pred = gnb.predict(x_test)#各类别的预测数量print(pd.Series(gnb_pred).value_counts())print('-----------------------------------')#导入第三方包from sklearn import metricsimport matplotlib.pyplot as pltimport seaborn as sns#构建混淆矩阵cm = pd.crosstab(gnb_pred,y_test)#绘制混淆矩阵图sns.heatmap(cm,annot = True , cmap = 'GnBu' , fmt = 'd')#去除x轴和y轴的标签plt.xlabel('Real')plt.ylabel('Predict')#显示图形plt.show()print('模型的准确率：\\n',metrics.accuracy_score(y_test,gnb_pred))print('模型的评估报告：\\n',metrics.classification_report(y_test,gnb_pred))#计算正例的预测概率，用于生成ROC曲线的数据y_score = gnb.predict_proba(x_test)[:,1]fpr,tpr,threshold = metrics.roc_curve(y_test,y_score)#计算AUC的值roc_auc = metrics.auc(fpr,tpr)#绘制面积图plt.stackplot(fpr,tpr,color = 'steelblue',alpha = 0.5,edgecolor = 'black')#添加边际线plt.plot(fpr,tpr,color= 'black',lw =1)#添加对角线plt.plot([0,1],[0,1],color = 'red',linestyle = '--')#添加文本信息plt.text(0.5,0.3,'ROC curve(area = %0.2f)'% roc_auc)#添加x轴与y轴标签plt.xlabel('1-Specificity')plt.ylabel('Sensitivity')#显示图形plt.show()输出结果：1234567891011121314151617181920212223242526272829303132Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: bool-----------------------------------0 4441 239Name: Class, dtype: int64-----------------------------------0 1231 82dtype: int64-----------------------------------模型的准确率： 0.9707317073170731模型的评估报告： precision recall f1-score support 0 0.99 0.96 0.98 127 1 0.94 0.99 0.96 78 accuracy 0.97 205 macro avg 0.97 0.97 0.97 205weighted avg 0.97 0.97 0.97 2052.多项式贝叶斯分类器代码：12345678910111213141516171819202122232425262728293031import pandas as pdfrom sklearn import model_selection,naive_bayes,metricsimport matplotlib.pyplot as pltdata=pd.read_csv(r'mushrooms.csv')print(data.head())#使用factorize函数将字符型数据做因子化处理，将其转换为整数型数据#factorize函数返回的是两个元素的元组，第一个元素为转换成的数值，第二个元素为数值对应的字符水平columns=data.columns[1:]for column in columns: data[column]=pd.factorize(data[column])[0]#拆分为训练集和测试集x_train,x_test,y_train,y_test=model_selection.train_test_split(data[columns],data.type,test_size=0.25,random_state=1234)#调用多项式朴素贝叶斯mnb=naive_bayes.MultinomialNB()mnb.fit(x_train,y_train)mnb_pred=mnb.predict(x_test)#显示预测结果，各类别的预测数量#模型检验print('模型的准确率为：',metrics.accuracy_score(y_test,mnb_pred))print('模型的评估报告：\\n',metrics.classification_report(y_test,mnb_pred))#绘制ROC曲线y_score=mnb.predict_proba(x_test)[:,1]fpr,tpr,threshold=metrics.roc_curve(y_test.map(&#123;'e':0,'p':1&#125;),y_score)roc_auc=metrics.auc(fpr,tpr)plt.stackplot(fpr,tpr,color='steelblue',alpha=0.5,edgecolor='black')plt.plot(fpr,tpr,color='black',lw=1)plt.plot([0,1],[0,1],color='red',linestyle='--')plt.text(0.5,0.3,'ROC Curve (area=%0.2f)' % roc_auc)plt.xlabel('l-Specificity')plt.ylabel('Sensitivity')plt.show()输出结果：123456789101112131415161718 type cap-shape cap-surface ... spore-print-color population habitat0 p x s ... k s u1 e x s ... n n g2 e b s ... n n m3 p x y ... k s u4 e x s ... n a g[5 rows x 23 columns]模型的准确率为： 0.8877400295420975模型的评估报告： precision recall f1-score support e 0.85 0.95 0.89 1017 p 0.94 0.83 0.88 1014 accuracy 0.89 2031 macro avg 0.89 0.89 0.89 2031weighted avg 0.89 0.89 0.89 20313.伯努利贝叶斯分类器未完待续案例：新闻分类代码：123456789101112131415161718192021222324252627282930313233#朴素贝叶斯对新闻数据集进行预测#1）导入库import numpy as npfrom sklearn import datasetsfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metrics#2）获取新闻的数据，20个类别news = fetch_20newsgroups(subset='all')#3）进行数据集分割x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.3)#4）对于文本数据，进行特征抽取tf = TfidfVectorizer()x_train = tf.fit_transform(x_train)#这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表print(tf.get_feature_names())# print(x_train.toarray())x_test = tf.transform(x_test) # 不需要fit_transform#5）estimator估计器流程mnb = MultinomialNB(alpha=1.0) #默认alpha=1.0mnb.fit(x_train, y_train)#6）进行预测（模型评估）y_predict = mnb.predict(x_test)print('预测每篇文章的类别：\\n', y_predict[:100])print('真实类别为：\\n', y_test[:100])print('模型预测的准确率为：\\n', mnb.score(x_test, y_test))#print('模型预测的准确率为：\\n',metrics.accuracy_score(y_test,y_predict))#混淆矩阵from sklearn.metrics import classification_reportprint('模型的评估报告：\\n',classification_report(y_test,y_predict))输出结果：123456789101112131415161718192021222324252627['00', '000', '0000', '00000', '000000', '00000000', '0000000004', '0000000005', '00000000b', '00000001', '00000001b', '00000010', ··································································· 'zzrk', 'zzs', 'zzt', 'zzvsi', 'zzx', 'zzy_3w', 'zzzzzz', 'zzzzzzt', '³ation', 'íålittin', 'ñaustin', 'ýé', 'ÿhooked']预测每篇文章的类别： [16 7 2 4 10 3 18 7 8 12 0 13 11 11 0 15 18 10 12 8 12 2 15 7 9 12 15 16 9 0 3 15 8 3 14 17 1 0 15 16 9 2 3 6 5 4 6 14 8 9 13 9 11 12 10 10 3 11 11 0 6 12 15 3 15 6 5 1 9 14 10 3 7 11 0 3 7 16 13 9 5 15 1 13 15 16 7 4 1 16 16 18 14 15 7 16 15 14 0 14]真实类别为： [19 7 2 4 10 3 18 3 8 12 0 13 11 11 0 0 18 10 12 15 12 2 15 7 9 12 15 16 9 0 3 15 8 3 14 17 1 0 15 16 9 2 3 8 5 4 6 14 8 9 13 9 11 12 10 10 3 11 11 0 6 12 0 3 0 6 5 1 9 14 12 6 8 12 0 3 7 18 13 9 5 15 1 13 15 16 7 4 1 16 18 18 14 15 7 13 15 1 0 14]模型预测的准确率为： 0.8593915811814644 模型的评估报告： precision recall f1-score support 0 0.89 0.76 0.82 235 1 0.93 0.72 0.81 320 ··································································· 18 0.99 0.63 0.77 235 19 1.00 0.21 0.35 177 accuracy 0.86 5654 macro avg 0.88 0.84 0.84 5654weighted avg 0.88 0.86 0.85 5654朴素贝叶斯总结优点：朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。对缺失数据不太敏感，算法也比较简单，常用于文本分类。分类准确度高，速度快缺点：由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好决策树认识决策树​ 如何高效的进行决策？​ 特征的先后顺序决策树分类原理详解​ 已知 四个特征值 预测 是否贷款给某个人​ 先看房子，再工作 -&gt; 是否贷款 只看了两个特征​ 年龄，信贷情况，工作 看了三个特征信息论基础1）信息：消除随机不确定性的东西小明：“我今年18岁” - 信息小华： ”小明明年19岁” - 不是信息(当小明告诉我他今年18岁，这就消除了对小明年龄的不确定性)2）信息的衡量标准 - 熵熵：表示随机变量不确定性的度量 熵单位为比特（bit）（解释：说白了就是物体内部的混乱程度，比如杂货市场里面什么都有那肯定混乱，专卖店里面只卖一个牌子那就稳定多了）公式：H（D）=-\\sum_{i=1}^{n} p(x i) * \\log _{2} p(x i)熵：不确定性越大，得到的熵值也就越大3）决策树的划分依据之一———信息增益特征A对训练数据集D的信息增益$g(D,A)$,定义为集合D的熵$H(D)$与特征A给定条件下D的信息条件熵$H(D|A)$之差，即公式为：g(D,A)=H(D)-H(D|A)公式的详细解释：熵的计算：H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log_{2} \\frac{\\left|C_{k}\\right|}{|D|}条件熵的计算公式：H(D | A)=\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} H\\left(D_{i}\\right)=-\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} \\sum_{k=1}^{K} \\frac{\\left|D_{i k}\\right|}{\\left|D_{i}\\right|} \\log_{2} \\frac{\\left|D_{i k}\\right|}{\\left|D_{i}\\right|}注：$C_{k}$表示属于某个类别的样本数注：信息增益表示得知特征X的信息而信息的不确定性减少的程度使得类Y的信息熵减少的程度例子根据某人年龄、工作、房子和信贷情况，判断是否贷款？在历史数据中（15次贷款）有6次没贷款，9次贷款，所以此时的熵应为：H(D)=-(\\frac{6}{15}*log_{2}\\frac{6}{15}+\\frac{9}{15}*log_{2}\\frac{9}{15})=0.971$H(青年)=-(\\frac{2}{5}log_{2}\\frac{2}{5}+\\frac{3}{5}log_{2}\\frac{3}{5})=0.971$$H(中年)=-(\\frac{2}{5}log_{2}\\frac{2}{5}+\\frac{3}{5}log_{2}\\frac{3}{5})=0.971$$H(老年)=-(\\frac{1}{5}log_{2}\\frac{1}{5}+\\frac{4}{5}log_{2}\\frac{4}{5})=0.722$$H(D|年龄)=\\frac{5}{15}H(青年)+\\frac{5}{15}H(中年)+\\frac{5}{15}H(老年)=0.888$$g(D,年龄)=H(D)-H(D|年龄)=0.083$ 注：别人计算为0.313我们以A1、A2、A3、A4代表年龄、有工作、有自己的房子和贷款情况。最终计算的结果g(D, A1) = 0.313, g(D, A2) = 0.324, g(D, A3) = 0.420,g(D, A4) = 0.363。所以我们选择A3 作为划分的第一个特征。这样我们就可以一棵树慢慢建立决策树的三种算法实现当然决策树的原理不止信息增益这一种，还有其他方法。但是原理都类似，我们就不去举例计算。ID3信息增益 最大的准则C4.5信息增益比 最大的准则CART分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则优势：划分更加细致（从后面例子的树显示来理解）决策树算法APIclass sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)决策树分类器criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’max_depth:树的深度大小random_state:随机数种子其中会有些超参数：max_depth:树的深度大小其它超参数我们会结合随机森林讲解案例1：鸢尾花种类预测代码：1234567891011121314151617181920212223#决策树对鸢尾花进行分类#1)导入库import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier#2)获取数据x,y = datasets.load_iris(True)#3）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#决策树不需要特征工程：标准化#4)决策树算法预估器(训练数据)estimator=DecisionTreeClassifier(criterion='entropy')estimator.fit(x_train,y_train)#5)模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=estimator.score(x_test,y_test)print('准确率为：\\n',score)输出结果：12345678y_predict: [1 2 0 0 1 2 0 1 2 2 2 1 1 0 1 2 1 0 0 2 2 2 2 2 2 2 0 2 1 2]直接比对真实值和预测值： [ True True True True True True True False True True False True True True True True True True True True False True True True True True True True True True]准确率为： 0.9注：比对kNN算法准确率低；由于鸢尾花数据少，而kNN算法使用场景为小数据场景决策树可视化1、sklearn.tree.export_graphviz() 该函数能够导出DOT格式tree.export_graphviz(estimator,out_file=’tree.dot’,feature_names=[‘’,’’])2、工具:(能够将dot文件转换为pdf、png)安装graphvizubuntu:sudo apt-get install graphviz Mac:brew install graphviz3、运行命令然后我们运行这个命令dot -Tpng tree.dot -o tree.png以上文鸢尾花数据为例：12from sklearn.tree import export_graphvizexport_graphviz(estimator,out_file='iris_tree.dot',feature_names=iris.feature_names)导出文档iris_tree.dot，打开文档全选复制，粘贴到http://www.webgraphviz.com/网页版上可以实现可视化。无需安装软件。案例：泰坦尼克号乘客生存预测泰坦尼克号数据在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#决策树进行乘客生存预测#1、导入库import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn import datasetsfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.tree import DecisionTreeClassifier,export_graphvizfrom sklearn import tree#2、获取数据titan = pd.read_csv(\"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt\")#3、数据的处理x = titan[['pclass', 'age', 'sex']]y = titan['survived']# ①缺失值处理，将特征当中有类别的这些特征进行字典特征抽取x['age'].fillna(x['age'].mean(), inplace=True)# ②对于x转换成字典数据x=x.to_dict(orient=\"records\") # [&#123;\"pclass\": \"1st\", \"age\": 29.00, \"sex\": \"female\"&#125;, &#123;&#125;]#4、划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)#5、字典特征抽取transfer = DictVectorizer()x_train = transfer.fit_transform(x_train)x_test=transfer.transform(x_test) #不需要调用fit_transformprint(transfer.get_feature_names())print('-----------------------------------------------------')#print(x)#决策树不需要特征工程：标准化#6、决策树算法预估器(训练数据)estimator=DecisionTreeClassifier(criterion='entropy')estimator.fit(x_train,y_train)#7、模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print('y_predict:\\n',y_predict)print('-----------------------------------------------------')print('直接比对真实值和预测值：\\n',y_test y_predict)print('-----------------------------------------------------')#方法2：计算准确率score=estimator.score(x_test,y_test)print('准确率为：\\n',score)#8、可视化导出titan_tree.dot文档export_graphviz(estimator,out_file='titan_tree.dot',feature_names=transfer.get_feature_names())输出结果：12345678910111213141516171819202122232425262728['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']-----------------------------------------------------y_predict: [0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1]-----------------------------------------------------直接比对真实值和预测值： 199 True1308 True814 True393 False560 True ... 955 True147 True30 True923 False404 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------准确率为： 0.779467680608365决策树总结优点：可视化 - 可解释能力强缺点：容易产生过拟合改进：减枝cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)随机森林集成学习方法之随机森林什么是集成学习方法集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。什么是随机森林在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。随机森林原理过程训练集：特征值 +目标值 N个样本 M个特征两个随机训练集随机 - N个样本中随机有放回的抽样N个bootstrap(随机有放回抽样)[1, 2, 3, 4, 5]经过随机有放回抽样得到新的树的训练 [2, 2, 3, 1, 5]特征随机 - 从M个特征中随机抽取m个特征​ 要求：M &gt;&gt; m 效果：降维随机森林算法APIclass sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)随机森林分类器n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200criterian：string，可选（default =“gini”）分割特征的测量方法max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30max_features=”auto”,每个决策树的最大特征数量​ If “auto”, then max_features=sqrt(n_features). 对M求平方根​ If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).​ If “log2”, then max_features=log2(n_features).​ If None, then max_features=n_features.bootstrap：boolean，optional（default = True） 是否在构建树时使用放回抽样min_samples_split:节点划分最少样本数min_samples_leaf:叶子节点的最小样本数超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf案例：泰坦尼克号乘客生存预测代码：1234567891011121314151617181920212223242526272829303132#复制上文中决策树代码比对决策树与随机森林效果#随机森林进行乘客生存预测from sklearn.ensemble import RandomForestClassifierestimator= RandomForestClassifier()# grid网格，search搜索，cv：cross_validation# 搜索算法最合适的参数from sklearn.model_selection import GridSearchCV#网格搜索GridSearchCV进行最佳参数的查找params =&#123;\"n_estimators\": [120,200,300,500,800,1200], \"max_depth\": [5, 8, 15, 25, 30]&#125;# cross_val_score类似estimator = GridSearchCV(estimator,param_grid=params,cv = 6)estimator.fit(x_train, y_train)#模型评估#方法1：直接比对真实值和预测值y_predict=estimator.predict(x_test)print('---------------以下为随机森林预测效果-----------------')print('y_predict:\\n',y_predict)print('随机森林预测的直接比对真实值和预测值：\\n',y_test y_predict)print('-----------------------------------------------------')#方法2：计算准确率score=estimator.score(x_test,y_test)print('随机森林预测的准确率为：\\n',score)print('-----------------------------------------------------')#查看了GridSearchCV最佳的参数组合#最佳参数：best_params_print('最佳参数：\\n',estimator.best_params_)#最佳结果：best_score_print('最佳结果：\\n',estimator.best_score_)#最佳估计器：best_estimator_print('最佳估计器：\\n',estimator.best_estimator_)#交叉验证结果：cv_results_#print('交叉验证结果：\\n',estimator.cv_results_)输出结果：注：随机森林输出结果等待时间长12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#决策树输出结果：['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']-----------------------------------------------------y_predict: [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0]-----------------------------------------------------直接比对真实值和预测值： 379 False1099 True970 True482 True573 True ... 80 True295 True4 True1173 True684 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------准确率为： 0.7490494296577946 #随机森林输出结果：---------------以下为随机森林预测效果-----------------y_predict: [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0]随机森林预测的直接比对真实值和预测值： 379 False1099 True970 True482 True573 True ... 80 True295 True4 True1173 True684 TrueName: survived, Length: 263, dtype: bool-----------------------------------------------------随机森林预测的准确率为： 0.7908745247148289-----------------------------------------------------最佳参数： &#123;'max_depth': 5, 'n_estimators': 300&#125;最佳结果： 0.8295238095238096最佳估计器： RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)总结在当前所有算法中，具有极好的准确率能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维能够评估各个特征在分类问题上的重要性回归算法回归问题：目标值 - 连续型的数据线性回归什么是线性回归线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。特点：只有一个自变量的情况称为单变量回归（如：$y=kx+b$），大于一个自变量情况的叫做多元回归。通用公式：$h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$其中$w$权重，$b$偏置$w$和$x$可以理解为矩阵：$\\mathbf{w}=\\left(\\begin{array}{c}b \\ w_{1} \\ w_{2}\\end{array}\\right)$,$\\mathbf{x}=\\left(\\begin{array}{c}1 \\ x_{1} \\ x_{2}\\end{array}\\right)$线性回归当中的关系有两种，一种是线性关系，另一种是非线性关系。例子：线性关系注：如果在单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系数据：工资和年龄（2个特征）目标：预测银行会贷款给我多少钱（标签）考虑：工资和年龄都会影响最终银行贷款的结果那么它们各自有多大的影响呢？（参数）$X1,X2$就是我们的两个特征（年龄，工资）$ Y$是银行最终会借给我们多少钱找到最合适的一条线（想象一个高维）来最好的拟合我们的数据点假设$w_1$是年龄的参数，$w_2$是工资的参数拟合的平面：$h_w(x)=w_1x_1+w_2x_2$整合：$h_w(x)=w^Tx$非线性关系如果是非线性关系，那么回归方程可以理解为：$w_1x_1+w_2x_2^2+w_3x_3^2$线性回归原理1、误差真实值和预测值之间肯定是要存在差异的（用$ε$来表示该误差）对于每个样本：$y^{(i)}=w^{T} x^{(i)}+\\varepsilon^{(i)}$误差$\\varepsilon^{(i)}$是独立同分布，并且服从均值为0方差为$θ^2$的高斯分布(正态分布)独立：张三和李四一起来贷款，他俩没关系同分布：他俩都来得是我们假定的这家银行高斯分布：银行可能会多给，也可能会少给，但是绝大多数情况下这个浮动不会太大，极小情况下浮动会比较大，符合正常情况预测值与误差：$y^{(i)}=w^{T} x^{(i)}+\\varepsilon^{(i)}$ $(1)$由于误差服从高斯分布：$p(\\varepsilon^{(i)})=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(\\varepsilon^{(i)})^{2}}{2 \\sigma^{2}}\\right)$ $(2)$将$(1)$式代入$(2)$式：$p(y^{(i)}|x^{(i)};w)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(y^{(i)}-w^Tx^{(i)})^{2}}{2 \\sigma^{2}}\\right)$似然函数：$L(w)=\\prod_{i=1}^{m} p\\left(y^{(i)} | x^{(i)} ; w\\right)=\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$解释：什么样的参数跟我们的数据组合后恰好是真实值对数似然：$logL(w)=log\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$解释：乘法难解，加法就容易了，对数里面乘法可以转换成加法展开化简：$\\sum_{i=1}^{m} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}}{2 \\sigma^{2}}\\right)$$=m \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma}-\\frac{1}{\\sigma^{2}} \\cdot \\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}$目标：让似然函数（对数变换后也一样）越大越好2、损失函数（Loss Function）$J(w)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(y^{(i)}-w^{T} x^{(i)}\\right)^{2}=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h_w(x^{(i)})-y^{(i)}\\right)^{2}$这里的这个损失函数就是著名的最小二乘损失函数3、优化算法如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）线性回归经常使用的两种优化算法:1)正规方程——天才（直接求解$w$）目标函数：$J(w)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h_w(x^{(i)})-y^{(i)}\\right)^{2}=\\frac{1}{2}\\left(Xw-y\\right)^T\\left(Xw-y\\right)$求偏导：\\begin{aligned} &\\nabla_{w} J(w)=\\nabla_{w}\\left(\\frac{1}{2}(X w-y)^{r}(X w-y)\\right)=\\nabla_{w}\\left(\\frac{1}{2}\\left(w^{T} X^{T}-y^{T}\\right)(X w-y)\\right)\\\\ &=\\nabla_{w}\\left(\\frac{1}{2}\\left(w^{T} X^{T} X w-w^{T} X^{T} y-y^{T} X w+y^{T} y\\right)\\right)\\\\ &=\\frac{1}{2}\\left(2 X^{T} X w-X^{T} y-\\left(y^{T} X\\right)^{\\bar{T}}\\right)=X^{T} X w-X^{T} y \\end{aligned}令偏导等于0得：$w=\\left(X^TX\\right)^{-1}X^Ty$理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果缺点：当特征过多过复杂时，求解速度太慢并且得不到结果2）梯度下降(Gradient Descent)——勤奋努力的普通人（一步一步的求$w$）理解：α为学习速率，需要手动指定（超参数），α旁边的整体表示方向沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果我们通过两个图更好理解梯度下降的过程:4、优化动态图演示线性回归算法API方法一sklearn.linear_model.LinearRegression(fit_intercept=True)通过正规方程优化fit_intercept：是否计算偏置LinearRegression.coef_：回归系数_LinearRegression.intercept_：偏置_方法二sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。loss:损失类型 loss=”squared_loss”: 普通最小二乘法fit_intercept：是否计算偏置learning_rate : string, optional​ 学习率填充​ ‘constant’: eta = eta0​ ‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]​ ‘invscaling’: eta = eta0 / pow(t, power_t) power_t=0.25:存在父类当中对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。SGDRegressor.coef_：回归系数__SGDRegressor.intercept_：偏置注：sklearn提供给我们两种实现的API， 可以根据选择使用波士顿房价预测代码：（补充完善https://blog.csdn.net/weixin_41890393/article/details/83589860）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#正规方程的优化方法对波士顿房价进行预测import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegressionfrom sklearn import metrics#1)获取数据x,y= datasets.load_boston(True)#平滑处理预测值y#平滑处理y值，x不处理。（x代表特征，y代表预测值）#y=np.log(y)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=22)#3)特征工程：标准化 不适合用标准化数据 一般用平滑处理#transfer=StandardScaler()#x_train=transfer.fit_transform(x_train)#x_test=transfer.transform(x_test)#4)正规方程的优化算法预估器estimator=LinearRegression()estimator.fit(x_train,y_train)#5)得出模型print('正规方程权重系数为:\\n',estimator.coef_)print('正规方程偏置为:\\n',estimator.intercept_)#6）模型评估y_predict=estimator.predict(x_test)print('预测房价:\\n',y_predict)print('正规方程—均方误差MSE为:', metrics.mean_squared_error(y_test, y_predict))#7）交叉验证from sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(estimator,x,y,cv=10)print(\"MSE:\", metrics.mean_squared_error(y, predicted))#6）可视化import matplotlib.pyplot as plt# scatterplt.scatter(y, predicted)# plotplt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)plt.xlabel(\"Measured\")plt.ylabel(\"Predicted\")plt.show()#展示结果#创建画布plt.figure(figsize=(20,8),dpi=80)#绘图plt.rcParams['font.sans-serif']='SimHei'plt.rcParams['axes.unicode_minus']=Falsex=range(len(y_predict))y1=y_predicty2=y_test#折线图plt.plot(x,y1,linestyle='-')plt.plot(x,y2,linestyle='-.')#增加图例plt.legend(['房价预测值','房价真实值'])plt.title('波士顿房价走势图')#展示plt.show()输出结果：（注：特征有几个权重就有几个）123456789101112正规方程权重系数为: [-0.64817766 1.14673408 -0.05949444 0.74216553 -1.95515269 2.70902585 -0.07737374 -3.29889391 2.50267196 -1.85679269 -1.75044624 0.87341624 -3.91336869]正规方程偏置为: 22.62137203166228预测房价: [28.22944896 31.5122308 21.11612841 32.6663189 20.0023467 19.07315705........................................................... 28.58237108]正规方程—均方误差MSE为: 20.6275137630954MSE: 34.53965953999329代码：123456789101112131415161718192021222324252627#梯度下降的优化方法对波士顿房价进行预测import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import SGDRegressorfrom sklearn import metrics#1)获取数据x,y= datasets.load_boston(True)#2）划分数据集x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=22)#3)特征工程：标准化 transfer=StandardScaler()x_train=transfer.fit_transform(x_train)x_test=transfer.transform(x_test)#4)梯度下降的优化算法预估器estimator=SGDRegressor(learning_rate='constant', eta0=0.01,max_iter=10000)#调参数estimator.fit(x_train,y_train)#5)得出模型print('梯度下降权重系数为:\\n',estimator.coef_)print('梯度下降截距为:\\n',estimator.intercept_)#6）模型评估y_predict=estimator.predict(x_test)print('预测房价:\\n',y_predict)print('梯度下降—均方误差MSE为:', metrics.mean_squared_error(y_test, y_predict))输出结果：1234567891011梯度下降权重系数为: [-0.529811 0.93849958 -0.43482307 0.77305338 -1.71206925 2.82393382 -0.16256326 -3.09703859 1.63812767 -0.93714655 -1.72483573 0.88640923 -3.90806571]梯度下降截距为: [22.61725005]预测房价: [28.29992805 31.66102185 21.46408381 32.63060514 20.23508805 18.98298548........................................................... 28.33964857]梯度下降—均方误差MSE为: 21.004247881383602正规方程和梯度下降对比文字对比梯度下降正规方程需要选择学习率不需要需要迭代求解一次运算得出特征数量较大可以使用需要计算方程，时间复杂度高O(n3)选择：小规模数据：LinearRegression(不能解决拟合问题)岭回归大规模数据：SGDRegressor拓展-关于优化方法BGD、SGD、MBGD、SAG目标函数：$J(w)=\\frac{1}{2m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_w(x^{(i)})\\right)^{2}$① 批量梯度下降（batch gradient descent）：（容易得到最优解，但是由于每次考虑所有样本，速度很慢）$\\frac{\\partial J(w)}{\\partial w_{j}}=-\\frac{1}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$$w_{j}^{\\prime}=w_{j}+\\frac{α}{m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$批量梯度下降的算法执行过程如下图：② 随机梯度下降（Stochastic Gradient Descent, SGD）：（每次找一个样本，迭代速度快，但不一定每次都朝着收敛的方向，而是震荡的方式趋向极小点）$w_{j}^{\\prime}=w_{j}+α\\left(y^{(i)}-h_{w}\\left(x^{(i)}\\right)\\right) x_{j}^{(i)}$SGD的优点是：高效容易实现SGD的缺点是：SGD需要许多超参数：比如正则项参数、迭代数。SGD对于特征标准化是敏感的。③ 小批量梯度下降法（Mini-batch gradient descent）：（每次更新选择一小部分数据来算，实用！$w_{j}^{\\prime}=w_{j}+α\\frac{1}{10} \\sum_{k=i}^{i+9}\\left(y^{(k)}-h_{w}\\left(x^{(k)}\\right)\\right) x_{j}^{(k)}$④随机平均梯度法(Stochasitc Average Gradient)：（由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法）Scikit-learn：SGDRegressor、岭回归、逻辑回归等当中都会有SAG优化欠拟合与过拟合欠拟合（模型过于简单）过拟合（模型过于复杂）解决办法欠拟合解决办法增加新的特征，可以考虑加入进特征组合、高次特征，来增大假设空间采用非线性模型，比如核SVM 、决策树、DNN等模型Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等如果已正则化，尝试减少正则化程度$λ$过拟合解决办法交叉检验，通过交叉检验得到较优的模型参数特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间正则化，常用的有 L1、L2 正则如果已正则化，尝试增大正则化程度λ增加训练数据可以有限的避免过拟合Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等正则化类别L1正则化作用：可以使其中一些W的值直接为0，删除这个特征的影响。LOSSO回归L2正则化作用：可以使得其中一些W的值都很小，都接近于0，削弱某个特征的影响。优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。Ridge回归（岭回归）。加入L2正则化后的损失函数：$J(w)=\\frac{1}{2m} \\sum_{i=1}^{m}\\left(y^{(i)}-h_w(x^{(i)})\\right)^{2}+λ\\sum_{j=1}^{n}w_j^2$其中m为样本数，n为特征数岭回归岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化（L2）的限制，从而达到解决过拟合的效果。参数推导线性回归模型的目标函数:$J(β)=\\sum\\left(y-Xβ\\right)^{2}$为了保证回归系数$β$可求，岭回归模型在目标函数上加了一个L2范数的惩罚项$J(β)=\\sum\\left(y-Xβ\\right)^{2}+λ||β||_2^2=\\sum\\left(y-Xβ\\right)^{2}+\\sumλβ^2$其中$λ$为非负数，$λ$越大，则为了使$J(β)$最小，回归系数$β$就越小。推导过程：$\\begin{array}{c}J(\\beta)=(y-X \\beta)^{T}(y-X \\beta)+\\lambda \\beta^{T} \\beta \\\\=y^{T} y-y^{T} X \\beta-\\beta^{T} X^{T} y+\\beta^{T} X^{T} X \\beta+\\lambda \\beta^{T} \\beta \\\\令 \\frac{\\partial J(\\beta)}{\\partial \\beta}=0 \\\\\\Rightarrow 0-X^{T} y-X^{T} y+2 X^{T} X \\beta+2 \\lambda \\beta=0 \\\\\\Rightarrow \\beta=\\left(X^{T} X+\\lambda I\\right)^{-1} X^{T} y\\end{array}$L2范数惩罚项的加入使得$(X^{T}X+λI)$满秩，保证了可逆，但是也由于惩罚项的加$λ$，使得回归系数$β$的估计不再是无偏估计。所以岭回归是以放弃无偏性、降低精度为代价解决病态矩阵问题的回归方法。单位矩阵$I$的对角线上全是1，像一条山岭一样，这也是岭回归名称的由来。$λ$的选择模型的方差：回归系数的方差模型的偏差：预测值和真实值的差异随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的$λ$而言，随着$λ$的增大，$(X^{T}X+λI)$就越大，$(X^{T}X+λI)^{-1}$就越小，模型的方差就越小；而$λ$越大使得$β$的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的$λ$值来平衡模型的方差和偏差。根据凸优化，可以将岭回归模型的目标函数$J(β)$最小化问题等价于$\\left\\{\\begin{array}{l}\\operatorname{argmin}\\left\\{\\Sigma\\left(y-X \\beta^{2}\\right)\\right\\} \\ \\Sigma \\beta^{2} \\leq t\\end{array}\\right.$其中$t$为一个常数。以最简单的二维为例，即$β(β_1,β_2)$其几何图形是:抛物面代表的是$\\sum(y-X\\beta)^2$的部分，圆柱体代表的是$β_1^{1}+β_2^{2}≤t$的部分。最小二乘解是抛物面的中心，岭回归解是抛物面与圆柱体的交点。岭回归的惩罚项$∑λβ^2$是关于回归系数$β$的二次函数，对目标函数求偏导时会保留$β$，抛物面与圆柱体很难相交于轴上使某个变量的回归系数为0，因此岭回归不能实现变量的剔除。（1）岭迹法确定$λ$值由$β=(X^TX+λI)^{−1}X^Ty$ 可知$β$是$λ$的函数，当$λ∈[0,∞)$时，在平面直角坐标系中的$β−λ$曲线称为岭迹曲线。当$β$趋于稳定的点就是所要寻找的$λ$值。代码：1234567891011121314151617181920212223242526272829import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridgeimport matplotlib.pyplot as pltdata=pd.read_csv('diabetes.csv')x=data.iloc[:,1:-1]y=data['Outcome']#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#存放偏回归系数ridge_cofficients=[]for Lambda in Lambdas: ridge=Ridge(alpha=Lambda,normalize=True) ridge.fit(x_train,y_train) ridge_cofficients.append(ridge.coef_)#绘制岭迹曲线plt.rcParams['font.sans-serif']=['Microsoft YaHei']plt.rcParams['axes.unicode_minus']=Falseplt.style.use('ggplot')plt.plot(Lambdas,ridge_cofficients)#x轴做对数处理plt.xscale('log')plt.xlabel('Log(Lambda)')plt.ylabel('Cofficients')plt.show()输出结果：正则化力度λ越大，权重系数w越小正则化力度λ越小，权重系数w越大书上说在0.01附近大多数回归系数就趋于稳定，这哪看得出？所以定性的方法一般不太靠谱，还是用定量的方法吧！（2）交叉验证法确定$λ$值交叉验证法的思想是，将数据集拆分为$k$个数据组(每组样本量大体相当)，从$k$组中挑选$k-1$组用于模型的训练，剩下的1组用于模型的测试，则会有$k-1$个训练集和测试集配对，每一种训练集和测试集下都会有对应的一个模型及模型评分（如均方误差），进而可以得到一个平均评分。对于$λ$值则选择平均评分最优的$λ$值。RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)•lambdas：用于指定多个$λ$值的元组或数组对象，默认包含0.1,1,10三种值。fit_intercept：bool类型，是否需要拟合截距项，默认为True。normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。scoring：指定用于模型评估的度量方法。cv：指定交叉验证的重数。gcv_mode：指定广义交叉验证的方法。store_cv_values：bool类型，是否保存每个λ\\lambdaλ下交叉验证的评估信息，默认为False，只有cv为None时有效。代码：12345678910111213141516import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import RidgeCVimport matplotlib.pyplot as pltdata=pd.read_csv('diabetes.csv')x=data.iloc[:,1:-1]y=data['Outcome']#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)ridge_cv.fit(x_train,y_train)print(ridge_cv.alpha_)输出结果：10.038720387818125535代码实现Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=‘auto’, random_state=None)具有l2正则化的线性回归alpha：正则化力度，也叫 λ，默认为1。λ取值：0.1~1 ~10fit_intercept：bool类型，是否需要拟合截距项，默认为True。normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据copy_X：bool类型，是否复制自变量X的数值，默认为True。max_iter：指定模型的最大迭代次数。solver：指定模型求解最优化问题的算法，默认为’auto’。sag:如果数据集、特征都比较大，选择该随机梯度下降优化random_state：指定随机生成器的种子。Ridge.coef_:回归权重_Ridge.intercept_:回归偏置All last four solvers support both dense and sparse data. However,only ‘sag’ supports sparse input when ‘fit_intercept’is True.Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)_具有l2正则化的线性回归，可以进行交叉验证_coef_:回归系数12345class _BaseRidgeCV(LinearModel): def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False):代码：123456789101112131415161718192021222324252627import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridge,RidgeCVdata=pd.read_csv('diabetes.csv')x=data.iloc[:,1:-1]y=data['Outcome']#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)ridge_cv.fit(x_train,y_train)print(ridge_cv.alpha_)#基于最佳lambda值建模ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)ridge.fit(x_train,y_train)#打印回归系数print(pd.Series(index=['Intercept']+x_train.columns.tolist(), data=[ridge.intercept_]+ridge.coef_.tolist()))#模型评估ridge_pred=ridge.predict(x_test)#均方误差from sklearn.metrics import mean_squared_errorMSE=mean_squared_error(y_test,ridge_pred)print(MSE)输出结果：12345678910110.038720387818125535Intercept -0.829372Glucose 0.005658BloodPressure -0.002019SkinThickness -0.000805Insulin -0.000016BMI 0.012776DiabetesPedigreeFunction 0.158436Age 0.004989dtype: float640.16870817670347735LASSO回归参数推导岭回归无法剔除变量，而LASSO回归模型，将惩罚项由L2范数变为L1范数，可以将一些不重要的回归系数缩减为0，达到剔除变量的目的。$J(β)=\\sum\\left(y-Xβ\\right)^{2}+λ||β||_1=\\sum\\left(y-Xβ\\right)^{2}+\\sumλ|β|=ESS（β）+λl_1(β)$$λ$的选择直接使用交叉验证法LassoCV(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute=‘auto’, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection=‘cyclic’)eps：指代λ\\lambdaλ最小值与最大值的商，默认为0.001。n_alphas：指定λ\\lambdaλ的个数，默认为100个。alphas：指定具体的λ\\lambdaλ列表用于模型的运算。fit_intercept：bool类型，是否需要拟合截距项，默认为True。normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。max_iter：指定模型的最大迭代次数。tol：指定模型收敛的阈值，默认为0.0001。copy_X：bool类型，是否复制自变量X的数值，默认为True。cv：指定交叉验证的重数。verbose：bool类型，是否返回模型运行的详细信息，默认为False。n_jobs：指定使用的CPU数量，默认为1，如果为-1表示所有CPU用于交叉验证的运算。positive：bool类型，是否将回归系数强制为正数，默认为False。random_state：指定随机生成器的种子。selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。代码：123456789101112131415import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LassoCVdata=pd.read_csv('diabetes.csv')x=data.iloc[:,1:-1]y=data['Outcome']#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估lasso_cv=LassoCV(alphas=Lambdas,normalize=True,cv=10,max_iter=10000)lasso_cv.fit(x_train,y_train)print(lasso_cv.alpha_)输出结果：10.00011357333583431052代码实现Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=‘cyclic’)alphas：指定λ\\lambdaλ值，默认为1。fit_intercept：bool类型，是否需要拟合截距项，默认为True。normalize：bool类型，建模时是否对数据集做标准化处理，默认为False。precompute：bool类型，是否在建模前计算Gram矩阵提升运算速度，默认为False。copy_X：bool类型，是否复制自变量X的数值，默认为True。max_iter：指定模型的最大迭代次数。tol：指定模型收敛的阈值，默认为0.0001。warm_start：bool类型，是否将前一次训练结果用作后一次的训练，默认为False。positive：bool类型，是否将回归系数强制为正数，默认为False。random_state：指定随机生成器的种子。selection：指定每次迭代选择的回归系数，如果为’random’，表示每次迭代中将随机更新回归系数；如果为’cyclic’，则每次迭代时回归系数的更新都基于上一次运算。代码：1234567891011121314151617181920212223242526import pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Lasso,LassoCVdata=pd.read_csv('diabetes.csv')x=data.iloc[:,1:-1]y=data['Outcome']#拆分为训练集和测试集x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#构造不同的lambda值Lambdas=np.logspace(-5,2,200)#设置交叉验证的参数，使用均方误差评估lasso_cv=LassoCV(alphas=Lambdas,normalize=True,cv=10,max_iter=10000)lasso_cv.fit(x_train,y_train)#基于最佳lambda值建模lasso=Lasso(alpha=lasso_cv.alpha_,normalize=True,max_iter=10000)lasso.fit(x_train,y_train)#打印回归系数print(pd.Series(index=['Intercept']+x_train.columns.tolist(), data=[lasso.intercept_]+lasso.coef_.tolist()))#模型评估lasso_pred=lasso.predict(x_test)#均方误差from sklearn.metrics import mean_squared_errorMSE=mean_squared_error(y_test,lasso_pred)print(MSE)输出结果：12345678910Intercept -0.842415Glucose 0.005801BloodPressure -0.001983SkinThickness -0.000667Insulin -0.000011BMI 0.012670DiabetesPedigreeFunction 0.153236Age 0.004865dtype: float640.16876008361250405相对于岭回归而言，可以看到LASSO回归剔除了两个变量，降低了模型的复杂度，同时减少了均方误差，提高了模型的拟合效果。逻辑回归逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。逻辑回归的应用场景广告点击率是否为垃圾邮件是否患病金融诈骗虚假账号目的：分类还是回归？经典的二分类算法！逻辑回归的决策边界：可以是非线性的逻辑回归的原理输入$h_w(x)=w_1x_1+w_2x_2+w_3x_3…+b=w^Tx+b$Sigmoid函数公式：$g(z)=\\frac{1}{1+e^{-z}}$自变量取值为任意实数，值域[0,1]解释：将任意的输入映射到了[0,1]区间，我们在线性回归中可以得到一个预测值，再将该值映射到Sigmoid 函数中这样就完成了由值到概率的转换，也就是分类任务预测函数：h_w(x)=g(w^Tx)=\\frac{1}{1+e^{-w^Tx}}分类任务：$P(y=1|x;w)=h_w(x)$ ; $P(y=0|x;w)=1-h_w(x)$整合得： $P(y|x;w)=(h_w(x))^y(1-h_w(x))^{1-y}$解释：对于二分类任务（0，1），整合后y取0只保留$(1-h_w(x))^{1-y}$，y取1只保留$(h_w(x))^y$损失以及优化1、损失逻辑回归的损失，称之为对数似然损失，公式如下：分开类别：\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{array}{ll} -\\log \\left(h_{\\theta}(x)\\right) & \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) & \\text { if } y=0 \\end{array}\\right.综合完整损失函数：$$\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=\\sum_{i=1}^{m}-y_{i} \\log \\left(h_{\\theta}(x)\\right)-\\left(1-y_{i}\\right) \\log \\left(1-h_{\\theta}(x)\\right)$$2、优化同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。逻辑回归算法APIsklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)solver:优化求解方式默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数sag：根据数据集自动选择，随机平均梯度下降penalty：正则化的种类C：正则化力度默认将类别数量少的当做正例LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)良／恶性乳腺癌肿瘤预测案例：癌症分类预测良/恶性乳腺癌肿瘤预测数据描述:（1）699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤相关的医学特征，最后一列表示肿瘤类型的数值。（2）包含16个缺失值，用”?”标出。原始数据的下载地址：https://archive.ics.uci.edu/ml/machine-learning-databases/分析:缺失值处理标准化处理逻辑回归预测代码：123456789101112131415161718192021222324252627282930313233343536373839#逻辑回归进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegression# 1、读取数据column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\", names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace='?', value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data['Class']#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用逻辑回归lr = LogisticRegression()lr.fit(x_train, y_train)#逻辑回归的模型参数：回归系数和偏置print(\"权重：\\n\", lr.coef_)print(\"偏置：\\n\", lr.intercept_)#6、模型评估#方法1：直接比对真实值和预测值y_predict=lr.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=lr.score(x_test,y_test)print('准确率为：\\n',score)输出结果：123456789101112131415161718192021222324252627282930313233343536373839Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: bool权重： [[1.4449604 0.10902357 0.64529009 1.02979746 0.2544256 1.55064687 0.92516667 0.62683691 0.54739363]]偏置： [-1.08426503]y_predict: #注：2——良性 4——恶性 [4 4 2 4 2 2 4 2 4 4 2 2 4 2 4 2 2 2 2 4 4 2 2 2 4 2 2 4 2 2 2 2 2 2 2 4 2 2 2 4 2 4 2 2 2 4 4 2 2 2 2 4 4 4 2 2 2 4 4 2 4 2 2 4 2 2 2 4 2 4 2 2 4 4 2 2 2 2 2 2 4 4 2 2 2 4 2 4 4 4 4 2 4 2 2 4 2 2 4 2 4 4 2 2 4 2 2 4 2 2 4 2 2 4 2 2 4 2 4 2 2 2 4 4 4 2 4 2 2 2 2 2 4 2 4 2 4 2 4 4 2 4 2 2 2 4 4 2 2 2 2 2 4 2 2 4 4 4 4 2 2 4 4 2 4 4 2 4 2 4 4 4 2 2 4 2 2 2 2 4 2 4 2 4 2 2 4 4 2 4 2 4 2 4 2 2 2 2 4 4 4 4 4 2 4]直接比对真实值和预测值： 221 True266 True4 True183 True341 True ... 55 True132 True15 True597 True479 TrueName: Class, Length: 205, dtype: bool准确率为： 0.9707317073170731在很多分类场景当中我们不一定只关注预测的准确率！！！！！比如以这个癌症举例子！！！我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。分类模型的评估准确率、精确率、召回率、f1_score，混淆矩阵，ks，ks曲线，ROC曲线，psi等。准确率、精确率、召回率、f1_score准确率（Accuracy）的定义是：对于给定的测试集，分类模型正确分类的样本数与总样本数之比；代码示例：1234567891011#1、准确率import numpy as npfrom sklearn.metrics import accuracy_scorey_pred = [0, 2, 1, 3,9,9,8,5,8]y_true = [0, 1, 2, 3,2,6,3,5,9]accuracy_score(y_true, y_pred)Out[127]: 0.33333333333333331 accuracy_score(y_true, y_pred, normalize=False) # 类似海明距离，每个类别求准确后，再求微平均Out[128]: 3精确率(Precision)：预测结果为正例样本中真实为正例的比例（了解）召回率(Recall)：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）召回率：查得全不全 应用：质量检测 次品那么怎么更好理解这个两个概念F1_score：反映了模型的稳健型在理想情况下，我们希望模型的精确率越高越好，同时召回率也越高越高，但是，现实情况往往事与愿违，在现实情况下，精确率和召回率像是坐在跷跷板上一样，往往出现一个值升高，另一个值降低，那么，有没有一个指标来综合考虑精确率和召回率了，这个指标就是F值。F值的计算公式为：$F=\\frac{(a^2+1)PR}{a^2*(P+R)}$ 式中：$P：Precision$，$R：Recall$，$a$：权重因子。当$a=1$时，$F$值便是$F1$值，代表精确率和召回率的权重是一样的，是最常用的一种评价指标。$F 1=\\frac{2 T P}{2 T P+F N+F P}=\\frac{2 \\cdot \\text { Precision } \\cdot \\text { Recall}}{\\text { Precision }+\\text { Recall}}$混淆矩阵在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)混淆矩阵一级指标（最底层的）：真实值是positive，模型认为是positive的数量（True Positive=TP）；真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第一类错误（Type I Error）；真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第二类错误（Type II Error）；真实值是negative，模型认为是negative的数量（True Negative=TN）示例及实现代码123456789101112# 假如有一个模型在测试集上得到的预测结果为：y_true=[1,0,0,2,1,0,3,3,3]# 实际的类别 y_pred=[1,1,0,2,1,0,1,3,3]# 模型预测的类别 #使用sklearn模块计算混淆矩阵from sklearn.metrics import confusion_matrixconfusion_mat=confusion_matrix(y_true,y_pred)print(confusion_mat) #看看混淆矩阵长啥样 [[2 1 0 0] [0 2 0 0] [0 0 1 0] [0 1 0 2]]混淆矩阵可视化12345678910111213import matplotlib.pyplot as pltimport numpy as npdef plot_confusion_matrix(confusion_mat): #将混淆矩阵画图并显示出来''' plt.title('Confusion matrix') plt.imshow(confusion_mat,interpolation='nearest',cmap=plt.cm.gray) plt.colorbar() tick_marks=np.arange(confusion_mat.shape[0]) plt.xticks(tick_marks,tick_marks) plt.yticks(tick_marks,tick_marks) plt.ylabel('True label') plt.xlabel('Predicted label') plt.show()plot_confusion_matrix(confusion_mat)分类评估报告APIsklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )y_true：真实目标值y_pred：估计器预测目标值labels:指定类别对应的数字target_names：目标类别名称return：每个类别精确率与召回率上文逻辑回归进行癌症预测代码+下文代码：123from sklearn.metrics import classification_reportreport=classification_report(y_test, lr.predict(x_test), labels=[2, 4], target_names=['良性', '恶性'])print(\"精确率和召回率为：\",report)输出结果：12345678精确率和召回率为： precision recall f1-score support 良性 0.98 0.98 0.98 132 恶性 0.96 0.97 0.97 73 accuracy 0.98 205 macro avg 0.97 0.97 0.97 205weighted avg 0.98 0.98 0.98 205二级指标混淆矩阵里面统计的是个数，有时候面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）：准确率（Accuracy）—— 针对整个模型精确率（Precision）灵敏度（Sensitivity）：就是召回率（Recall）特异度（Specificity）例：假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题解：准确率：99% 召回率：99/99=100% 精确率：99%F1—score：2*99%/199%=99.497%AUC：0.5 —不好的模型 AUC下文介绍TPR=100%FPR=1/1=100%问题：如何衡量样本不均衡下的评估？ROC曲线和AUC计算计算ROC值1234567import numpy as npfrom sklearn.metrics import roc_auc_scorey_true = np.array([0, 0, 1, 1])y_scores = np.array([0.1, 0.4, 0.35, 0.8])print(roc_auc_score(y_true, y_scores))0.75TPR = TP / (TP + FN)所有真实类别为1的样本中，预测类别为1的比例FPR = FP / (FP + FN)所有真实类别为0的样本中，预测类别为1的比例ROC曲线ROC曲线的横轴就是FPR，纵轴就是TPR，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5AUC指标AUC(AUC值是一个概率值)的概率意义是随机取一对正负样本，正样本得分大于负样本的概率AUC的最小值为0.5，最大值为1，取值越高越好AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。AUC就是ROC 曲线下的面积，通常情况下数值介于0.5-1之间，可以评价分类器的好坏，数值越大说明越好。AUC计算APIfrom sklearn.metrics import roc_auc_scoresklearn.metrics.roc_auc_score(y_true, y_score)计算ROC曲线面积，即AUC值y_true:每个样本的真实类别，必须为0(反例),1(正例)标记y_score:每个样本预测的概率值代码（接上文代码）：1234567891011121314151617181920212223242526print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(\"AUC指标：\", roc_auc_score(y_test, lr.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率y_score = lr.predict_proba(x_test)[:, 1]# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color='steelblue', alpha=0.5, edgecolor='black')# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color='black', lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color='red', linestyle='--')plt.text(0.5, 0.3, 'ROC curve (area=%0.2f)' % roc_auc)plt.xlabel('1-Specificity')plt.ylabel('Sensitivity')plt.show()输出结果：123456753 4503 2434 2495 2595 2Name: Class, dtype: int64AUC指标： 0.9556899934597778总结AUC只能用来评价二分类AUC非常适合评价样本不平衡中的分类器性能LIft和gainLift图衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。Gain图是描述整体精准度的指标。计算公式如下：$Lift=\\frac{\\frac{TP}{TP+FP}}{\\frac{P}{P+N}}$$Gain=\\frac{TP}{TP+FP}$作图步骤：（1） 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）——-这就是截断点依次选取的顺序；（2） 按顺序选取截断点，并计算Lift和Gain —-也可以只选取n个截断点，分别在1/n，2/n，3/n等位置回归模型的评估主要有以下方法：指标描述metrics方法Mean Absolute Error (MAE)平均绝对误差from sklearn.metrics import mean_absolute_errorMean Square Error(MSE)平均方差from sklearn.metrics import mean_squared_errorR-SquaredR平方值from sklearn.metrics import r2_score代码：12345678#sklearn的调用from sklearn.metrics import mean_absolute_errorfrom sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score mean_absolute_error(y_test,y_predict)mean_squared_error(y_test,y_predict)r2_score(y_test,y_predict)（一）平均绝对误差（Mean Absolute Error，MAE）平均绝对误差就是指预测值与真实值之间平均相差多大 ：$MAE=\\frac{1}{m}\\sum_{i=1}^{m}|f_i-y_i|$平均绝对误差能更好地反映预测值误差的实际情况.（二）均方误差（Mean Squared Error，MSE）观测值与真值偏差的平方和与观测次数的比值：$MSE=\\frac{1}{m}\\sum_{i=1}^{m}(f_i-y_i)^2$这也是线性回归中最常用的损失函数，线性回归过程中尽量让该损失函数最小。那么模型之间的对比也可以用它来比较。MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。（三）R-square(决定系数)$R^2=1-\\frac{\\sum(Y_actual-Y_predict)^2}{\\sum(Y_actual-Y_mean)^2}$123456#以线性回归为例lr = LinearRegression()lr.fit(x_train,y_train)y_ = lr.predict(X_test)# R2 决定系数lr.score(X_test,y_test) #与用from sklearn.metrics import r2_score相同数学理解： 分母理解为原始数据的离散程度，分子为预测数据和原始数据的误差，二者相除可以消除原始数据离散程度的影响其实“决定系数”是通过数据的变化来表征一个拟合的好坏。理论上取值范围$ （-∞，1] $, 正常取值范围为[0 1] ———实际操作中通常会选择拟合较好的曲线计算R²，因此很少出现$-∞$越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好越接近0，表明模型拟合的越差经验值：&gt;0.4， 拟合效果好缺点：数据集的样本越大，R²越大，因此，不同数据集的模型结果比较会有一定的误差（四）Adjusted R-Square (校正决定系数）$R^2_adjusted=1-\\frac{(1-R^2)(n-1)}{n-p-1}$$n$为样本数量，$p$为特征数量消除了样本数量和特征数量的影响（五）交叉验证（Cross-Validation）交叉验证，有的时候也称作循环估计（Rotation Estimation），是一种统计学上将数据样本切割成较小子集的实用方法，该理论是由Seymour Geisser提出的。在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为PRESS(predicted Error Sum of Squares)。交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set)，另一部分做为验证集(validation set or test set)。首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型(model)，以此来做为评价分类器的性能指标。无论分类还是回归模型，都可以利用交叉验证，进行模型评估，示例代码：123from sklearn.cross_validation import cross_val_scoreprint(cross_val_score(knn, X_train, y_train, cv=4))print(cross_cal_score(lr, X, y, cv=2))模型保存和加载当训练或者计算好一个模型之后，那么如果别人需要我们提供结果预测，就需要保存模型（主要是保存算法的参数）sklearn模型的保存和加载APIfrom sklearn.externals import joblib保存：joblib.dump(rf, ‘test.pkl’)加载：estimator = joblib.load(‘test.pkl’)线性回归的模型保存加载案例保存1234567#使用线性模型进行预测#使用正规方程求解lr = LinearRegression()#此时在干什么？lr.fit(x_train, y_train)#保存训练完结束的模型joblib.dump(lr, \"test.pkl\")加载1234567891011#使用线性模型进行预测#使用正规方程求解#lr = LinearRegression()#此时在干什么？#lr.fit(x_train, y_train)#保存训练完结束的模型#joblib.dump(lr, \"test.pkl\")#当上步模型已经保存好了就不需要训练fit和保存模型了#通过已有的模型去预测房价model = joblib.load(\"test.pkl\")print(\"从文件加载进来的模型预测房价的结果：\", std_y.inverse_transform(model.predict(x_test)))无监督学习-K-means算法什么是无监督学习没有目标值—无监督学习例：一家广告平台需要根据相似的人口学特征和购买习惯将美国人口分成不同的小组，以便广告客户可以通过有关联的广告接触到他们的目标客户。Airbnb 需要将自己的房屋清单分组成不同的社区，以便用户能更轻松地查阅这些清单。一个数据科学团队需要降低一个大型数据集的维度的数量，以便简化建模和降低文件大小。我们可以怎样最有用地对其进行归纳和分组？我们可以怎样以一种压缩格式有效地表征数据？这都是无监督学习的目标，之所以称之为无监督，是因为这是从无标签的数据开始学习的。无监督学习包含算法聚类K-means(K均值聚类)降维PCAK-means原理我们先来看一下一个K-means的聚类效果图5.3.1. K-means聚类步骤1、随机设置K个特征空间内的点作为初始的聚类中心2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程K—超参数 根据以下确定： 1）看需求；2）调节超参数我们以一张图来解释效果K-means算法APIsklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)k-means聚类n_clusters:开始的聚类中心数量init:初始化方法，默认为’k-means ++’labels_:默认标记的类型，可以和真实值比较（不是值比较）案例：k-means对Instacart Market用户聚类分析1、PCA降维数据2、k-means聚类3、聚类结果显示代码：123456789101112131415161718192021222324252627282930313233343536373839import pandas as pd# 1、获取数据order_products = pd.read_csv(\"./instacart/order_products__prior.csv\")products = pd.read_csv(\"./instacart/products.csv\")orders = pd.read_csv(\"./instacart/orders.csv\")aisles = pd.read_csv(\"./instacart/aisles.csv\")# 2、合并表# order_products__prior.csv：订单与商品信息# 字段：order_id, product_id, add_to_cart_order, reordered# products.csv：商品信息# 字段：product_id, product_name, aisle_id, department_id# orders.csv：用户的订单信息# 字段：order_id,user_id,eval_set,order_number,….# aisles.csv：商品所属具体物品类别# 字段： aisle_id, aisle# 合并aisles和products aisle和product_idtab1 = pd.merge(aisles, products, on=[\"aisle_id\", \"aisle_id\"])tab2 = pd.merge(tab1, order_products, on=[\"product_id\", \"product_id\"])tab3 = pd.merge(tab2, orders, on=[\"order_id\", \"order_id\"])#print(tab3.head())# 3、找到user_id和aisle之间的关系table = pd.crosstab(tab3[\"user_id\"], tab3[\"aisle\"])data = table[:10000] #缩小数据# 4、PCA降维from sklearn.decomposition import PCA# 1）实例化一个转换器类transfer = PCA(n_components=0.95)# 2）调用fit_transformdata_new = transfer.fit_transform(data)print(data_new.shape)#5、预估器流程from sklearn.cluster import KMeansestimator=KMeans(n_clusters=3)#KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,# n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',# random_state=None, tol=0.0001, verbose=0)estimator.fit(data_new)y_predict=estimator.predict(data_new)print(y_predict[:300])输出结果：12345678910(10000, 42)[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 2 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 2 1 1 1 2 1 1 1 1 0 1 0 1 2 1 1 1 0 1 1 1 1 2 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 2 0 1 1 1 1 0 1 1 1 1 0]问题：如何去评估聚类的效果呢？Kmeans性能评估指标轮廓系数$SC_i=\\frac{b_i-a_i}{max(b_i,a_i)}$注：对于每个点 $i$为已聚类数据中的样本 ，$b_i$ 为 $i$ 到其它族群的所有样本的距离最小值，$a_i $为 $i$ 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值轮廓系数值分析分析过程（我们以一个蓝1点为例）1、计算出蓝1离本身族群所有点的距离的平均值a_i2、蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为b_i根据公式：极端值考虑：如果b_i &gt;&gt;a_i: 那么公式结果趋近于1；如果a_i&gt;&gt;&gt;b_i: 那么公式结果趋近于-1结论如果$b_i&gt;&gt;a_i$:趋近于1效果越好，$ b_i&lt;&lt;a_i$:趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。轮廓系数APIsklearn.metrics.silhouette_score(X, labels)计算所有样本的平均轮廓系数X：特征值labels：被聚类标记的目标值用户聚类结果评估代码接上文代码：123#6、模型评估—轮廓系数from sklearn.metrics import silhouette_scoreprint(silhouette_score(data_new,y_predict))输出结果：10.5396819903993842K-means总结特点分析：采用迭代式算法，直观易懂并且非常实用缺点：容易收敛到局部最优解(多次聚类)注：聚类一般做在分类之前SVM算法代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#SVC算法进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\", names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace='?', value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data['Class']#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用SVC算法from sklearn import svm #支持向量机#module = svm.SVC()module.fit(x_train, y_train)#6、模型评估#方法1：直接比对真实值和预测值y_predict=module.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=module.score(x_test,y_test)print('准确率为：\\n',score)from sklearn.metrics import classification_reportreport=classification_report(y_test, module.predict(x_test), labels=[2, 4], target_names=['良性', '恶性'])print(\"精确率和召回率为：\",report)print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(\"AUC指标：\", roc_auc_score(y_test, module.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率###通过decision_function()计算得到的y_score的值，用在roc_curve()函数中y_score = module.fit(x_train, y_train).decision_function(x_test)# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color='steelblue', alpha=0.5, edgecolor='black')# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color='black', lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color='red', linestyle='--')plt.text(0.5, 0.3, 'ROC curve (area=%0.2f)' % roc_auc)plt.xlabel('1-Specificity')plt.ylabel('Sensitivity')plt.show()输出结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Sample code number FalseClump Thickness FalseUniformity of Cell Size FalseUniformity of Cell Shape FalseMarginal Adhesion FalseSingle Epithelial Cell Size FalseBare Nuclei FalseBland Chromatin FalseNormal Nucleoli FalseMitoses FalseClass Falsedtype: booly_predict: [2 4 2 2 2 2 2 2 2 4 2 2 4 4 2 2 2 2 2 2 4 4 2 2 2 4 4 4 2 4 4 4 2 2 2 2 2 4 2 2 2 2 4 2 2 2 4 2 4 2 2 2 2 4 4 4 2 4 2 2 2 2 4 2 4 2 2 4 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 4 2 2 2 2 4 2 4 2 2 4 2 2 2 4 4 2 2 2 2 4 4 2 4 2 4 2 4 4 2 2 2 4 4 4 2 4 2 2 2 2 2 2 4 2 2 2 2 2 2 4 2 4 4 2 4 2 2 2 4 2 4 4 2 4 4 2 2 2 2 2 2 2 4 4 2 2 2 2 4 2 4 2 2 4 2 2 4 4 2 2 4 2 2 4 4 2 4 4 2 4 4 2 2 2 4 2 2 2 2 4 2 4 2 4 4 4 2]直接比对真实值和预测值： 492 True236 True208 True518 True81 True ... 653 True434 False196 False210 True137 TrueName: Class, Length: 205, dtype: bool准确率为： 0.9658536585365853精确率和召回率为： precision recall f1-score support 良性 0.99 0.96 0.97 139 恶性 0.92 0.98 0.95 66 accuracy 0.97 205 macro avg 0.95 0.97 0.96 205weighted avg 0.97 0.97 0.97 205492 2236 4208 2518 281 2Name: Class, dtype: int64AUC指标： 0.9708415086112929GBDT算法代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#GBDT进行癌症预测import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler# 1、读取数据column_name = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\", names=column_name)# 2、数据处理—处理缺失值data = data.replace(to_replace='?', value=np.nan) #1)替换np.nandata = data.dropna() #2)删除缺失值print(data.isnull().any()) #确认不存在缺失值# 取出特征值x = data[column_name[1:10]] #x=data.iloc[:,1:-1]y = data[column_name[10]] #y=data['Class']#3、分割数据集x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)#4、特征工程—标准化std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test)# 5、使用GBDTfrom sklearn.ensemble import GradientBoostingClassifier #Gradient Boosting 和 AdaBoost算法#from sklearn.ensemble import GradientBoostingRegressormodule = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)module.fit(x_train, y_train)#6、模型评估#方法1：直接比对真实值和预测值y_predict=module.predict(x_test)print('y_predict:\\n',y_predict)print('直接比对真实值和预测值：\\n',y_test y_predict)#方法2：计算准确率score=module.score(x_test,y_test)print('准确率为：\\n',score)from sklearn.metrics import classification_reportreport=classification_report(y_test, module.predict(x_test), labels=[2, 4], target_names=['良性', '恶性'])print(\"精确率和召回率为：\",report)print(y_test.head())#y_true:每个样本的真实类别，必须为0（反例），1（正例）标记#将y_test 转换成0和1y_test = np.where(y_test &gt; 2.5, 1, 0) #y_test数值大于2.5设置为1，不大于设置为0from sklearn.metrics import roc_auc_scoreprint(\"AUC指标：\", roc_auc_score(y_test, module.predict(x_test)))# ROC曲线# y_score为模型预测正例的概率y_score = module.predict_proba(x_test)[:, 1]# 计算不同阈值下，fpr和tpr的组合之，fpr表示1-Specificity，tpr表示Sensitivityfrom sklearn import metricsfpr, tpr, threshold = metrics.roc_curve(y_test, y_score)# 计算AUCroc_auc = metrics.auc(fpr, tpr)# 绘制面积图import matplotlib.pyplot as pltplt.stackplot(fpr, tpr, color='steelblue', alpha=0.5, edgecolor='black')# 添加ROC曲线的轮廓plt.plot(fpr, tpr, color='black', lw=1)# 添加对角线作为参考线plt.plot([0, 1], [0, 1], color='red', linestyle='--')plt.text(0.5, 0.3, 'ROC curve (area=%0.2f)' % roc_auc)plt.xlabel('1-Specificity')plt.ylabel('Sensitivity')plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.tk/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"分类算法","slug":"机器学习/分类算法","permalink":"https://qikaile.tk/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"name":"回归算法","slug":"机器学习/分类算法/回归算法","permalink":"https://qikaile.tk/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"},{"name":"聚类算法","slug":"机器学习/分类算法/回归算法/聚类算法","permalink":"https://qikaile.tk/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://qikaile.tk/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Hexo搭建个人博客系列：主题美化篇","slug":"hexo-theme-beautify","date":"2020-03-18T16:00:00.000Z","updated":"2020-04-04T08:33:14.914Z","comments":true,"path":"archives/20b03m19d.html","link":"","permalink":"https://qikaile.tk/archives/20b03m19d.html","excerpt":"本文介绍了在Next主题的基础上进一步对博客进行美化的方案，主要包括:在文章末尾添加结束标记修改侧边栏的位置到左边添加不同类型的动态背景效果添加 live2d 看板娘为布局元素添加边缘弹性摆动效果个性化回到顶部按钮添加不同类型的鼠标点击特效评论区输入打字礼花特效读者可以根据需要选择其中喜欢的方案应用到站点博客中。","text":"本文介绍了在Next主题的基础上进一步对博客进行美化的方案，主要包括:在文章末尾添加结束标记修改侧边栏的位置到左边添加不同类型的动态背景效果添加 live2d 看板娘为布局元素添加边缘弹性摆动效果个性化回到顶部按钮添加不同类型的鼠标点击特效评论区输入打字礼花特效读者可以根据需要选择其中喜欢的方案应用到站点博客中。修改博客字体在 Google Fonts 上找到心仪的字体，然后在主题配置文件中为不同的应用场景配置字体：themes\\next\\_config.yml123456789101112131415161718192021222324252627282930font: enable: true # 外链字体库地址，例如 //fonts.googleapis.com (默认值) host: # 全局字体，应用在 body 元素上 global: external: true family: Monda # 标题字体 (h1, h2, h3, h4, h5, h6) headings: external: true family: Roboto Slab # 文章字体 posts: external: true family: # Logo 字体 logo: external: true family: # 代码字体，应用于 code 以及代码块 codes: external: true family:文章页末美化为标签添加图标默认情况下标签前缀是 # 字符，用户可以通过修改主题源码将标签的字符前缀改为图标前缀，更改后效果如下：在文章布局模板中找到文末标签相关代码段，将 # 换成 &lt;i class=&quot;fa fa-tags&quot;&gt;&lt;/i&gt; 即可：themes\\next\\layout\\_macro\\post.swig1234567891011 &lt;footer class=\"post-footer\"&gt; &#123;% if post.tags and post.tags.length and not is_index %&#125; &lt;div class=\"post-tags\"&gt; &#123;% for tag in post.tags %&#125;- &lt;a href=\"&#123;&#123; url_for(tag.path) &#125;&#125;\" rel=\"tag\"&gt;# &#123;&#123; tag.name &#125;&#125;&lt;/a&gt;+ &lt;a href=\"&#123;&#123; url_for(tag.path) &#125;&#125;\" rel=\"tag\"&gt;&lt;i class=\"fa fa-tags\"&gt;&lt;/i&gt; &#123;&#123; tag.name &#125;&#125;&lt;/a&gt; &#123;% endfor %&#125; &lt;/div&gt; &#123;% endif %&#125; ... &lt;/footer&gt;Next中使用 FontAwesome 作为图标库，用户可以在 FontAwesome 上找到心仪的图标来替换标签的字符前缀。添加结束标记本章节参考 asdfv1929 | Hexo NexT主题内给每篇文章后添加结束标语在文末添加结束标记，效果如下：新建布局模板文件 post-end-tag.swig，添加如下代码：themes\\next\\layout\\_macro\\post-end-tag.swig123456789&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=\"text-align:center;color:#bfbfbf;font-size:16px;\"&gt; &lt;span&gt;-------- 本文结束 &lt;/span&gt; &lt;i class=\"fa fa-&#123;&#123; config.post_end_tag.icon &#125;&#125;\"&gt;&lt;/i&gt; &lt;span&gt; 感谢阅读 --------&lt;/span&gt; &lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt;在文章布局模板中添加如下代码：themes\\next\\layout\\_macro\\post123456789101112131415&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125;+ &#123;% if config.post_end_tag.enabled and not is_index %&#125;+ &lt;div&gt;+ &#123;% include 'post-end-tag.swig' %&#125;+ &lt;/div&gt;+ &#123;% endif %&#125;&#123;% if theme.wechat_subscriber.enabled and not is_index %&#125; &lt;div&gt; &#123;% include 'wechat-subscriber.swig' %&#125; &lt;/div&gt;&#123;% endif %&#125;在站点配置文件末尾添加如下代码：_config.yml123post_end_tag: enabled: true # 是否开启文末的本文结束标记 icon: paw # 结束标记之间的图标重启服务器后即可在文末看到结束标记。页面加载进度条当网络不好的时候可能会在打开站点或跳转文章时出现短暂的白屏，此时如果能有加载进度提示将会提高用户操作体验。在根目录下执行以下命令安装相关依赖：1$ git clone https://github.com/theme-next/theme-next-pace themes/next/source/lib/pace在主题配置文件中设置 pace: true。默认提供了多种主题的进度条加载样式，有顶部提示的，有中间提示的，还有全页面遮挡提示的，个人认为默认的进度条效果就恰如其当，既能够在页面空白的时候起到加载作用，也不会因为太过花里胡哨而喧宾夺主，尤其是当你如果使用了不蒜子的站点访问统计的功能的时候，常常会遇到所有资源都加载完毕而不蒜子还在等待响应，如果这个时候在页面较显眼的位置出现一个停滞不前的进度条，很让人抓狂。侧边栏放左边受 猪猪侠的博客 所启发，萌生了想把主题侧边栏放在左侧的想法。Next主题各系列中只有Pisces和Gemini支持通过主题配置文件来将侧边栏置于左侧或右侧，而Muse和Mist则需要深度修改源码才能实现改变侧边栏位置。在自定义样式文件中添加如下规则：themes\\next\\source\\css\\_custom\\custom.styl1234567.sidebar-toggle &#123; left: 30px;&#125;.sidebar &#123; left: 0px;&#125;修改动效脚本代码：themes\\next\\source\\js\\src\\motion.js1234567891011121314151617181920$(document) .on('sidebar.isShowing', function() &#123; NexT.utils.isDesktop() &amp;&amp; $('body').velocity('stop').velocity(- &#123;paddingRight: SIDEBAR_WIDTH&#125;,+ &#123;paddingLeft: SIDEBAR_WIDTH&#125;, SIDEBAR_DISPLAY_DURATION ); &#125;) .on('sidebar.isHiding', function() &#123; &#125;); ... hideSidebar: function() &#123;- NexT.utils.isDesktop() &amp;&amp; $('body').velocity('stop').velocity(&#123;paddingRight: 0&#125;);+ NexT.utils.isDesktop() &amp;&amp; $('body').velocity('stop').velocity(&#123;paddingLeft: 0&#125;); this.sidebarEl.find('.motion-element').velocity('stop').css('display', 'none'); this.sidebarEl.velocity('stop').velocity(&#123;width: 0&#125;, &#123;display: 'none'&#125;); sidebarToggleLines.init(); ...&#125;如此以来就可以将侧边栏放置在左边了，但当窗口宽度缩小到991px之后会出现样式错误：侧边栏收缩消失但是页面左侧仍留有空白间距，此时修改如下代码即可：themes\\next\\source\\css\\_common\\scaffolding\\base.styl1234567891011121314body &#123; position: relative; // Required by scrollspy font-family: $font-family-base; font-size: $font-size-base; line-height: $line-height-base; color: $text-color; background: $body-bg-color;- +mobile() &#123; padding-left: 0 !important; &#125;- +tablet() &#123; padding-left: 0 !important; &#125; + +mobile() &#123; padding-right: 0 !important; &#125;+ +tablet() &#123; padding-right: 0 !important; &#125; +desktop-large() &#123; font-size: $font-size-large; &#125;&#125;添加动态背景Next主题可以通过安装插件快速为站点添加不同效果的动态背景。粒子漂浮聚合应用效果如下图：该功能由 theme-next-canvas-nest 插件提供，在根目录下执行如下命令：1$ git clone https://github.com/theme-next/theme-next-canvas-nest themes/next/source/lib/canvas-nest然后在主题配置文件中设置 canvas_nest: true 即可。Next v6.5.0 及以上版本支持更多的自定义选项：themes\\next\\_config.yml1234567canvas_nest: enable: true onmobile: true # 是否在移动端显示 color: '0,0,255' # 动态背景中线条的 RGB 颜色 opacity: 0.5 # 动态背景中线条透明度 zIndex: -1 # 动态背景的 z-index 属性值 count: 99 # 动态背景中线条数量Three 三维动效theme-next-three 插件提供了三个类型的背景动效，应用效果如下：three-wavescanvas-linescanvas-sphere在根目录下执行如下命令安装相关依赖：1$ git clone https://github.com/theme-next/theme-next-three themes/next/source/lib/three然后在主题配置文件中设置开启对应的动效选项即可。themes\\next\\_config.yml12345678# JavaScript 3D library.# Dependencies: https://github.com/theme-next/theme-next-three# three_wavesthree_waves: true# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false个人认为在站点中添加动态背景并没有实际的意义，只会凭空增加页面内存占用及CPU消耗，所以本站没有添加任何动态背景。随机三角丝带该功能由 Vue 作者 尤雨溪 首创。本章节中核心代码来源于 DIYgod 编写的 sagiri 主题。点击下方按钮下载相应的脚本，并置于 themes\\\\next\\\\source\\\\js\\ 目录下：随机三角丝带在主题自定义布局文件中添加以下代码：themes\\next\\layout\\_custom\\custom.swig123456789101112131415&#123;# 随机三角丝带背景 #&#125;&#123;% if theme.evanyou %&#125; &lt;canvas id=\"evanyou\"&gt;&lt;/canvas&gt; &lt;style&gt; #evanyou &#123; position: fixed; width: 100%; height: 100%; top: 0; left: 0; z-index: -1; &#125; &lt;/style&gt; &lt;script src=\"/js/evan-you.js\"&gt;&lt;/script&gt;&#123;% endif %&#125;如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：themes\\next\\layout\\_layout.swig12345678 ... &#123;% include '_third-party/exturl.swig' %&#125; &#123;% include '_third-party/bookmark.swig' %&#125; &#123;% include '_third-party/copy-code.swig' %&#125;+ &#123;% include '_custom/custom.swig' %&#125; &lt;/body&gt; &lt;/html&gt;在主题配置文件中添加以下代码：themes\\next\\_config.yml12# colorful trilateral riband backgroundevanyou: true如果从本地加载JS脚本速度较慢，可以考虑将脚本放到CDN上再引入。添加看板娘本章节部分内容参考 FJKang | 添加一个萌物该功能由 hexo-helper-live2d 插件支持，效果如下图：在站点根目录下执行以下命令安装依赖：1$ npm install --save hexo-helper-live2d在站点配置文件中添加以下下配置项_config.yml123456789101112131415161718192021222324252627282930# Live2D# https://github.com/EYHN/hexo-helper-live2dlive2d: enable: true pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ Relative) # 脚本加载源 scriptFrom: local # 默认从本地加载脚本 # scriptFrom: jsdelivr # 从 jsdelivr CDN 加载脚本 # scriptFrom: unpkg # 从 unpkg CDN 加载脚本 # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 从自定义地址加载脚本 tagMode: false # 只在有 &#123;&#123; live2d() &#125;&#125; 标签的页面上加载 / 在所有页面上加载 log: false # 是否在控制台打印日志 # 选择看板娘模型 model: use: live2d-widget-model-shizuku # npm package的名字 # use: wanko # /live2d_models/ 目录下的模型文件夹名称 # use: ./wives/wanko # 站点根目录下的模型文件夹名称 # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 自定义网络数据源 display: position: left # 显示在左边还是右边 width: 100 # 宽度 height: 180 # 高度 mobile: show: false react: opacityDefault: 0.7 # 默认透明度更多配置参数请查看 L2Dwidget | live2d-widget.js此时重启服务器暂时还看不到看板娘，需要手动下载或安装模型资源。可以从 hexo live2d 模型预览 里找到你喜欢的角色，然后根据 live2d-widget-models 中提供的方法来下载模型数据.例如通过以下命令下载模型 shizuku：1$ npm install live2d-widget-model-shizuku因为修改了站点配置文件，所以需要重启服务器才能预览模型效果。如果设置了 live2d.tagMode: true，则可以在指定页面中插入以下标签：1&#123;&#123; live2d() &#125;&#125;只有拥有该标签的页面才会渲染live2d模型，这样以来就可以精确控制在哪些页面上显示看板娘了。如果只想在一级菜单页面上显示看板娘，可以在Header模板中添加以下代码：themes\\next\\layout\\_partials\\header\\index.swig123+ &#123;% if is_index %&#125;+ &#123;&#123; live2d() &#125;&#125;+ &#123;% endif %&#125;个人认为在文章内出现看板娘将会影响读者注意力的集中，毕竟一篇博客里最重要的是内容，而不是这些花里胡哨转移注意力的东西。所以本站只在一级菜单页面添加了看板娘，文章页面则保持极致精简的阅读体验。经过测试发现 live2d.mobile.show: false 并没有生效，暂时没有找到好的解决方法，参考 EYHN/hexo-helper-live2d Issues #12 后发现可以在自定义样式文件中添加以下代码来解决：themes/next/source/css/_custom/custom.styl12345678#live2dcanvas &#123; +mobile() &#123; display: none; &#125; +tablet() &#123; display: none; &#125;&#125;不要乱点不该点的地方，会生气的。边缘摆动效果在 猪猪侠的博客 里发现的这种特效，觉得挺有意思的，就从他Github上给扒过来了点击下方按钮下载脚本，并置于 themes\\\\next\\\\source\\\\js\\ 目录下：wobblewindow.js在主题自定义布局文件中添加以下代码：themes\\next\\layout\\_custom\\custom.swig12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;# wobble窗口摆动特效 #&#125;&#123;% if theme.wobble %&#125; &lt;script src=\"/js/wobblewindow.js\"&gt;&lt;/script&gt; &lt;script&gt; //只在桌面版网页启用特效 if( window.innerWidth &gt; 768 )&#123; $(document).ready(function () &#123; &#123;% if theme.wobble.header %&#125; $('#header').wobbleWindow(&#123; radius: &#123;&#123; theme.wobble.radius &#125;&#125;, movementTop: false, movementLeft: false, movementRight: false, debug: false, &#125;); &#123;% endif %&#125; &#123;% if theme.wobble.sidebar %&#125; $('#sidebar').wobbleWindow(&#123; radius: &#123;&#123; theme.wobble.radius &#125;&#125;, movementLeft: false, movementTop: false, movementBottom: false, position: 'fixed', debug: false, &#125;); &#123;% endif %&#125; &#123;% if theme.wobble.footer %&#125; $('#footer').wobbleWindow(&#123; radius: &#123;&#123; theme.wobble.radius &#125;&#125;, movementBottom: false, movementLeft: false, movementRight: false, offsetX: &#123;&#123; theme.wobble.offset &#125;&#125;, position: 'absolute', debug: false, &#125;); &#123;% endif %&#125; &#125;); &#125; &lt;/script&gt;&#123;% endif %&#125;如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：themes\\next\\layout\\_layout.swig12345678 ... &#123;% include '_third-party/exturl.swig' %&#125; &#123;% include '_third-party/bookmark.swig' %&#125; &#123;% include '_third-party/copy-code.swig' %&#125;+ &#123;% include '_custom/custom.swig' %&#125; &lt;/body&gt; &lt;/html&gt;在自定义样式文件中添加以下样式：themes\\next\\source\\css\\_custom\\custom.styl1234567891011121314151617181920212223242526272829//窗口波动效果相关样式if hexo-config('wobble') &#123; .sidebar &#123; box-shadow: none; &#125; .wobbleTransparentBK&#123; background-color: rgba(0,0,0,0) !important; &#125; .wobbleTransparentLine&#123; border-color: rgba(0,0,0,0) !important; &#125; //Next.Muse中为Header和Footer添加背景色 #header, #footer &#123; background-color: rgb(245, 245, 245); &#125; //防止sidebar和footer同时开启动效时堆叠异常 #sidebar, header &#123; z-index: 1 !important; &#125; //防止挡住页末文章的阅读全文按钮 .main &#123; padding-bottom: 200px; &#125;&#125;Next.Muse主题方案中Header和Footer是没有背景色的，所以需要添加背景色后才能看出边缘摆动效果。另外，实现边缘摆动效果所需的 z-index 属性可能会导致元素堆叠异常，需要添加以上样式来矫正。在主题配置文件中添加以下代码：themes\\next\\_config.yml1234567# window wobllewobble: enable: true # 是否开启边缘波动效果 radius: 50 # 波动半径 sidebar: true # 开启侧边栏边缘摆动 header: true # 开启头部边缘摆动 footer: true # 开启脚部边缘摆动用户可以根据需要在配置文件中为选择开启边缘摆动效果的布局元素。刷新浏览器，然后将鼠标移动到布局边缘上尽情的挑逗它吧。如果从本地加载JS脚本速度较慢，可以考虑将脚本放到CDN上再引入。个性化回到顶部从 DIYgod的博客 里扒来的，效果如下：原理很简单，将 back-to-top 按钮添加图片背景，并添加CSS3动效即可。首先，找到自己喜欢的图片素材放到 source\\\\images\\ 目录下。你可以点击下方按钮下载本站所使用的小猫上吊素材（小猫咪这么可爱，当然要多放点孜然啦…）下载图片然后在自定义样式文件中添加如下代码：themes\\next\\source\\css\\_custom\\custom.styl1234567891011121314151617181920//自定义回到顶部样式.back-to-top &#123; right: 60px; width: 70px; //图片素材宽度 height: 900px; //图片素材高度 top: -900px; bottom: unset; transition: all .5s ease-in-out; background: url(\"/images/scroll.png\"); //隐藏箭头图标 &gt; i &#123; display: none; &#125; &amp;.back-to-top-on &#123; bottom: unset; top: 100vh &lt; (900px + 200px) ? calc( 100vh - 900px - 200px ) : 0px; &#125;&#125;刷新浏览器即可预览效果。鼠标点击特效从各个站点里搜罗了以下四个比较常用的鼠标点击特效：礼花特效爆炸特效浮出爱心浮出文字点击下方按钮下载相应的脚本，并置于 themes\\\\next\\\\source\\\\js\\\\cursor\\ 目录下：礼花特效 爆炸特效 浮出爱心 浮出文字在主题自定义布局文件中添加以下代码：themes\\next\\layout\\_custom\\custom.swig123456789101112&#123;# 鼠标点击特效 #&#125;&#123;% if theme.cursor_effect == \"fireworks\" %&#125; &lt;script async src=\"/js/cursor/fireworks.js\"&gt;&lt;/script&gt;&#123;% elseif theme.cursor_effect == \"explosion\" %&#125; &lt;canvas class=\"fireworks\" style=\"position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;\" &gt;&lt;/canvas&gt; &lt;script src=\"//cdn.bootcss.com/animejs/2.2.0/anime.min.js\"&gt;&lt;/script&gt; &lt;script async src=\"/js/cursor/explosion.min.js\"&gt;&lt;/script&gt;&#123;% elseif theme.cursor_effect == \"love\" %&#125; &lt;script async src=\"/js/cursor/love.min.js\"&gt;&lt;/script&gt;&#123;% elseif theme.cursor_effect == \"text\" %&#125; &lt;script async src=\"/js/cursor/text.js\"&gt;&lt;/script&gt;&#123;% endif %&#125;如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：themes\\next\\layout\\_layout.swig12345678 ... &#123;% include '_third-party/exturl.swig' %&#125; &#123;% include '_third-party/bookmark.swig' %&#125; &#123;% include '_third-party/copy-code.swig' %&#125;+ &#123;% include '_custom/custom.swig' %&#125; &lt;/body&gt; &lt;/html&gt;在主题配置文件中添加以下代码：themes\\next\\_config.yml12# mouse click effect: fireworks | explosion | love | textcursor_effect: fireworks这样即可在配置文件中一键快速切换鼠标点击特效。如果从本地加载JS脚本速度较慢，可以考虑将脚本放到CDN上再引入。打字特效本章节参考 千灵夙赋 | Hexo 优化汇总 #31，原文出自 龙笑天下 | 给 WordPress 博客网站添加评论输入打字礼花及震动特效点击下方按钮下载相应的脚本，并置于 themes\\\\next\\\\source\\\\js\\ 目录下：打字特效在主题自定义布局文件中添加以下代码：themes\\next\\layout\\_custom\\custom.swig123456789&#123;# 打字特效 #&#125;&#123;% if theme.typing_effect %&#125; &lt;script src=\"/js/activate-power-mode.min.js\"&gt;&lt;/script&gt; &lt;script&gt; POWERMODE.colorful = &#123;&#123; theme.typing_effect.colorful &#125;&#125;; POWERMODE.shake = &#123;&#123; theme.typing_effect.shake &#125;&#125;; document.body.addEventListener('input', POWERMODE); &lt;/script&gt;&#123;% endif %&#125;如果 custom.swig 文件不存在，需要手动新建并在布局页面中 body 末尾引入：themes\\next\\layout\\_layout.swig12345678 ... &#123;% include '_third-party/exturl.swig' %&#125; &#123;% include '_third-party/bookmark.swig' %&#125; &#123;% include '_third-party/copy-code.swig' %&#125;+ &#123;% include '_custom/custom.swig' %&#125; &lt;/body&gt; &lt;/html&gt;在主题配置文件中添加以下代码：themes\\next\\_config.yml1234# typing effecttyping_effect: colorful: true # 礼花特效 shake: false # 震动特效如果从本地加载JS脚本速度较慢，可以考虑将脚本放到CDN上再引入。结束语本文记录了本站在Next的基础上的进阶美化方案，除了一些简单的样式修改外，还添加了一些由插件支持的高级动效，包括动态背景、看板娘、边缘摆动、鼠标点击和打字特效等。笔者认为，动效可以使得站点变有趣，但同时也会增加网页的资源消耗，以及影响用户的关注点，有时候会喧宾夺主适得其反，建议珍爱PC资源，合理使用动效。","categories":[{"name":"技术","slug":"技术","permalink":"https://qikaile.tk/categories/%E6%8A%80%E6%9C%AF/"},{"name":"博客","slug":"技术/博客","permalink":"https://qikaile.tk/categories/%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://qikaile.tk/tags/Hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-03-13T16:00:00.000Z","updated":"2020-03-29T08:24:08.235Z","comments":true,"path":"archives/4a17b156.html","link":"","permalink":"https://qikaile.tk/archives/4a17b156.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new \"My New Post\"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment","categories":[],"tags":[]}]}